{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to loxilb documentation Background loxilb started as a project to ease deployments of cloud-native/kubernetes workloads for the edge. When we deploy services in public clouds like AWS/GCP, the services becomes easily accessible or exported to the outside world. The public cloud providers, usually by default, associate load-balancer instances for incoming requests to these services to ensure everything is quite smooth. However, for on-prem and edge deployments, there is no service type - external load balancer provider by default. For a long time, MetalLB was the only choice for the needy. But edge services are a different ball game altogether due to the fact that there are so many exotic protocols in play like GTP, SCTP, SRv6 etc and integrating everything into a seamlessly working solution has been quite difficult. loxilb dev team was approached by many people who wanted to solve this problem. As a first step to solve the problem, it became apparent that networking stack provided by Linux kernel, although very solid, really lacked the development process agility to quickly provide support for a wide variety of permutations and combinations of protocols and stateful load-balancing on them. Our search led us to the awesome tech developed by the Linux community - eBPF. The flexibility to introduce new functionality into the OS Kernel as a safe sandbox program was a complete fit to our design philosophy. It also does not need any dedicated CPU cores which makes it perfect for designing energy-efficient edge architectures. What is loxilb loxilb is an open source cloud-native load-balancer based on GoLang/eBPF with the goal of achieving cross-compatibity across a wide range of on-prem, public-cloud or hybrid K8s environments. Kubernetes with loxilb Kubernetes defines many service constructs like cluster-ip, node-port, load-balancer, ingress etc for pod to pod, pod to service and outside-world to service communication. All these services are provided by load-balancers/proxies operating at Layer4/Layer7. Since Kubernetes's is highly modular, these services can be provided by different software modules. For example, kube-proxy is used by default to provide cluster-ip and node-port services. Service type load-balancer is usually provided by public cloud-provider(s) as a managed entity. But for on-prem and self-managed clusters, there are only a few good options available. Even for provider-managed K8s like EKS, there are many who would want to bring their own LB to clusters running anywhere. loxilb provides service type load-balancer as its main use-case . loxilb can be run in-cluster or ext-to-cluster as per user need. loxilb works as a L4 load-balancer/service-proxy by default. Although L4 load-balancing provides great performance and functionality, at times, an equally performant L7 load-balancer is also necessary in K8s for various use-cases. loxilb also supports L7 load-balancing in the form of Kubernetes Ingress implementation. This also benefit users who need L4 and L7 load-balancing under the same hood. Additionally, loxilb also supports: - kube-proxy replacement with eBPF(full cluster-mesh implementation for Kubernetes) - Ingress Support - Kubernetes Gateway API - Kubernetes Network Policies (in-progress) Telco-Cloud with loxilb For deploying telco-cloud with cloud-native functions, loxilb can be used as a SCP(service communication proxy). SCP is a communication proxy defined by 3GPP and aimed at optimizing telco micro-services running in cloud-native environment. Read more about it here . Telco-cloud requires load-balancing and communication across various interfaces/standards like N2, N4, E2(ORAN), S6x, 5GLAN, GTP etc. Each of these present its own unique challenges which loxilb aims to solve e.g.: - N4 requires PFCP level session-intelligence - N2 requires NGAP parsing capability(Related Blogs - Blog-1 , Blog-2 , Blog-3 ) - S6x requires Diameter/SCTP multi-homing LB support(Related Blog ) - MEC use-cases might require UL-CL understanding(Related Blog ) - Hitless failover support might be essential for mission-critical applications - E2 might require SCTP-LB with OpenVPN bundled together - SIP support is needed to enable cloud-native VOIP Why choose loxilb? Performs much better compared to its competitors across various architectures Single-Node Performance Multi-Node Performance Performance on ARM Short Demo on Performance Utitlizes ebpf which makes it flexible as well as customizable Advanced quality of service for workloads (per LB, per end-point or per client) Works with any Kubernetes distribution/CNI - k8s/k3s/k0s/kind/OpenShift + Calico/Flannel/Cilium/Weave/Multus etc Extensive support for SCTP workloads (with multi-homing) on k8s Dual stack with NAT66, NAT64 support for k8s k8s multi-cluster support (planned \ud83d\udea7) Runs in any cloud (public cloud/on-prem) or standalone environments Overall features of loxilb L4/NAT stateful loadbalancer NAT44, NAT66, NAT64 with One-ARM, FullNAT, DSR etc Support for TCP, UDP, SCTP (w/ multi-homing), QUIC, FTP, TFTP etc High-availability support with hitless/maglev/cgnat clustering Extensive and scalable end-point liveness probes for cloud-native environments Stateful firewalling and IPSEC/Wireguard support Optimized implementation for features like Conntrack , QoS etc Full compatibility for ipvs (ipvs policies can be auto inherited) Policy oriented L7 proxy support - HTTP1.0, 1.1, 2.0 etc (planned \ud83d\udea7) Components of loxilb GoLang based control plane components A scalable/efficient eBPF based data-path implementation Integrated goBGP based routing stack A kubernetes agent kube-loxilb written in Go Architectural Considerations Understanding loxilb modes and deployment in K8s with kube-loxilb Understanding High-availability with loxilb Getting started with different K8s distributions & tools loxilb as ext-cluster pod K3s : loxilb with default flannel K3s : loxilb with calico K3s : loxilb with cilium K0s : loxilb with default kube-router networking EKS : loxilb ext-mode loxilb as in-cluster pod K3s : loxilb in-cluster mode K0s : loxilb in-cluster mode MicroK8s : loxilb in-cluster mode EKS : loxilb in-cluster mode loxilb as service-proxy K3s : loxilb service-proxy with flannel K3s : loxilb service-proxy with calico loxilb in standalone mode Run loxilb standalone Advanced Guides How-To : Service-group zones with loxilb How-To : Access end-points outside K8s How-To : Deploy multi-server K3s HA with loxilb How-To : Deploy loxilb with Ingress How-To : Deploy loxilb with multi-AZ HA support in AWS How-To : Deploy loxilb with multi-cloud HA support Knowledge-Base What is eBPF What is k8s service - load-balancer Architecture in brief Code organization eBPF internals of loxilb What are loxilb NAT Modes loxilb load-balancer algorithms Manual steps to build/run Debugging loxilb loxicmd command-line tool usage Developer's guide to loxicmd Developer's guide to loxilb API API Reference - loxilb web-Api Performance Reports Development Roadmap Contribute System Requirements Frequenctly Asked Questions- FAQs Blogs K8s - Elevating cluster networking eBPF - Map sync using Go K8s in-cluster service LB with LoxiLB K8s - Introducing SCTP Multihoming with LoxiLB Load-balancer performance comparison on Amazon Graviton2 Hyperscale anycast load balancing with HA Getting started with loxilb on Amazon EKS K8s - Deploying \"hitless\" Load-balancing Oracle Cloud - Hitless HA load balancing Ipv6 migration in Kubernetes made easy Community Posts 5G SCTP LoadBalancer Using loxilb 5G Uplink Classifier Using loxilb K3s - Using loxilb as external service lb K8s - Bringing load-balancing to multus workloads with loxilb 5G SCTP LoadBalancer Using LoxiLB on free5GC Kubernetes Services: Achieving optimal performance is elusive Research Papers (featuring loxilb) Mitigating Spectre-PHT using Speculation Barriers in Linux BPF Latest CICD Status Features(Ubuntu20.04) Features(Ubuntu22.04) K3s Tests K8s Cluster Tests EKS Test","title":"Home"},{"location":"#welcome-to-loxilb-documentation","text":"","title":"Welcome to loxilb documentation"},{"location":"#background","text":"loxilb started as a project to ease deployments of cloud-native/kubernetes workloads for the edge. When we deploy services in public clouds like AWS/GCP, the services becomes easily accessible or exported to the outside world. The public cloud providers, usually by default, associate load-balancer instances for incoming requests to these services to ensure everything is quite smooth. However, for on-prem and edge deployments, there is no service type - external load balancer provider by default. For a long time, MetalLB was the only choice for the needy. But edge services are a different ball game altogether due to the fact that there are so many exotic protocols in play like GTP, SCTP, SRv6 etc and integrating everything into a seamlessly working solution has been quite difficult. loxilb dev team was approached by many people who wanted to solve this problem. As a first step to solve the problem, it became apparent that networking stack provided by Linux kernel, although very solid, really lacked the development process agility to quickly provide support for a wide variety of permutations and combinations of protocols and stateful load-balancing on them. Our search led us to the awesome tech developed by the Linux community - eBPF. The flexibility to introduce new functionality into the OS Kernel as a safe sandbox program was a complete fit to our design philosophy. It also does not need any dedicated CPU cores which makes it perfect for designing energy-efficient edge architectures.","title":"Background"},{"location":"#what-is-loxilb","text":"loxilb is an open source cloud-native load-balancer based on GoLang/eBPF with the goal of achieving cross-compatibity across a wide range of on-prem, public-cloud or hybrid K8s environments.","title":"What is loxilb"},{"location":"#kubernetes-with-loxilb","text":"Kubernetes defines many service constructs like cluster-ip, node-port, load-balancer, ingress etc for pod to pod, pod to service and outside-world to service communication. All these services are provided by load-balancers/proxies operating at Layer4/Layer7. Since Kubernetes's is highly modular, these services can be provided by different software modules. For example, kube-proxy is used by default to provide cluster-ip and node-port services. Service type load-balancer is usually provided by public cloud-provider(s) as a managed entity. But for on-prem and self-managed clusters, there are only a few good options available. Even for provider-managed K8s like EKS, there are many who would want to bring their own LB to clusters running anywhere. loxilb provides service type load-balancer as its main use-case . loxilb can be run in-cluster or ext-to-cluster as per user need. loxilb works as a L4 load-balancer/service-proxy by default. Although L4 load-balancing provides great performance and functionality, at times, an equally performant L7 load-balancer is also necessary in K8s for various use-cases. loxilb also supports L7 load-balancing in the form of Kubernetes Ingress implementation. This also benefit users who need L4 and L7 load-balancing under the same hood. Additionally, loxilb also supports: - kube-proxy replacement with eBPF(full cluster-mesh implementation for Kubernetes) - Ingress Support - Kubernetes Gateway API - Kubernetes Network Policies (in-progress)","title":"Kubernetes with loxilb"},{"location":"#telco-cloud-with-loxilb","text":"For deploying telco-cloud with cloud-native functions, loxilb can be used as a SCP(service communication proxy). SCP is a communication proxy defined by 3GPP and aimed at optimizing telco micro-services running in cloud-native environment. Read more about it here . Telco-cloud requires load-balancing and communication across various interfaces/standards like N2, N4, E2(ORAN), S6x, 5GLAN, GTP etc. Each of these present its own unique challenges which loxilb aims to solve e.g.: - N4 requires PFCP level session-intelligence - N2 requires NGAP parsing capability(Related Blogs - Blog-1 , Blog-2 , Blog-3 ) - S6x requires Diameter/SCTP multi-homing LB support(Related Blog ) - MEC use-cases might require UL-CL understanding(Related Blog ) - Hitless failover support might be essential for mission-critical applications - E2 might require SCTP-LB with OpenVPN bundled together - SIP support is needed to enable cloud-native VOIP","title":"Telco-Cloud with loxilb"},{"location":"#why-choose-loxilb","text":"Performs much better compared to its competitors across various architectures Single-Node Performance Multi-Node Performance Performance on ARM Short Demo on Performance Utitlizes ebpf which makes it flexible as well as customizable Advanced quality of service for workloads (per LB, per end-point or per client) Works with any Kubernetes distribution/CNI - k8s/k3s/k0s/kind/OpenShift + Calico/Flannel/Cilium/Weave/Multus etc Extensive support for SCTP workloads (with multi-homing) on k8s Dual stack with NAT66, NAT64 support for k8s k8s multi-cluster support (planned \ud83d\udea7) Runs in any cloud (public cloud/on-prem) or standalone environments","title":"Why choose loxilb?"},{"location":"#overall-features-of-loxilb","text":"L4/NAT stateful loadbalancer NAT44, NAT66, NAT64 with One-ARM, FullNAT, DSR etc Support for TCP, UDP, SCTP (w/ multi-homing), QUIC, FTP, TFTP etc High-availability support with hitless/maglev/cgnat clustering Extensive and scalable end-point liveness probes for cloud-native environments Stateful firewalling and IPSEC/Wireguard support Optimized implementation for features like Conntrack , QoS etc Full compatibility for ipvs (ipvs policies can be auto inherited) Policy oriented L7 proxy support - HTTP1.0, 1.1, 2.0 etc (planned \ud83d\udea7)","title":"Overall features of loxilb"},{"location":"#components-of-loxilb","text":"GoLang based control plane components A scalable/efficient eBPF based data-path implementation Integrated goBGP based routing stack A kubernetes agent kube-loxilb written in Go","title":"Components of loxilb"},{"location":"#architectural-considerations","text":"Understanding loxilb modes and deployment in K8s with kube-loxilb Understanding High-availability with loxilb","title":"Architectural Considerations"},{"location":"#getting-started-with-different-k8s-distributions-tools","text":"","title":"Getting started with different K8s distributions &amp; tools"},{"location":"#loxilb-as-ext-cluster-pod","text":"K3s : loxilb with default flannel K3s : loxilb with calico K3s : loxilb with cilium K0s : loxilb with default kube-router networking EKS : loxilb ext-mode","title":"loxilb as ext-cluster pod"},{"location":"#loxilb-as-in-cluster-pod","text":"K3s : loxilb in-cluster mode K0s : loxilb in-cluster mode MicroK8s : loxilb in-cluster mode EKS : loxilb in-cluster mode","title":"loxilb as in-cluster pod"},{"location":"#loxilb-as-service-proxy","text":"K3s : loxilb service-proxy with flannel K3s : loxilb service-proxy with calico","title":"loxilb as service-proxy"},{"location":"#loxilb-in-standalone-mode","text":"Run loxilb standalone","title":"loxilb in standalone mode"},{"location":"#advanced-guides","text":"How-To : Service-group zones with loxilb How-To : Access end-points outside K8s How-To : Deploy multi-server K3s HA with loxilb How-To : Deploy loxilb with Ingress How-To : Deploy loxilb with multi-AZ HA support in AWS How-To : Deploy loxilb with multi-cloud HA support","title":"Advanced Guides"},{"location":"#knowledge-base","text":"What is eBPF What is k8s service - load-balancer Architecture in brief Code organization eBPF internals of loxilb What are loxilb NAT Modes loxilb load-balancer algorithms Manual steps to build/run Debugging loxilb loxicmd command-line tool usage Developer's guide to loxicmd Developer's guide to loxilb API API Reference - loxilb web-Api Performance Reports Development Roadmap Contribute System Requirements Frequenctly Asked Questions- FAQs","title":"Knowledge-Base"},{"location":"#blogs","text":"K8s - Elevating cluster networking eBPF - Map sync using Go K8s in-cluster service LB with LoxiLB K8s - Introducing SCTP Multihoming with LoxiLB Load-balancer performance comparison on Amazon Graviton2 Hyperscale anycast load balancing with HA Getting started with loxilb on Amazon EKS K8s - Deploying \"hitless\" Load-balancing Oracle Cloud - Hitless HA load balancing Ipv6 migration in Kubernetes made easy","title":"Blogs"},{"location":"#community-posts","text":"5G SCTP LoadBalancer Using loxilb 5G Uplink Classifier Using loxilb K3s - Using loxilb as external service lb K8s - Bringing load-balancing to multus workloads with loxilb 5G SCTP LoadBalancer Using LoxiLB on free5GC Kubernetes Services: Achieving optimal performance is elusive","title":"Community Posts"},{"location":"#research-papers-featuring-loxilb","text":"Mitigating Spectre-PHT using Speculation Barriers in Linux BPF","title":"Research Papers (featuring loxilb)"},{"location":"#latest-cicd-status","text":"Features(Ubuntu20.04) Features(Ubuntu22.04) K3s Tests K8s Cluster Tests EKS Test","title":"Latest CICD Status"},{"location":"api-dev/","text":"loxilb API Development Guide For building and extending LoxiLB API server. API source Architecture . \u251c\u2500\u2500 certification \u2502 \u251c\u2500\u2500 serverca.crt \u2502 \u2514\u2500\u2500 serverkey.pem \u251c\u2500\u2500 cmd \u2502 \u2514\u2500\u2500 loxilb_rest_api-server \u2502 \u2514\u2500\u2500 main.go \u251c\u2500\u2500 \u2026. \u251c\u2500\u2500 models \u2502 \u251c\u2500\u2500 error.go \u2502 \u251c\u2500\u2500 \u2026.. \u251c\u2500\u2500 restapi \u2502 \u251c\u2500\u2500 configure_loxilb_rest_api.go \u2502 \u251c\u2500\u2500 \u2026.. \u2502 \u251c\u2500\u2500 handler \u2502 \u2502 \u251c\u2500\u2500 common.go \u2502 \u2502 \u2514\u2500\u2500\u2026.. \u2502 \u251c\u2500\u2500 operations \u2502 \u2502 \u251c\u2500\u2500 get_config_conntrack_all.go \u2502 \u2502 \u2514\u2500\u2500 \u2026. \u2502 \u2514\u2500\u2500 server.go \u2514\u2500\u2500 swagger.yml * Except for the ./api/restapi/handler and ./api/certification directories, the rest of the contents are automatically created. * Add the logic for the function to the handler directory. * Add logic to file ./api/restapi/configure_loxilb_rest_api.go Swagger.yml file update paths: '/additional/url/{param}': get: summary: Test Swagger API Server. description: Check Swagger API server. This basic information or architecture is for the later applications. parameters: - name: param in: path required: true type: string format: string description: Description of the additional url responses: '204': description: OK '400': description: Malformed arguments for API call schema: $ref: '#/definitions/Error' '401': description: Invalid authentication credentials path.{Set path and parameter URL}.{get,post,etc RESTful setting}.{Description} {Set path and parameter URL} Set the path used in the RESTful API. It begins with \"config/\" and is defined as a sub-category from a large category. Define the parameters using the symbol {param}. The parameters are defined in the description section. {get,post,etc RESTful setting} Use get, post, delete, and patch to define queries, registrations, deletions, and modifications. {Description} Summary description of API Detailed description of API Parameters Set the name, path, etc. Define the content of the response Creating Additional Parts with Swagger # alias swagger='docker run --rm -it --user $(id -u):$(id -g) -e GOPATH=$(go env GOPATH):/go -v $HOME:$HOME -w $(pwd) quay.io/goswagger/swagger' # swagger generate server Development of Additional Partial Handlers package handler import ( \"fmt\" \"github.com/go-openapi/runtime/middleware\" \"testswagger.com/restapi/operations\" ) func ConfigAdditionalUrl(params operations.GetAdditionalUrlParams) middleware.Responder { ///////////////////////////////////////////// // Add logic Here // ////////////////////////////////////////////. return &ResultResponse{Result: fmt.Sprintf(\"params.param : %s\", params.param)} } Select the logic required for the ConfigAdditionalUrl portion of the handler directory. The required parameters come from operations.GetAdditionalUrlParams. Additional Partial Handler Registration func configureAPI(api *operations.LoxilbRestAPIAPI) http.Handler { ...... // Change it by putting a function here api.GetAdditionalUrlHandler = operations.GetAdditionalUrlHandlerFunc(handler.ConfigAdditionalUrl) \u2026. } if api.{REST}...The Handler form is automatically generated, where if nil is erased and a handler is added to the operation function. In many cases, additional generation is not possible. In that case, you can add the function by entering it separately. The name of the function consists of a combination of Method, URL, and Parameter.","title":"Developer's guide to loxilb API"},{"location":"api-dev/#loxilb-api-development-guide","text":"For building and extending LoxiLB API server.","title":"loxilb API Development Guide"},{"location":"api-dev/#api-source-architecture","text":". \u251c\u2500\u2500 certification \u2502 \u251c\u2500\u2500 serverca.crt \u2502 \u2514\u2500\u2500 serverkey.pem \u251c\u2500\u2500 cmd \u2502 \u2514\u2500\u2500 loxilb_rest_api-server \u2502 \u2514\u2500\u2500 main.go \u251c\u2500\u2500 \u2026. \u251c\u2500\u2500 models \u2502 \u251c\u2500\u2500 error.go \u2502 \u251c\u2500\u2500 \u2026.. \u251c\u2500\u2500 restapi \u2502 \u251c\u2500\u2500 configure_loxilb_rest_api.go \u2502 \u251c\u2500\u2500 \u2026.. \u2502 \u251c\u2500\u2500 handler \u2502 \u2502 \u251c\u2500\u2500 common.go \u2502 \u2502 \u2514\u2500\u2500\u2026.. \u2502 \u251c\u2500\u2500 operations \u2502 \u2502 \u251c\u2500\u2500 get_config_conntrack_all.go \u2502 \u2502 \u2514\u2500\u2500 \u2026. \u2502 \u2514\u2500\u2500 server.go \u2514\u2500\u2500 swagger.yml * Except for the ./api/restapi/handler and ./api/certification directories, the rest of the contents are automatically created. * Add the logic for the function to the handler directory. * Add logic to file ./api/restapi/configure_loxilb_rest_api.go Swagger.yml file update paths: '/additional/url/{param}': get: summary: Test Swagger API Server. description: Check Swagger API server. This basic information or architecture is for the later applications. parameters: - name: param in: path required: true type: string format: string description: Description of the additional url responses: '204': description: OK '400': description: Malformed arguments for API call schema: $ref: '#/definitions/Error' '401': description: Invalid authentication credentials path.{Set path and parameter URL}.{get,post,etc RESTful setting}.{Description} {Set path and parameter URL} Set the path used in the RESTful API. It begins with \"config/\" and is defined as a sub-category from a large category. Define the parameters using the symbol {param}. The parameters are defined in the description section. {get,post,etc RESTful setting} Use get, post, delete, and patch to define queries, registrations, deletions, and modifications. {Description} Summary description of API Detailed description of API Parameters Set the name, path, etc. Define the content of the response Creating Additional Parts with Swagger # alias swagger='docker run --rm -it --user $(id -u):$(id -g) -e GOPATH=$(go env GOPATH):/go -v $HOME:$HOME -w $(pwd) quay.io/goswagger/swagger' # swagger generate server Development of Additional Partial Handlers package handler import ( \"fmt\" \"github.com/go-openapi/runtime/middleware\" \"testswagger.com/restapi/operations\" ) func ConfigAdditionalUrl(params operations.GetAdditionalUrlParams) middleware.Responder { ///////////////////////////////////////////// // Add logic Here // ////////////////////////////////////////////. return &ResultResponse{Result: fmt.Sprintf(\"params.param : %s\", params.param)} } Select the logic required for the ConfigAdditionalUrl portion of the handler directory. The required parameters come from operations.GetAdditionalUrlParams. Additional Partial Handler Registration func configureAPI(api *operations.LoxilbRestAPIAPI) http.Handler { ...... // Change it by putting a function here api.GetAdditionalUrlHandler = operations.GetAdditionalUrlHandlerFunc(handler.ConfigAdditionalUrl) \u2026. } if api.{REST}...The Handler form is automatically generated, where if nil is erased and a handler is added to the operation function. In many cases, additional generation is not possible. In that case, you can add the function by entering it separately. The name of the function consists of a combination of Method, URL, and Parameter.","title":"API source Architecture"},{"location":"api/","text":"loxilb Web-APIs Loxilb can be fully configured using extensive list of RestAPI. Refer to the API Documentation .","title":"loxilb api-reference"},{"location":"api/#loxilb-web-apis","text":"Loxilb can be fully configured using extensive list of RestAPI. Refer to the API Documentation .","title":"loxilb Web-APIs"},{"location":"arch/","text":"loxilb architecture and modules loxilb consists of the following modules : loxilb CCM plugin It fully implements K8s CCM load-balancer interface and talks to goLang based loxilb process using Restful APIs. Although loxilb CCM is logically shown as part of loxilb cluster nodes, it will usually run in the worker/master nodes of the K8s cluster. LoxiCCM can easily be used as part of any CCM operator implementation. loxicmd loxicmd is command line tool to configure and dump loxilb information which is based on same foundation as the wildly popular kubectl tools. loxilb loxilb is a modern goLang based framework (process) which mantains information coming in from various sources e.g apiserver and populates the eBPF maps used by the loxilb eBPF kernel. It is also responsible for loading eBPF programs to the interfaces.It also acts as a client to goBGP to exchange routes based on information from loxilb CCM. Last but not the least, it will be finally responsible for maintaining HA state sync with its remote peers. Almost all serious lb implementations need to be deployed as a HA cluster. loxilb eBPF kernel eBPF kernel module implements the data-plane of loxilb which provides complete kernel bypass. It is a fully self contained and feature-rich stack able to process packets from rx to tx without invoking linux native kernel networking. goBGP Although goBGP is a separate project, loxilb has adopted and integrated with goBGP as its routing stack of choice. We also hope to develop features for this awesome project in the future. DashBoards Grafana based dashboards to provide highly dynamic insight into loxilb state. The following is a typical loxilb deployment topology (Currently HA implementation is in development) :","title":"Architecture in brief"},{"location":"arch/#loxilb-architecture-and-modules","text":"loxilb consists of the following modules : loxilb CCM plugin It fully implements K8s CCM load-balancer interface and talks to goLang based loxilb process using Restful APIs. Although loxilb CCM is logically shown as part of loxilb cluster nodes, it will usually run in the worker/master nodes of the K8s cluster. LoxiCCM can easily be used as part of any CCM operator implementation. loxicmd loxicmd is command line tool to configure and dump loxilb information which is based on same foundation as the wildly popular kubectl tools. loxilb loxilb is a modern goLang based framework (process) which mantains information coming in from various sources e.g apiserver and populates the eBPF maps used by the loxilb eBPF kernel. It is also responsible for loading eBPF programs to the interfaces.It also acts as a client to goBGP to exchange routes based on information from loxilb CCM. Last but not the least, it will be finally responsible for maintaining HA state sync with its remote peers. Almost all serious lb implementations need to be deployed as a HA cluster. loxilb eBPF kernel eBPF kernel module implements the data-plane of loxilb which provides complete kernel bypass. It is a fully self contained and feature-rich stack able to process packets from rx to tx without invoking linux native kernel networking. goBGP Although goBGP is a separate project, loxilb has adopted and integrated with goBGP as its routing stack of choice. We also hope to develop features for this awesome project in the future. DashBoards Grafana based dashboards to provide highly dynamic insight into loxilb state. The following is a typical loxilb deployment topology (Currently HA implementation is in development) :","title":"loxilb architecture and modules"},{"location":"aws-multi-az/","text":"Deploy LoxiLB with multi-AZ HA support in AWS LoxiLB supports stateful HA configuration in various cloud environments such as AWS. Especially for AWS, one can configure HA using the Floating IP pattern , together with LoxiLB . The HA configuration described in the above document has certain limitations. It could only be configured within a single Availability-Zone(AZ). The HA instances need to share the VIP of the same subnet in order to provide a single access point to users, but this configuration was so far not possible in a multi-AZ environment. This blog explains how to deploy LoxiLB in a multi-AZ environment and configure HA. Overall Scenario Two LoxiLB instances - loxilb1 and loxilb2 will be deployed in different AZs. These two loxilbs form a HA pair and operate in active-backup roles. The active loxilb1 instance is additionally assigned a secondary network interface called loxi-eni. The loxi-eni network interface has a private IP (192.168.248.254 in this setup) which is used as a secondary IP. loxilb1 associates this 192.168.248.254 secondary IP with an user-specified public ElasticIP address. When a user accesses the EKS service externally using an ElasticIP address, this traffic is NATed to the 192.168.248.254 IP and delivered to the active loxilb instance. The active loxilb instance can then load balance the traffic to the appropriate endpoint in EKS. If loxilb1 goes down due to any reason, the status of loxilb2, which was backup previously, changes to active. During this transition, loxilb2 instance is assigned a new loxil-eni secondary network interface, and the 192.168.248.254 IP used by the the original master \"loxilb1\" is set to the secondary network interface of loxilb2. The ElasticIP used by the user is also (re)associated to the 192.168.248.254 private IP address of the \"new\" active instance. This makes it possible to maintain active sessions even during failover or situations where there is a need to upgrade orginal master instance etc. To summarize, when a failover occurs the public ElasticIP address is always associated to the active LoxiLB instance, so users who were previously accessing EKS using the same ElasticIP address can continue to do so without being affected by any node failure or other issues. An example scenario We will use eksctl to create an EKS cluster. To use eksctl, we need to register authentication information through AWS CLI. Instructions for installing aws CLI & eksctl etc are omitted in this document and can be found in AWS. Using eksctl, let's create an EKS cluster with the following command. For this test, we are using AWS's Osaka region, so using ap-northeast-3 in the --region option. eksctl create cluster \\ --version 1.24 \\ --name multi-az-eks \\ --vpc-nat-mode Single \\ --region ap-northeast-3 \\ --node-type t3.medium \\ --nodes 2 \\ --with-oidc \\ --managed After running the above, we will have an EKS clsuter with two nodes named \"multi-az-eks\". Configuring LoxiLB EC2 Instances Create LoxiLB subnet After configuring EKS, it's time to configure the LoxiLB instance. Let's create the subnet that each of the LoxiLB instances will use. LoxiLB instances will be created each located in a different AZ. Therefore, the subnets to be used by the instances will also be created in different AZs: AZ-a and AZ-b. First, create a subnet loxilb-subnet-a in ap-northeast-3a with the subnet 192.168.218.0/24. Similarly, create a subnet loxilb-subnet-b in ap-northeast-3b with the subnet 192.168.228.0/24. After creating it, we can double check the \"Enable auto-assign public IPv4 address\" setting so that interfaces connected to each subnet are automatically assigned a public IP. AWS Route table Newly created subnets automatically use the default route table. We will connect the default route table to the internet gateway so that users can access the LoxiLB instance from outside. LoxiLB IAM Settings LoxiLB instances require permission to access the AWS EC2 API to associate ElasticIPs and create secondary interfaces and subnets. We will create a role with the following IAM policy for LoxiLB EC2 instances. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"*\", \"Resource\": \"*\" } ] } LoxiLB EC2 instance creation We will create two LoxiLB instances for this example and connect the instances wits subnets A and B created above. And specify to use the IAM role created above in the IAM instance profile of the Advanced details settings. After the instance is created, go to the Action \u2192 networking \u2192 Change Source /destination check menu in the instance menu and disable this check. Since LoxiLB is a load balancer, this configration must be disabled for LoxiLB to operate properly. Create Elastic IP Next we will create an Elastic IP to use to access the service from outside. For this example, the IP 13.208.x.x was assigned. The Elastic IP is used when deploying kube-loxilb, and is automatically associated to the LoxiLB master instance when configuring LoxiLB HA without any user intervention. kube-loxilb deployment kube-loxilb is a K8s operator for LoxiLB. Download the manifest file required for your deployment in EKS. wget https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/ext-cluster/kube-loxilb.yaml Change the args inside this yaml (as applicable) spec: containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: - --loxiURL=http://192.168.228.108:11111,http://192.168.218.60:11111 - --externalCIDR=13.208.X.X/32 - --privateCIDR=192.168.248.254/32 - --setRoles=0.0.0.0 - --setLBMode=2 Modify loxiURL with the IPs of the LoxiLB EC2instances created above. For externalCIDR, specify the Elastic IP created above. PrivateCIDR specifies the VIP that will be associated with the Elastic IP. As described in the scenario above, we will use 192.168.248.254 as the VIP in this article. The IP must be set within the range of the VPC CIDR and not currently part of any another subnet. Run LoxiLB Pods Install docker on LoxiLB instance(s) LoxiLB is deployed as a container on each instance. To use containers, docker must first be installed on the instance. Docker installation guide can be found here Running LoxiLB container The following command is for a LoxiLB instance (loxilb1) using subnet-a. sudo docker run -u root --cap-add SYS_ADMIN \\ --restart unless-stopped \\ --net=host \\ --privileged \\ -dit \\ -v /dev/log:/dev/log -e AWS_REGION=ap-northeast-3 \\ --name loxilb \\ ghcr.io/loxilb-io/loxilb:aws-support \\ --cloud=aws --cloudcidrblock=192.168.248.0/24 --cluster=192.168.228.108 --self=0 In the cloudcidrblock option, specify the IP band that includes the VIP set in kube-loxilb's privateCIDR. master LoxiLB uses the value set here to create a new subnet in the AZ where it is located and uses it for HA operation. The cluster option specifies the IP of the partner instance (LoxiLB instance using subnet-b) for which HA is configured. The self option is set to 0. It is just a identier used internally to identify each instance Similarily we can run loxilb2 instance in the second EC2 instance using subnet-b: sudo docker run -u root --cap-add SYS_ADMIN \\ --restart unless-stopped \\ --net=host \\ --privileged \\ -dit \\ -v /dev/log:/dev/log -e AWS_REGION=ap-northeast-3 \\ --name loxilb \\ ghcr.io/loxilb-io/loxilb:aws-support \\ --cloud=aws --cloudcidrblock=192.168.248.0/24 --cluster=192.168.218.60 --self=1 For each instance, HA status can be checked as follows: When the container runs, you can check the HA status as follows: ubuntu@ip-192-168-218-60:~$ sudo docker exec -ti loxilb bash root@ip-192-168-228-108:/# loxicmd get ha | INSTANCE | HASTATE | |----------|---------| | default | MASTER | root@ip-192-168-228-108:/# Creating a service Let's create a test service to test HA functionality. Below are the manifest files for the nginx pod and service that we will use for testing. apiVersion: v1 kind: Service metadata: name: nginx-lb1 spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: nginx-test ports: - port: 55002 targetPort: 80 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: nginx-test labels: what: nginx-test spec: containers: - name: nginx-test image: nginx:stable ports: - containerPort: 80 After creating an nginx service with the above, weu can see that the ElasticIP has been designated as the externalIP of the service. LEIS6N3:~/workspace/aws-demo$ kubectl apply -f nginx.yaml service/nginx-lb1 created pod/nginx-test created LEIS6N3:~/workspace/aws-demo$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 22h nginx-lb1 LoadBalancer 10.100.178.3 llb-13.208.X.X 55002:32403/TCP 15s We can now access the service from a host client : Testing HA functionality Once LoxiLB HA is configured, we can check in the AWS console that a secondary interface has been added to the master. To test HA operation, simply stop the LoxiLB pod in master state. ubuntu@ip-192-168-228-108:~$ sudo docker stop loxilb loxilb ubuntu@ip-192-168-228-108:~$ Even after stopping the masterLB, the service can be accessed without interruption : During failover, a secondary interface is created on the new master instance, and you can see that the ElasticIP is also associated to the new interface.","title":"Aws multi az"},{"location":"aws-multi-az/#deploy-loxilb-with-multi-az-ha-support-in-aws","text":"LoxiLB supports stateful HA configuration in various cloud environments such as AWS. Especially for AWS, one can configure HA using the Floating IP pattern , together with LoxiLB . The HA configuration described in the above document has certain limitations. It could only be configured within a single Availability-Zone(AZ). The HA instances need to share the VIP of the same subnet in order to provide a single access point to users, but this configuration was so far not possible in a multi-AZ environment. This blog explains how to deploy LoxiLB in a multi-AZ environment and configure HA.","title":"Deploy LoxiLB with multi-AZ HA support in AWS"},{"location":"aws-multi-az/#overall-scenario","text":"Two LoxiLB instances - loxilb1 and loxilb2 will be deployed in different AZs. These two loxilbs form a HA pair and operate in active-backup roles. The active loxilb1 instance is additionally assigned a secondary network interface called loxi-eni. The loxi-eni network interface has a private IP (192.168.248.254 in this setup) which is used as a secondary IP. loxilb1 associates this 192.168.248.254 secondary IP with an user-specified public ElasticIP address. When a user accesses the EKS service externally using an ElasticIP address, this traffic is NATed to the 192.168.248.254 IP and delivered to the active loxilb instance. The active loxilb instance can then load balance the traffic to the appropriate endpoint in EKS. If loxilb1 goes down due to any reason, the status of loxilb2, which was backup previously, changes to active. During this transition, loxilb2 instance is assigned a new loxil-eni secondary network interface, and the 192.168.248.254 IP used by the the original master \"loxilb1\" is set to the secondary network interface of loxilb2. The ElasticIP used by the user is also (re)associated to the 192.168.248.254 private IP address of the \"new\" active instance. This makes it possible to maintain active sessions even during failover or situations where there is a need to upgrade orginal master instance etc. To summarize, when a failover occurs the public ElasticIP address is always associated to the active LoxiLB instance, so users who were previously accessing EKS using the same ElasticIP address can continue to do so without being affected by any node failure or other issues.","title":"Overall Scenario"},{"location":"aws-multi-az/#an-example-scenario","text":"We will use eksctl to create an EKS cluster. To use eksctl, we need to register authentication information through AWS CLI. Instructions for installing aws CLI & eksctl etc are omitted in this document and can be found in AWS. Using eksctl, let's create an EKS cluster with the following command. For this test, we are using AWS's Osaka region, so using ap-northeast-3 in the --region option. eksctl create cluster \\ --version 1.24 \\ --name multi-az-eks \\ --vpc-nat-mode Single \\ --region ap-northeast-3 \\ --node-type t3.medium \\ --nodes 2 \\ --with-oidc \\ --managed After running the above, we will have an EKS clsuter with two nodes named \"multi-az-eks\".","title":"An example scenario"},{"location":"aws-multi-az/#configuring-loxilb-ec2-instances","text":"","title":"Configuring LoxiLB EC2 Instances"},{"location":"aws-multi-az/#create-loxilb-subnet","text":"After configuring EKS, it's time to configure the LoxiLB instance. Let's create the subnet that each of the LoxiLB instances will use. LoxiLB instances will be created each located in a different AZ. Therefore, the subnets to be used by the instances will also be created in different AZs: AZ-a and AZ-b. First, create a subnet loxilb-subnet-a in ap-northeast-3a with the subnet 192.168.218.0/24. Similarly, create a subnet loxilb-subnet-b in ap-northeast-3b with the subnet 192.168.228.0/24. After creating it, we can double check the \"Enable auto-assign public IPv4 address\" setting so that interfaces connected to each subnet are automatically assigned a public IP.","title":"Create LoxiLB subnet"},{"location":"aws-multi-az/#aws-route-table","text":"Newly created subnets automatically use the default route table. We will connect the default route table to the internet gateway so that users can access the LoxiLB instance from outside.","title":"AWS Route table"},{"location":"aws-multi-az/#loxilb-iam-settings","text":"LoxiLB instances require permission to access the AWS EC2 API to associate ElasticIPs and create secondary interfaces and subnets. We will create a role with the following IAM policy for LoxiLB EC2 instances. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"*\", \"Resource\": \"*\" } ] }","title":"LoxiLB IAM Settings"},{"location":"aws-multi-az/#loxilb-ec2-instance-creation","text":"We will create two LoxiLB instances for this example and connect the instances wits subnets A and B created above. And specify to use the IAM role created above in the IAM instance profile of the Advanced details settings. After the instance is created, go to the Action \u2192 networking \u2192 Change Source /destination check menu in the instance menu and disable this check. Since LoxiLB is a load balancer, this configration must be disabled for LoxiLB to operate properly.","title":"LoxiLB EC2 instance creation"},{"location":"aws-multi-az/#create-elastic-ip","text":"Next we will create an Elastic IP to use to access the service from outside. For this example, the IP 13.208.x.x was assigned. The Elastic IP is used when deploying kube-loxilb, and is automatically associated to the LoxiLB master instance when configuring LoxiLB HA without any user intervention.","title":"Create Elastic IP"},{"location":"aws-multi-az/#kube-loxilb-deployment","text":"kube-loxilb is a K8s operator for LoxiLB. Download the manifest file required for your deployment in EKS. wget https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/ext-cluster/kube-loxilb.yaml Change the args inside this yaml (as applicable) spec: containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: - --loxiURL=http://192.168.228.108:11111,http://192.168.218.60:11111 - --externalCIDR=13.208.X.X/32 - --privateCIDR=192.168.248.254/32 - --setRoles=0.0.0.0 - --setLBMode=2 Modify loxiURL with the IPs of the LoxiLB EC2instances created above. For externalCIDR, specify the Elastic IP created above. PrivateCIDR specifies the VIP that will be associated with the Elastic IP. As described in the scenario above, we will use 192.168.248.254 as the VIP in this article. The IP must be set within the range of the VPC CIDR and not currently part of any another subnet.","title":"kube-loxilb deployment"},{"location":"aws-multi-az/#run-loxilb-pods","text":"","title":"Run LoxiLB Pods"},{"location":"aws-multi-az/#install-docker-on-loxilb-instances","text":"LoxiLB is deployed as a container on each instance. To use containers, docker must first be installed on the instance. Docker installation guide can be found here","title":"Install docker on LoxiLB instance(s)"},{"location":"aws-multi-az/#running-loxilb-container","text":"The following command is for a LoxiLB instance (loxilb1) using subnet-a. sudo docker run -u root --cap-add SYS_ADMIN \\ --restart unless-stopped \\ --net=host \\ --privileged \\ -dit \\ -v /dev/log:/dev/log -e AWS_REGION=ap-northeast-3 \\ --name loxilb \\ ghcr.io/loxilb-io/loxilb:aws-support \\ --cloud=aws --cloudcidrblock=192.168.248.0/24 --cluster=192.168.228.108 --self=0 In the cloudcidrblock option, specify the IP band that includes the VIP set in kube-loxilb's privateCIDR. master LoxiLB uses the value set here to create a new subnet in the AZ where it is located and uses it for HA operation. The cluster option specifies the IP of the partner instance (LoxiLB instance using subnet-b) for which HA is configured. The self option is set to 0. It is just a identier used internally to identify each instance Similarily we can run loxilb2 instance in the second EC2 instance using subnet-b: sudo docker run -u root --cap-add SYS_ADMIN \\ --restart unless-stopped \\ --net=host \\ --privileged \\ -dit \\ -v /dev/log:/dev/log -e AWS_REGION=ap-northeast-3 \\ --name loxilb \\ ghcr.io/loxilb-io/loxilb:aws-support \\ --cloud=aws --cloudcidrblock=192.168.248.0/24 --cluster=192.168.218.60 --self=1 For each instance, HA status can be checked as follows: When the container runs, you can check the HA status as follows: ubuntu@ip-192-168-218-60:~$ sudo docker exec -ti loxilb bash root@ip-192-168-228-108:/# loxicmd get ha | INSTANCE | HASTATE | |----------|---------| | default | MASTER | root@ip-192-168-228-108:/#","title":"Running LoxiLB container"},{"location":"aws-multi-az/#creating-a-service","text":"Let's create a test service to test HA functionality. Below are the manifest files for the nginx pod and service that we will use for testing. apiVersion: v1 kind: Service metadata: name: nginx-lb1 spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: nginx-test ports: - port: 55002 targetPort: 80 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: nginx-test labels: what: nginx-test spec: containers: - name: nginx-test image: nginx:stable ports: - containerPort: 80 After creating an nginx service with the above, weu can see that the ElasticIP has been designated as the externalIP of the service. LEIS6N3:~/workspace/aws-demo$ kubectl apply -f nginx.yaml service/nginx-lb1 created pod/nginx-test created LEIS6N3:~/workspace/aws-demo$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 22h nginx-lb1 LoadBalancer 10.100.178.3 llb-13.208.X.X 55002:32403/TCP 15s We can now access the service from a host client :","title":"Creating a service"},{"location":"aws-multi-az/#testing-ha-functionality","text":"Once LoxiLB HA is configured, we can check in the AWS console that a secondary interface has been added to the master. To test HA operation, simply stop the LoxiLB pod in master state. ubuntu@ip-192-168-228-108:~$ sudo docker stop loxilb loxilb ubuntu@ip-192-168-228-108:~$ Even after stopping the masterLB, the service can be accessed without interruption : During failover, a secondary interface is created on the new master instance, and you can see that the ElasticIP is also associated to the new interface.","title":"Testing HA functionality"},{"location":"ccm/","text":"Howto - ccm plugin loxi-ccm is a cloud-manager that provides kubernetes with loxilb load balancer. kubernetes provides the cloud-provider interface for the implementation of external cloud provider-specific logic, and loxi-ccm is an implementation of the cloud-provider interface. Typical loxi-ccm deployment topology As seen in the loxilb architecture documentation , loxi-ccm is logically shown as part of the loxilb cluster. But it's actually running on the k8s master/control-plane node. loxi-ccm implements the k8s load balancer service function using RESTful API of loxilb. When a user creates a k8s load balancer type service, loxi-ccm allocates an IP from the registered External IP subnet Pool. loxi-ccm sets rules in loxilb to allow service access from external with the assigned IP. In other words, loxi-ccm needs two information. loxilb API server address External IP Subnet These informations are managed through k8s ConfigMap. loxi-ccm users should modify this informations to suit your environment. Deploy loxi-ccm on kubernetes The guide below has been tested in environment on Ubuntu 20.04, kubernetes v1.24 (calico CNI) 1. Modify k8s ConfigMap In the manifests/loxi-ccm.yaml manifests file, the ConfigMap is defined as follows --- apiVersion: v1 kind: ConfigMap metadata: name: loxilb-config namespace: kube-system data: apiServerURL: \"http://192.168.20.54:11111\" externalIPcidr: 123.123.123.0/24 --- The ConfigMap has two values: apiServerURL and externalIPcidr. apiServerURL : API Server address of loxilb. externalIPcidr : Subnet band to be allocated by loxilb as External IP of the load balancer. apiServerURL and externalIPcidr must be modified according to the environment of the user using loxi-ccm. 2. Deploy loxi-ccm Once you have modified ConfigMap, you can deploy loxi-ccm using the loxi-ccm.yaml manifest file. Run the following command on the kubernetes you want to deploy. kubectl apply -f https://github.com/loxilb-io/loxi-ccm/raw/master/manifests/loxi-ccm.yaml After entering the command, check whether loxi-cloud-controller-manager is created in the daemonset of the kube-system namespace. Manual build If you want to build loxi-ccm manually, do the following: 1. build ./build.sh 2. Build & upload container image Below is an example. This case use docker to build container images, and images is uploaded to docker hub. TAG=\"0.1\" DOCKER_ID=YOUR_DOCKER_ID sudo docker build -t $DOCKER_ID/loxi-ccm:$TAG -f ./Dockerfile . sudo docker push $DOCKER_ID/loxi-ccm:$TAG 3. create loxi-ccm daemonset using custom image In the DaemonSet section of the ./manifests/loxi-ccm.yaml file, change the image name to a custom image. (spec.template.spec.containers.image) --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: k8s-app: loxi-cloud-controller-manager name: loxi-cloud-controller-manager namespace: kube-system spec: selector: matchLabels: k8s-app: loxi-cloud-controller-manager template: metadata: labels: k8s-app: loxi-cloud-controller-manager spec: serviceAccountName: loxi-cloud-controller-manager containers: - name: loxi-cloud-controller-manager imagePullPolicy: Always # for in-tree providers we use k8s.gcr.io/cloud-controller-manager # this can be replaced with any other image for out-of-tree providers image: {DOCKER_ID}/loxi-ccm:{TAG} command: - /bin/loxi-cloud-controller-manager","title":"Howto - ccm plugin"},{"location":"ccm/#howto-ccm-plugin","text":"loxi-ccm is a cloud-manager that provides kubernetes with loxilb load balancer. kubernetes provides the cloud-provider interface for the implementation of external cloud provider-specific logic, and loxi-ccm is an implementation of the cloud-provider interface.","title":"Howto - ccm plugin"},{"location":"ccm/#typical-loxi-ccm-deployment-topology","text":"As seen in the loxilb architecture documentation , loxi-ccm is logically shown as part of the loxilb cluster. But it's actually running on the k8s master/control-plane node. loxi-ccm implements the k8s load balancer service function using RESTful API of loxilb. When a user creates a k8s load balancer type service, loxi-ccm allocates an IP from the registered External IP subnet Pool. loxi-ccm sets rules in loxilb to allow service access from external with the assigned IP. In other words, loxi-ccm needs two information. loxilb API server address External IP Subnet These informations are managed through k8s ConfigMap. loxi-ccm users should modify this informations to suit your environment.","title":"Typical loxi-ccm deployment topology"},{"location":"ccm/#deploy-loxi-ccm-on-kubernetes","text":"The guide below has been tested in environment on Ubuntu 20.04, kubernetes v1.24 (calico CNI)","title":"Deploy loxi-ccm on kubernetes"},{"location":"ccm/#1-modify-k8s-configmap","text":"In the manifests/loxi-ccm.yaml manifests file, the ConfigMap is defined as follows --- apiVersion: v1 kind: ConfigMap metadata: name: loxilb-config namespace: kube-system data: apiServerURL: \"http://192.168.20.54:11111\" externalIPcidr: 123.123.123.0/24 --- The ConfigMap has two values: apiServerURL and externalIPcidr. apiServerURL : API Server address of loxilb. externalIPcidr : Subnet band to be allocated by loxilb as External IP of the load balancer. apiServerURL and externalIPcidr must be modified according to the environment of the user using loxi-ccm.","title":"1. Modify k8s ConfigMap"},{"location":"ccm/#2-deploy-loxi-ccm","text":"Once you have modified ConfigMap, you can deploy loxi-ccm using the loxi-ccm.yaml manifest file. Run the following command on the kubernetes you want to deploy. kubectl apply -f https://github.com/loxilb-io/loxi-ccm/raw/master/manifests/loxi-ccm.yaml After entering the command, check whether loxi-cloud-controller-manager is created in the daemonset of the kube-system namespace.","title":"2. Deploy loxi-ccm"},{"location":"ccm/#manual-build","text":"If you want to build loxi-ccm manually, do the following:","title":"Manual build"},{"location":"ccm/#1-build","text":"./build.sh","title":"1. build"},{"location":"ccm/#2-build-upload-container-image","text":"Below is an example. This case use docker to build container images, and images is uploaded to docker hub. TAG=\"0.1\" DOCKER_ID=YOUR_DOCKER_ID sudo docker build -t $DOCKER_ID/loxi-ccm:$TAG -f ./Dockerfile . sudo docker push $DOCKER_ID/loxi-ccm:$TAG","title":"2. Build &amp; upload container image"},{"location":"ccm/#3-create-loxi-ccm-daemonset-using-custom-image","text":"In the DaemonSet section of the ./manifests/loxi-ccm.yaml file, change the image name to a custom image. (spec.template.spec.containers.image) --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: k8s-app: loxi-cloud-controller-manager name: loxi-cloud-controller-manager namespace: kube-system spec: selector: matchLabels: k8s-app: loxi-cloud-controller-manager template: metadata: labels: k8s-app: loxi-cloud-controller-manager spec: serviceAccountName: loxi-cloud-controller-manager containers: - name: loxi-cloud-controller-manager imagePullPolicy: Always # for in-tree providers we use k8s.gcr.io/cloud-controller-manager # this can be replaced with any other image for out-of-tree providers image: {DOCKER_ID}/loxi-ccm:{TAG} command: - /bin/loxi-cloud-controller-manager","title":"3. create loxi-ccm daemonset using custom image"},{"location":"cmd-dev/","text":"loxicmd development guide This guide should help developers extend and enhance loxicmd. The guide is divided into three main stages: design, development, and testing. Start with cloning the loxicmd git: git clone git@github.com:loxilb-io/loxicmd.git API check and command design Before developing Command, we need to check if the API of the necessary functions is provided. Check the official API document of LoxiLB to see if the required API is provided. Afterwards, the GET, POST, and DELETE methods are designed with get, create, and delete commands according to the API provided. loxicmd$ tree . \u251c\u2500\u2500 AUTHORS \u251c\u2500\u2500 cmd \u2502 \u251c\u2500\u2500 create \u2502 \u2502 \u251c\u2500\u2500 create.go \u2502 \u2502 \u2514\u2500\u2500 create_loadbalancer.go \u2502 \u251c\u2500\u2500 delete \u2502 \u2502 \u251c\u2500\u2500 delete.go \u2502 \u2502 \u2514\u2500\u2500 delete_loadbalancer.go \u2502 \u251c\u2500\u2500 get \u2502 \u2502 \u251c\u2500\u2500 get.go \u2502 \u2502 \u251c\u2500\u2500 get_loadbalancer.go \u2502 \u2514\u2500\u2500 root.go \u251c\u2500\u2500 go.mod \u251c\u2500\u2500 go.sum \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 main.go \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 pkg \u2502 \u2514\u2500\u2500 api \u2502 \u251c\u2500\u2500 client.go \u2502 \u251c\u2500\u2500 common.go \u2502 \u251c\u2500\u2500 loadBalancer.go \u2502 \u2514\u2500\u2500 rest.go \u2514\u2500\u2500 README.md Add the code in the ./cmd/get, ./cmd/delete, ./cmd/create, and ./pkg/api directories to add functionality. Add structure in pkg/api and register method (example of connection track API) CommonAPI embedding Using embedding the CommonAPI for the Methods and variables, to use in the Connecttrack structure. type Conntrack struct { CommonAPI } Add Structure Configuration and JSON Structure Define the structure for JSON Unmashal. type CtInformationGet struct { CtInfo []ConntrackInformation `json:\"ctAttr\"` } type ConntrackInformation struct { Dip string `json:\"destinationIP\"` Sip string `json:\"sourceIP\"` Dport uint16 `json:\"destinationPort\"` Sport uint16 `json:\"sourcePort\"` Proto string `json:\"protocol\"` CState string `json:\"conntrackState\"` CAct string `json:\"conntrackAct\"` } Define Method Functions in pkg/api/client.go Define the URL in the Resource constant. Defines the function to be used in the command. const ( \u2026 loxiConntrackResource = \"config/conntrack/all\" ) func (l *LoxiClient) Conntrack() *Conntrack { return &Conntrack{ CommonAPI: CommonAPI{ restClient: &l.restClient, requestInfo: RequestInfo{ provider: loxiProvider, apiVersion: loxiApiVersion, resource: loxiConntrackResource, }, }, } } Add get, create, delete functions within cmd Use the Cobra library to define commands, Alise, descriptions, options, and callback functions, and then create a function that returns. Create a function such as PrintGetCTReturn and add logic when the status code is 200. func NewGetConntrackCmd(restOptions *api.RESTOptions) *cobra.Command { var GetctCmd = &cobra.Command{ Use: \"conntrack\", Aliases: []string{\"ct\", \"conntracks\", \"cts\"}, Short: \"Get a Conntrack\", Long: `It shows connection track Information`, Run: func(cmd *cobra.Command, args []string) { client := api.NewLoxiClient(restOptions) ctx := context.TODO() var cancel context.CancelFunc if restOptions.Timeout > 0 { ctx, cancel = context.WithTimeout(context.TODO(), time.Duration(restOptions.Timeout)*time.Second) defer cancel() } resp, err := client.Conntrack().Get(ctx) if err != nil { fmt.Printf(\"Error: %s\\n\", err.Error()) return } if resp.StatusCode == http.StatusOK { PrintGetCTResult(resp, *restOptions) return } }, } return GetctCmd } Register command in cmd Register Cobra as defined in 3. func GetCmd(restOptions *api.RESTOptions) *cobra.Command { var GetCmd = &cobra.Command{ Use: \"get\", Short: \"A brief description of your command\", Long: `A longer description that spans multiple lines and likely contains examples and usage of using your command. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly Get a Cobra application.`, Run: func(cmd *cobra.Command, args []string) { fmt.Println(\"Get called\") }, } GetCmd.AddCommand(NewGetLoadBalancerCmd(restOptions)) GetCmd.AddCommand(NewGetConntrackCmd(restOptions)) return GetCmd } Build & Test make","title":"Developer's guide to loxicmd"},{"location":"cmd-dev/#loxicmd-development-guide","text":"This guide should help developers extend and enhance loxicmd. The guide is divided into three main stages: design, development, and testing. Start with cloning the loxicmd git: git clone git@github.com:loxilb-io/loxicmd.git","title":"loxicmd development guide"},{"location":"cmd-dev/#api-check-and-command-design","text":"Before developing Command, we need to check if the API of the necessary functions is provided. Check the official API document of LoxiLB to see if the required API is provided. Afterwards, the GET, POST, and DELETE methods are designed with get, create, and delete commands according to the API provided. loxicmd$ tree . \u251c\u2500\u2500 AUTHORS \u251c\u2500\u2500 cmd \u2502 \u251c\u2500\u2500 create \u2502 \u2502 \u251c\u2500\u2500 create.go \u2502 \u2502 \u2514\u2500\u2500 create_loadbalancer.go \u2502 \u251c\u2500\u2500 delete \u2502 \u2502 \u251c\u2500\u2500 delete.go \u2502 \u2502 \u2514\u2500\u2500 delete_loadbalancer.go \u2502 \u251c\u2500\u2500 get \u2502 \u2502 \u251c\u2500\u2500 get.go \u2502 \u2502 \u251c\u2500\u2500 get_loadbalancer.go \u2502 \u2514\u2500\u2500 root.go \u251c\u2500\u2500 go.mod \u251c\u2500\u2500 go.sum \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 main.go \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 pkg \u2502 \u2514\u2500\u2500 api \u2502 \u251c\u2500\u2500 client.go \u2502 \u251c\u2500\u2500 common.go \u2502 \u251c\u2500\u2500 loadBalancer.go \u2502 \u2514\u2500\u2500 rest.go \u2514\u2500\u2500 README.md Add the code in the ./cmd/get, ./cmd/delete, ./cmd/create, and ./pkg/api directories to add functionality.","title":"API check and command design"},{"location":"cmd-dev/#add-structure-in-pkgapi-and-register-method-example-of-connection-track-api","text":"CommonAPI embedding Using embedding the CommonAPI for the Methods and variables, to use in the Connecttrack structure. type Conntrack struct { CommonAPI } Add Structure Configuration and JSON Structure Define the structure for JSON Unmashal. type CtInformationGet struct { CtInfo []ConntrackInformation `json:\"ctAttr\"` } type ConntrackInformation struct { Dip string `json:\"destinationIP\"` Sip string `json:\"sourceIP\"` Dport uint16 `json:\"destinationPort\"` Sport uint16 `json:\"sourcePort\"` Proto string `json:\"protocol\"` CState string `json:\"conntrackState\"` CAct string `json:\"conntrackAct\"` } Define Method Functions in pkg/api/client.go Define the URL in the Resource constant. Defines the function to be used in the command. const ( \u2026 loxiConntrackResource = \"config/conntrack/all\" ) func (l *LoxiClient) Conntrack() *Conntrack { return &Conntrack{ CommonAPI: CommonAPI{ restClient: &l.restClient, requestInfo: RequestInfo{ provider: loxiProvider, apiVersion: loxiApiVersion, resource: loxiConntrackResource, }, }, } }","title":"Add structure in pkg/api and register method (example of connection track API)"},{"location":"cmd-dev/#add-get-create-delete-functions-within-cmd","text":"Use the Cobra library to define commands, Alise, descriptions, options, and callback functions, and then create a function that returns. Create a function such as PrintGetCTReturn and add logic when the status code is 200. func NewGetConntrackCmd(restOptions *api.RESTOptions) *cobra.Command { var GetctCmd = &cobra.Command{ Use: \"conntrack\", Aliases: []string{\"ct\", \"conntracks\", \"cts\"}, Short: \"Get a Conntrack\", Long: `It shows connection track Information`, Run: func(cmd *cobra.Command, args []string) { client := api.NewLoxiClient(restOptions) ctx := context.TODO() var cancel context.CancelFunc if restOptions.Timeout > 0 { ctx, cancel = context.WithTimeout(context.TODO(), time.Duration(restOptions.Timeout)*time.Second) defer cancel() } resp, err := client.Conntrack().Get(ctx) if err != nil { fmt.Printf(\"Error: %s\\n\", err.Error()) return } if resp.StatusCode == http.StatusOK { PrintGetCTResult(resp, *restOptions) return } }, } return GetctCmd }","title":"Add get, create, delete functions within cmd"},{"location":"cmd-dev/#register-command-in-cmd","text":"Register Cobra as defined in 3. func GetCmd(restOptions *api.RESTOptions) *cobra.Command { var GetCmd = &cobra.Command{ Use: \"get\", Short: \"A brief description of your command\", Long: `A longer description that spans multiple lines and likely contains examples and usage of using your command. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly Get a Cobra application.`, Run: func(cmd *cobra.Command, args []string) { fmt.Println(\"Get called\") }, } GetCmd.AddCommand(NewGetLoadBalancerCmd(restOptions)) GetCmd.AddCommand(NewGetConntrackCmd(restOptions)) return GetCmd }","title":"Register command in cmd"},{"location":"cmd-dev/#build-test","text":"make","title":"Build &amp; Test"},{"location":"cmd/","text":"Table of Contents What is loxicmd How to build How to run and configure loxilb Load balancer Endpoint BFD Session SessionUlCl IPaddress FDB Route Neighbor Vlan Vxlan Firewall Mirror Policy Session Recorder What is loxicmd loxicmd is command tool for loxilb's configuration. loxicmd aims to provide all configuation related to loxilb and is based on kubectl's look and feel. When running k8s, kube-loxilb usually takes care of loxilb configuration but nonetheless loxicmd can be used for enhanced config, debugging and observability. How to build Note - loxilb docker has this built-in and there is no need to build it when using loxilb docker Install package dependencies go get . Make loxicmd make Install loxicmd sudo cp -f ./loxicmd /usr/local/sbin How to run and configure loxilb Load Balancer Get load-balancer rules Get basic information loxicmd get lb Get detailed information loxicmd get lb -o wide Get info in json loxicmd get lb -o json Configure load-balancer rule Simple NAT44 tcp (round-robin) load-balancer loxicmd create lb 1.1.1.1 --tcp=1828:1920 --endpoints=2.2.3.4:1 Note: - Round-robin is default mode in loxilb - End-point format is specified as <CIDR:weight>. For round-robin, weight(1) has no significance. NAT66 (round-robin) load-balancer loxicmd create lb 2001::1 --tcp=2020:8080 --endpoints=4ffe::1:1,5ffe::1:1,6ffe::1:1 NAT64 (round-robin) load-balancer loxicmd create lb 2001::1 --tcp=2020:8080 --endpoints=31.31.31.1:1,32.32.32.1:1,33.33.33.1:1 WRR (Weighted round-robin) load-balancer (Divide traffic in 40%, 40% and 20% ratio among end-points) loxicmd create lb 20.20.20.1 --select=priority --tcp=2020:8080 --endpoints=31.31.31.1:40,32.32.32.1:40,33.33.33.1:20 Sticky end-point selection load-balancer (select end-points based on traffic hash) loxicmd create lb 20.20.20.1 --select=hash --tcp=2020:8080 --endpoints=31.31.31.1:1,32.32.32.1:1,33.33.33.1:1 Load-balancer with forceful tcp-reset session timeout after inactivity of 30s loxicmd create lb 20.20.20.1 --tcp=2020:8080 --endpoints=31.31.31.1:1,32.32.32.1:1,33.33.33.1:1 --inatimeout=30 Load-balancer with one-arm mode loxicmd create lb 20.20.20.1 --tcp=2020:8080 --endpoints=100.100.100.2:1,100.100.100.3:1,100.100.100.4:1 --mode=onearm Load-balancer with fullnat mode loxicmd create lb 88.88.88.1 --sctp=38412:38412 --endpoints=192.168.70.3:1 --mode=fullnat - For more information on one-arm and full-nat mode, please check this post Load-balancer config in DSR(direct-server return) mode loxicmd create lb 20.20.20.1 --tcp=2020:8080 --endpoints=31.31.31.1:1,32.32.32.1:1 --mode=dsr Load-balancer config with active endpoint monitoring loxicmd create lb 20.20.20.1 --tcp=2020:8080 --endpoints=31.31.31.1:1,32.32.32.1:1 --monitor Note: - By default loxilb does not do active endpoint monitoring i.e it will continue to select end-points which might be inactive - This is due to the fact kubernetes also has its own service monitoring mechanism and it can notify loxilb of any such endpoint health state - Based on user's requirements, one can specify active endpoint checks using \"--monitor\" flag - loxilb has extensive endpoint monitoring methods. Further details can be found in endpoint section Load-balancer yaml example apiVersion: netlox/v1 kind: Loadbalancer metadata: name: test spec: serviceArguments: externalIP: 1.2.3.1 port: 80 protocol: tcp sel: 0 endpoints: - endpointIP: 4.3.2.1 weight: 1 targetPort: 8080 - endpointIP: 4.3.2.2 weight: 1 targetPort: 8080 - endpointIP: 4.3.2.3 weight: 1 targetPort: 8080 Delete a load-balancer rule loxicmd delete lb 1.1.1.1 --tcp=1828 Endpoint Get load-balancer end-point health information loxicmd get ep Create end-point for health probing # loxicmd create endpoint IP [--name=<id>] [--probetype=<probetype>] [--probereq=<probereq>] [--proberesp=<proberesp>] [--probeport=<port>] [--period=<period>] [--retries=<retries>] loxicmd create endpoint 32.32.32.1 --probetype=http --probeport=8080 --period=60 --retries=2 IP(string) : Endpoint target IPaddress name(string) : Endpoint Identifier probetype(string): Probe-type:ping,http,https,udp,tcp,sctp,none probereq(string): If probe is http/https, one can specify additional uri path proberesp(string): If probe is http/https, one can specify custom response string probeport(int): If probe is http,https,tcp,udp,sctp one can specify custom l4port to use period(int): Period of probing retries(int): Number of retries before marking endPoint inactive Notes: - \"name\" is not required when endpoint is created initially. loxilb will allocate the name which can be checked with \"loxicmd get ep\". \"name\" can be given as an Identifier when user wants to modify endpoint probe parameters - Initial state of endpoint will be decided within 15 seconds of rule addition (We cant be sure if service is immediately up so this is the init liveness check timeout. It is not configurable at this time) - After init liveness check, probes will be done as per default (60s) or whatever value is set by the user - When endpoint is inactive we have internal logic and timeouts to minimize blocking calls and maintain stability. Only when endpoint is active, we use probe timeout given by user - For UDP end-points and probe-type, there are two ways to check end-point health currently: - If the service can respond to probe requests with pre-defined responses sent over UDP, we can use the following : loxicmd create endpoint 172.1.217.133 --name=\"udpep1\" --probetype=udp --probeport=32031 --period=60 --retries=2 --probereq=\"probe\" --proberesp=\"hello\" - If the services cannot support the above mechanism, loxilb will try to check for \"ICMP Port unreachable\" after sending UDP probes. If an \"ICMP Port unreachable\" is received, it means the endpoint is not up. Examples : loxicmd create endpoint 32.32.32.1 --probetype=http --probeport=8080 --period=60 --retries=2 loxicmd get ep | HOST | NAME | PTYPE | PORT | DURATION | RETRIES | MINDELAY | AVGDELAY | MAXDELAY | STATE | |------------|----------------------|-------|------|----------|---------|----------|----------|----------|-------| | 32.32.32.1 | 32.32.32.1_http_8080 | http: | 8080 | 60 | 2 | 0s | 0s | 0s | ok | # Modify duration and retry count using name loxicmd create endpoint 32.32.32.1 --name=32.32.32.1_http_8080 --probetype=http --probeport=8080 --period=40 --retries=4 loxicmd get ep | HOST | NAME | PTYPE | PORT | DURATION | RETRIES | MINDELAY | AVGDELAY | MAXDELAY | STATE | |------------|----------------------|-------|------|----------|---------|----------|-----------|-----------|-------| | 32.32.32.1 | 32.32.32.1_http_8080 | http: | 8080 | 40 | 4 | 0s | 0s | 0s | ok | Create end-point with https probing information # loxicmd create endpoint IP [--name=<id>] [--probetype=<probetype>] [--probereq=<probereq>] [--proberesp=<proberesp>] [--probeport=<port>] [--period=<period>] [--retries=<retries>] loxicmd create endpoint 32.32.32.1 --probetype=https --probeport=8080 --probereq=\"health\" --proberesp=\"OK\" --period=60 --retries=2 Note: loxilb requires CA certificate for TLS connection and private certificate and private key for mTLS connection. Admin can keep a common(default) CA certificate for all the endpoints at \"/opt/loxilb/cert/rootCA.crt\" or per-endpoint certificates can be kept as \"/opt/loxilb/cert/\\<IP>/rootCA.crt\", private key must be kept at \"/opt/loxilb/cert/server.key\" and private certificate at \"/opt/loxilb/cert/server.crt\". Please see Minica or Certstrap or this CICD test case to know how to generate certificates. Endpoint yaml example apiVersion: netlox/v1 kind: Endpoint metadata: name: test spec: hostName: \"Test\" description: string inactiveReTries: 0 probeType: string probeReqUrl: string probeDuration: 0 probePort: 0 Delete end-point informtion loxicmd delete endpoint 31.31.31.31 BFD Get BFD Session information loxicmd get bfd Create BFD Session #loxicmd create bfd <remoteIP> --sourceIP=<sourceIP> --interval=<time in usecs> --retryCount=<count> loxicmd create bfd 192.168.80.253 --sourceIP=192.168.80.252 --interval=500000 --retryCount=3 remoteIP(string): Remote IP address sourceIP(string): Source IP address for binding interval(int): BFD packet Tx Interval Time in microseconds retryCount(int): Number of retry counts to detect failure. Set BFD Session #loxicmd set bfd <remoteIP> --interval=<time in usecs> --retryCount=<count> loxicmd set bfd 192.168.80.253 --interval=400000 --retryCount=5 interval(int): BFD packet Tx Interval Time in microseconds retryCount(int): Number of retry counts to detect failure. Delete BFD Session #loxicmd delete bfd <remoteIP> loxicmd delete bfd 192.168.80.253 remoteIP(string): Remote IP address sourceIP(string): Source IP address for binding interval(int): BFD packet Tx Interval Time in microseconds retryCount(int): Number of retry counts to detect failure. BFD yaml example apiVersion: netlox/v1 kind: BFD metadata: name: test spec: instance: \"default\" remoteIp: \"192.168.80.253\" sourceIp: \"192.168.80.252\" interval: 300000 retryCount: 4 Session Get Session information loxicmd get session Create Session information #loxicmd create session <userID> <sessionIP> --accessNetworkTunnel=<TeID>:<TunnelIP> --coreNetworkTunnel=<TeID>:<TunnelIP> loxicmd create session user1 192.168.20.1 --accessNetworkTunnel=1:1.232.16.1 coreNetworkTunnel=1:1.233.16.1 userID(string): User Identifier sessionIP(string): Session IP address accessNetworkTunnel(string): accessNetworkTunnel has pairs that can be specified as 'TeID:IP' coreNetworkTunnel(string): coreNetworkTunnel has pairs that can be specified as 'TeID:IP' Session yaml example apiVersion: netlox/v1 kind: Session metadata: name: test spec: ident: user1 sessionIP: 88.88.88.88 accessNetworkTunnel: TeID: 1 tunnelIP: 11.11.11.11 coreNetworkTunnel: TeID: 1 tunnelIP: 22.22.22.22 Delete Session information loxicmd delete session user1 SessionUlCl Get SessionUlCl information loxicmd get sessionulcl Create SessionUlCl information #loxicmd create sessionulcl <userID> --ulclArgs=<QFI>:<ulclIP>,... loxicmd create sessionulcl user1 --ulclArgs=16:192.33.125.1 userID(string): User Identifier ulclArgs(string): Port pairs can be specified as 'QFI:UlClIP' SessionUlCl yaml example apiVersion: netlox/v1 kind: SessionULCL metadata: name: test spec: ulclIdent: user1 ulclArgument: qfi: 11 ulclIP: 8.8.8.8 Delete SessionUlCl information loxicmd delete sessionulcl --ulclArgs=192.33.125.1 ulclArgs(string): UlCl IP address can be specified as 'UlClIP'. It don't need QFI. IPaddress Get IPaddress information loxicmd get ip Create IPaddress information #loxicmd create ip <DeviceIPNet> <device> loxicmd create ip 192.168.0.1/24 eno7 DeviceIPNet(string): Actual IP address with mask device(string): name of the related device IPaddress yaml example apiVersion: netlox/v1 kind: IPaddress metadata: name: test spec: dev: eno8 ipAddress: 192.168.23.1/32 Delete IPaddress information #loxicmd delete ip <DeviceIPNet> <device> loxicmd delete ip 192.168.0.1/24 eno7 FDB Get FDB information loxicmd get fdb Create FDB information #loxicmd create fdb <MacAddress> <DeviceName> loxicmd create fdb aa:aa:aa:aa:bb:bb eno7 MacAddress(string): mac address DeviceName(string): name of the related device FDB yaml example apiVersion: netlox/v1 kind: FDB metadata: name: test spec: dev: eno8 macAddress: aa:aa:aa:aa:aa:aa Delete FDB information #loxicmd delete fdb <MacAddress> <DeviceName> loxicmd delete fdb aa:aa:aa:aa:bb:bb eno7 Route Get Route information loxicmd get route Create Route information #loxicmd create route <DestinationIPNet> <gateway> loxicmd create route 192.168.212.0/24 172.17.0.254 DestinationIPNet(string): Actual IP address route with mask gateway(string): gateway information if any Route yaml example apiVersion: netlox/v1 kind: Route metadata: name: test spec: destinationIPNet: 192.168.30.0/24 gateway: 172.17.0.1 Delete Route information #loxicmd delete route <DestinationIPNet> loxicmd delete route 192.168.212.0/24 Neighbor Get Neighbor information loxicmd get neighbor Create Neighbor information #loxicmd create neighbor <DeviceIP> <DeviceName> [--macAddress=aa:aa:aa:aa:aa:aa] loxicmd create neighbor 192.168.0.1 eno7 --macAddress=aa:aa:aa:aa:aa:aa DeviceIP(string): The IP address DeviceName(string): name of the related device macAddress(string): resolved hardware address if any Neighbor yaml example apiVersion: netlox/v1 kind: Neighbor metadata: name: test spec: dev: eno8 macAddress: aa:aa:aa:aa:aa:aa ipAddress: 192.168.23.21 Delete Neighbor information #loxicmd delete neighbor <DeviceIP> <device> loxicmd delete neighbor 192.168.0.1 eno7 Vlan Get Vlan and Vlan Member information loxicmd get vlan loxicmd get vlanmember Create Vlan and Vlan Member information #loxicmd create vlan <Vid> loxicmd create vlan 100 Vid(int): vlan identifier #loxicmd create vlanmember <Vid> <DeviceName> --tagged=<Tagged> loxicmd create vlanmember 100 eno7 --tagged=true loxicmd create vlanmember 100 eno7 Vid(int): vlan identifier DeviceName(string): name of the related device tagged(boolean): tagged or not (default is false) Vlan yaml example apiVersion: netlox/v1 kind: Vlan metadata: name: test spec: vid: 100 Vlan Member yaml example apiVersion: netlox/v1 kind: VlanMember metadata: name: test vid: 100 spec: dev: eno8 Tagged: true Delete Vlan and Vlan Member information #loxicmd delete vlan <Vid> loxicmd delete vlan 100 #loxicmd delete vlanmember <Vid> <DeviceName> --tagged=<Tagged> loxicmd delete vlanmember 100 eno7 --tagged=true loxicmd delete vlanmember 100 eno7 Vxlan Get Vxlan and Vxlan Peer information loxicmd get vxlan loxicmd get vxlanpeer Create Vxlan and Vxlan Peer information #loxicmd create vxlan <VxlanID> <EndpointDeviceName> loxicmd create vxlan 100 eno7 VxlanID(int): Vxlan Identifier EndpointDeviceName(string): VTEP Device name(It must have own IP address for peering) #loxicmd create vxlanpeer <VxlanID> <PeerIP> loxicmd create vxlan-peer 100 30.1.3.1 VxlanID(int): Vxlan Identifier PeerIP(string): Vxlan peer device IP address Vxlan yaml example apiVersion: netlox/v1 kind: Vxlan metadata: name: test spec: epIntf: eno8 vxlanID: 100 Vxlan Peer yaml example apiVersion: netlox/v1 kind: VxlanPeer metadata: name: test vxlanID: 100 spec: peerIP: 21.21.21.1 Delete Vxlan and Vxlan Peer information #loxicmd delete vxlan <VxlanID> loxicmd delete vxlan 100 #loxicmd delete vxlanpeer <VxlanID> <PeerIP> loxicmd delete vxlan-peer 100 30.1.3.1 Firewall Get Firewall information loxicmd get firewall Create Firewall information #loxicmd create firewall --firewallRule=<ruleKey>:<ruleValue>, [--allow] [--drop] [--trap] [--redirect=<PortName>] [--setmark=<FwMark> loxicmd create firewall --firewallRule=\"sourceIP:1.2.3.2/32,destinationIP:2.3.1.2/32,preference:200\" --allow loxicmd create firewall --firewallRule=\"sourceIP:1.2.3.2/32,destinationIP:2.3.1.2/32,preference:200\" --allow --setmark=10 loxicmd create firewall --firewallRule=\"sourceIP:1.2.3.2/32,destinationIP:2.3.1.2/32,preference:200\" --drop loxicmd create firewall --firewallRule=\"sourceIP:1.2.3.2/32,destinationIP:2.3.1.2/32,preference:200\" --trap loxicmd create firewall --firewallRule=\"sourceIP:1.2.3.2/32,destinationIP:2.3.1.2/32,preference:200\" --redirect=ensp0 firewallRule sourceIP(string) - Source IP in CIDR notation destinationIP(string) - Destination IP in CIDR notation minSourcePort(int) - Minimum source port range maxSourcePort(int) - Maximum source port range minDestinationPort(int) - Minimum destination port range maxDestinationPort(int) - Maximum source port range protocol(int) - the protocol portName(string) - the incoming port preference(int) - User preference for ordering Firewall yaml example apiVersion: netlox/v1 kind: Firewall metadata: name: test spec: ruleArguments: sourceIP: 192.169.1.2/24 destinationIP: 192.169.2.1/24 preference: 200 opts: allow: true Delete Firewall information #loxicmd delete firewall --firewallRule=<ruleKey>:<ruleValue> loxicmd delete firewall --firewallRule=\"sourceIP:1.2.3.2/32,destinationIP:2.3.1.2/32,preference:200 Mirror Get Mirror information loxicmd get mirror Create Mirror information #loxicmd create mirror <mirrorIdent> --mirrorInfo=<InfoOption>:<InfoValue>,... --targetObject=attachement:<port1,rule2>,mirrObjName:<ObjectName> loxicmd create mirror mirr-1 --mirrorInfo=\"type:0,port:ensp0\" --targetObject=\"attachement:1,mirrObjName:ensp1 mirrorIdent(string): Mirror identifier type(int) : Mirroring type as like 0 == SPAN, 1 == RSPAN, 2 == ERSPAN port(string) : The port where mirrored traffic needs to be sent vlan(int) : for RSPAN we may need to send tagged mirror traffic remoteIP(string) : For ERSPAN we may need to send tunnelled mirror traffic sourceIP(string): For ERSPAN we may need to send tunnelled mirror traffic tunnelID(int): For ERSPAN we may need to send tunnelled mirror traffic Mirror yaml example apiVersion: netlox/v1 kind: Mirror metadata: name: test spec: mirrorIdent: mirr-1 mirrorInfo: type: 0 port: eno1 targetObject: attachment: 1 mirrObjName: eno2 Delete Mirror information #loxicmd delete mirror <mirrorIdent> loxicmd delete mirror mirr-1 Policy Get Policy information loxicmd get policy Create Policy information #loxicmd create policy IDENT --rate=<Peak>:<Commited> --target=<ObjectName>:<Attachment> [--block-size=<Excess>:<Committed>] [--color] [--pol-type=<policy type>] loxicmd create policy pol-0 --rate=100:100 --target=ensp0:1 loxicmd create policy pol-1 --rate=100:100 --target=ensp0:1 --block-size=12000:6000 loxicmd create policy pol-1 --rate=100:100 --target=ensp0:1 --color loxicmd create policy pol-1 --rate=100:100 --target=ensp0:1 --color --pol-type 0 rate(string): Rate pairs can be specified as 'Peak:Commited'. rate unit : Mbps block-size(string): Block Size pairs can be specified as 'Excess:Committed'. block-size unit : bps target(string): Target Interface pairs can be specified as 'ObjectName:Attachment' color(boolean): Policy color enbale or not pol-type(int): Policy traffic control type. 0 : TrTCM, 1 : SrTCM Policy yaml example apiVersion: netlox/v1 kind: Policy metadata: name: test spec: policyIdent: pol-eno8 policyInfo: type: 0 colorAware: false committedInfoRate: 100 peakInfoRate: 100 targetObject: attachment: 1 polObjName: eno8 Delete Policy information #loxicmd delete policy <Polident> loxicmd delete policy pol-1 Session Recorder Set n-tuple policy for recording loxicmd create firewall --firewallRule=\"destinationIP:31.31.31.0/24,preference:200\" --allow --record loxilb will record any connection track entry which matches this policy (even for reverse direction) as a way to provide extended visibility for debugging Check or record with tcpdump tcpdump -i llb0 -n Any valid tcpdump option can be used including saving to a pcap file Get live connection-track information loxicmd get conntrack Get port-dump information loxicmd get port Save all loxilb's operational information in DBStore loxicmd save -a ** This will ensure that whenever loxilb restarts, it will start with last saved state from DBStore Configure loxicmd with yaml(Beta) The loxicmd support yaml based configuration. The format is same as Kubernetes. This beta version support only one configuraion per one file. That means \"Do not use --- in yaml file.\" . It will be supported at next release. Command #loxicmd apply -f <file.yaml> #loxicmd delete -f <file.yaml> loxicmd apply -f lb.yaml loxicmd delete -f lb.yaml File example(lb.yaml) apiVersion: netlox/v1 kind: Loadbalancer metadata: name: load spec: serviceArguments: externalIP: 123.123.123.1 port: 80 protocol: tcp sel: 0 endpoints: - endpointIP: 4.3.2.1 weight: 1 targetPort: 8080 - endpointIP: 4.3.2.2 weight: 1 targetPort: 8080 - endpointIP: 4.3.2.3 weight: 1 targetPort: 8080 It reuse API's json body as a \"Spec\". If the API URL has no param, it don't need to use \"metadata\". For example, The body of load Balancer rule is shown below. { \"serviceArguments\": { \"externalIP\": \"123.123.123.1\", \"port\": 80, \"protocol\": \"tcp\", \"sel\": 0 }, \"endpoints\": [ { \"endpointIP\": \"4.3.2.1\", \"weight\": 1, \"targetPort\": 8080 }, { \"endpointIP\": \"4.3.2.2\", \"weight\": 1, \"targetPort\": 8080 }, { \"endpointIP\": \"4.3.2.3\", \"weight\": 1, \"targetPort\": 8080 } } This json format can be converted Yaml format as shown below. serviceArguments: externalIP: 123.123.123.1 port: 80 protocol: tcp sel: 0 endpoints: - endpointIP: 4.3.2.1 weight: 1 targetPort: 8080 - endpointIP: 4.3.2.2 weight: 1 targetPort: 8080 - endpointIP: 4.3.2.3 weight: 1 targetPort: 8080 Finally, this is located in the Spec of the entire configuration file as File example(lb.yaml) If you want to add Vlan bridge, IPaddress or something else. Just change the Kind value from Loadbalancer to VlanBridge, IPaddress as like below example. apiVersion: netlox/v1 kind: IPaddress metadata: name: test spec: dev: eno8 ipAddress: 192.168.23.1/32 If the URL has param such as adding vlan-member, it must have metadata . apiVersion: netlox/v1 kind: VlanMember metadata: name: test vid: 100 spec: dev: eno8 Tagged: true The example of all the settings below, so please refer to it. More information There are tons of other commands, use help option! loxicmd help","title":"Table of Contents"},{"location":"cmd/#table-of-contents","text":"What is loxicmd How to build How to run and configure loxilb Load balancer Endpoint BFD Session SessionUlCl IPaddress FDB Route Neighbor Vlan Vxlan Firewall Mirror Policy Session Recorder","title":"Table of Contents"},{"location":"cmd/#what-is-loxicmd","text":"loxicmd is command tool for loxilb's configuration. loxicmd aims to provide all configuation related to loxilb and is based on kubectl's look and feel. When running k8s, kube-loxilb usually takes care of loxilb configuration but nonetheless loxicmd can be used for enhanced config, debugging and observability.","title":"What is loxicmd"},{"location":"cmd/#how-to-build","text":"Note - loxilb docker has this built-in and there is no need to build it when using loxilb docker Install package dependencies go get . Make loxicmd make Install loxicmd sudo cp -f ./loxicmd /usr/local/sbin","title":"How to build"},{"location":"cmd/#how-to-run-and-configure-loxilb","text":"","title":"How to run and configure loxilb"},{"location":"cmd/#load-balancer","text":"","title":"Load Balancer"},{"location":"cmd/#get-load-balancer-rules","text":"Get basic information loxicmd get lb Get detailed information loxicmd get lb -o wide Get info in json loxicmd get lb -o json","title":"Get load-balancer rules"},{"location":"cmd/#configure-load-balancer-rule","text":"Simple NAT44 tcp (round-robin) load-balancer loxicmd create lb 1.1.1.1 --tcp=1828:1920 --endpoints=2.2.3.4:1 Note: - Round-robin is default mode in loxilb - End-point format is specified as <CIDR:weight>. For round-robin, weight(1) has no significance. NAT66 (round-robin) load-balancer loxicmd create lb 2001::1 --tcp=2020:8080 --endpoints=4ffe::1:1,5ffe::1:1,6ffe::1:1 NAT64 (round-robin) load-balancer loxicmd create lb 2001::1 --tcp=2020:8080 --endpoints=31.31.31.1:1,32.32.32.1:1,33.33.33.1:1 WRR (Weighted round-robin) load-balancer (Divide traffic in 40%, 40% and 20% ratio among end-points) loxicmd create lb 20.20.20.1 --select=priority --tcp=2020:8080 --endpoints=31.31.31.1:40,32.32.32.1:40,33.33.33.1:20 Sticky end-point selection load-balancer (select end-points based on traffic hash) loxicmd create lb 20.20.20.1 --select=hash --tcp=2020:8080 --endpoints=31.31.31.1:1,32.32.32.1:1,33.33.33.1:1 Load-balancer with forceful tcp-reset session timeout after inactivity of 30s loxicmd create lb 20.20.20.1 --tcp=2020:8080 --endpoints=31.31.31.1:1,32.32.32.1:1,33.33.33.1:1 --inatimeout=30 Load-balancer with one-arm mode loxicmd create lb 20.20.20.1 --tcp=2020:8080 --endpoints=100.100.100.2:1,100.100.100.3:1,100.100.100.4:1 --mode=onearm Load-balancer with fullnat mode loxicmd create lb 88.88.88.1 --sctp=38412:38412 --endpoints=192.168.70.3:1 --mode=fullnat - For more information on one-arm and full-nat mode, please check this post Load-balancer config in DSR(direct-server return) mode loxicmd create lb 20.20.20.1 --tcp=2020:8080 --endpoints=31.31.31.1:1,32.32.32.1:1 --mode=dsr Load-balancer config with active endpoint monitoring loxicmd create lb 20.20.20.1 --tcp=2020:8080 --endpoints=31.31.31.1:1,32.32.32.1:1 --monitor Note: - By default loxilb does not do active endpoint monitoring i.e it will continue to select end-points which might be inactive - This is due to the fact kubernetes also has its own service monitoring mechanism and it can notify loxilb of any such endpoint health state - Based on user's requirements, one can specify active endpoint checks using \"--monitor\" flag - loxilb has extensive endpoint monitoring methods. Further details can be found in endpoint section","title":"Configure load-balancer rule"},{"location":"cmd/#load-balancer-yaml-example","text":"apiVersion: netlox/v1 kind: Loadbalancer metadata: name: test spec: serviceArguments: externalIP: 1.2.3.1 port: 80 protocol: tcp sel: 0 endpoints: - endpointIP: 4.3.2.1 weight: 1 targetPort: 8080 - endpointIP: 4.3.2.2 weight: 1 targetPort: 8080 - endpointIP: 4.3.2.3 weight: 1 targetPort: 8080","title":"Load-balancer yaml example"},{"location":"cmd/#delete-a-load-balancer-rule","text":"","title":"Delete a load-balancer rule"},{"location":"cmd/#loxicmd-delete-lb-1111-tcp1828","text":"","title":"loxicmd delete lb 1.1.1.1 --tcp=1828"},{"location":"cmd/#endpoint","text":"","title":"Endpoint"},{"location":"cmd/#get-load-balancer-end-point-health-information","text":"loxicmd get ep","title":"Get load-balancer end-point health information"},{"location":"cmd/#create-end-point-for-health-probing","text":"# loxicmd create endpoint IP [--name=<id>] [--probetype=<probetype>] [--probereq=<probereq>] [--proberesp=<proberesp>] [--probeport=<port>] [--period=<period>] [--retries=<retries>] loxicmd create endpoint 32.32.32.1 --probetype=http --probeport=8080 --period=60 --retries=2 IP(string) : Endpoint target IPaddress name(string) : Endpoint Identifier probetype(string): Probe-type:ping,http,https,udp,tcp,sctp,none probereq(string): If probe is http/https, one can specify additional uri path proberesp(string): If probe is http/https, one can specify custom response string probeport(int): If probe is http,https,tcp,udp,sctp one can specify custom l4port to use period(int): Period of probing retries(int): Number of retries before marking endPoint inactive Notes: - \"name\" is not required when endpoint is created initially. loxilb will allocate the name which can be checked with \"loxicmd get ep\". \"name\" can be given as an Identifier when user wants to modify endpoint probe parameters - Initial state of endpoint will be decided within 15 seconds of rule addition (We cant be sure if service is immediately up so this is the init liveness check timeout. It is not configurable at this time) - After init liveness check, probes will be done as per default (60s) or whatever value is set by the user - When endpoint is inactive we have internal logic and timeouts to minimize blocking calls and maintain stability. Only when endpoint is active, we use probe timeout given by user - For UDP end-points and probe-type, there are two ways to check end-point health currently: - If the service can respond to probe requests with pre-defined responses sent over UDP, we can use the following : loxicmd create endpoint 172.1.217.133 --name=\"udpep1\" --probetype=udp --probeport=32031 --period=60 --retries=2 --probereq=\"probe\" --proberesp=\"hello\" - If the services cannot support the above mechanism, loxilb will try to check for \"ICMP Port unreachable\" after sending UDP probes. If an \"ICMP Port unreachable\" is received, it means the endpoint is not up.","title":"Create end-point for health probing"},{"location":"cmd/#examples","text":"loxicmd create endpoint 32.32.32.1 --probetype=http --probeport=8080 --period=60 --retries=2 loxicmd get ep | HOST | NAME | PTYPE | PORT | DURATION | RETRIES | MINDELAY | AVGDELAY | MAXDELAY | STATE | |------------|----------------------|-------|------|----------|---------|----------|----------|----------|-------| | 32.32.32.1 | 32.32.32.1_http_8080 | http: | 8080 | 60 | 2 | 0s | 0s | 0s | ok | # Modify duration and retry count using name loxicmd create endpoint 32.32.32.1 --name=32.32.32.1_http_8080 --probetype=http --probeport=8080 --period=40 --retries=4 loxicmd get ep | HOST | NAME | PTYPE | PORT | DURATION | RETRIES | MINDELAY | AVGDELAY | MAXDELAY | STATE | |------------|----------------------|-------|------|----------|---------|----------|-----------|-----------|-------| | 32.32.32.1 | 32.32.32.1_http_8080 | http: | 8080 | 40 | 4 | 0s | 0s | 0s | ok |","title":"Examples :"},{"location":"cmd/#create-end-point-with-https-probing-information","text":"# loxicmd create endpoint IP [--name=<id>] [--probetype=<probetype>] [--probereq=<probereq>] [--proberesp=<proberesp>] [--probeport=<port>] [--period=<period>] [--retries=<retries>] loxicmd create endpoint 32.32.32.1 --probetype=https --probeport=8080 --probereq=\"health\" --proberesp=\"OK\" --period=60 --retries=2 Note: loxilb requires CA certificate for TLS connection and private certificate and private key for mTLS connection. Admin can keep a common(default) CA certificate for all the endpoints at \"/opt/loxilb/cert/rootCA.crt\" or per-endpoint certificates can be kept as \"/opt/loxilb/cert/\\<IP>/rootCA.crt\", private key must be kept at \"/opt/loxilb/cert/server.key\" and private certificate at \"/opt/loxilb/cert/server.crt\". Please see Minica or Certstrap or this CICD test case to know how to generate certificates.","title":"Create end-point with https probing information"},{"location":"cmd/#endpoint-yaml-example","text":"apiVersion: netlox/v1 kind: Endpoint metadata: name: test spec: hostName: \"Test\" description: string inactiveReTries: 0 probeType: string probeReqUrl: string probeDuration: 0 probePort: 0","title":"Endpoint yaml example"},{"location":"cmd/#delete-end-point-informtion","text":"","title":"Delete end-point informtion"},{"location":"cmd/#loxicmd-delete-endpoint-31313131","text":"","title":"loxicmd delete endpoint 31.31.31.31"},{"location":"cmd/#bfd","text":"","title":"BFD"},{"location":"cmd/#get-bfd-session-information","text":"loxicmd get bfd","title":"Get BFD Session information"},{"location":"cmd/#create-bfd-session","text":"#loxicmd create bfd <remoteIP> --sourceIP=<sourceIP> --interval=<time in usecs> --retryCount=<count> loxicmd create bfd 192.168.80.253 --sourceIP=192.168.80.252 --interval=500000 --retryCount=3 remoteIP(string): Remote IP address sourceIP(string): Source IP address for binding interval(int): BFD packet Tx Interval Time in microseconds retryCount(int): Number of retry counts to detect failure.","title":"Create BFD Session"},{"location":"cmd/#set-bfd-session","text":"#loxicmd set bfd <remoteIP> --interval=<time in usecs> --retryCount=<count> loxicmd set bfd 192.168.80.253 --interval=400000 --retryCount=5 interval(int): BFD packet Tx Interval Time in microseconds retryCount(int): Number of retry counts to detect failure.","title":"Set BFD Session"},{"location":"cmd/#delete-bfd-session","text":"#loxicmd delete bfd <remoteIP> loxicmd delete bfd 192.168.80.253 remoteIP(string): Remote IP address sourceIP(string): Source IP address for binding interval(int): BFD packet Tx Interval Time in microseconds retryCount(int): Number of retry counts to detect failure.","title":"Delete BFD Session"},{"location":"cmd/#bfd-yaml-example","text":"apiVersion: netlox/v1 kind: BFD metadata: name: test spec: instance: \"default\" remoteIp: \"192.168.80.253\" sourceIp: \"192.168.80.252\" interval: 300000 retryCount: 4","title":"BFD yaml example"},{"location":"cmd/#session","text":"","title":"Session"},{"location":"cmd/#get-session-information","text":"loxicmd get session","title":"Get Session information"},{"location":"cmd/#create-session-information","text":"#loxicmd create session <userID> <sessionIP> --accessNetworkTunnel=<TeID>:<TunnelIP> --coreNetworkTunnel=<TeID>:<TunnelIP> loxicmd create session user1 192.168.20.1 --accessNetworkTunnel=1:1.232.16.1 coreNetworkTunnel=1:1.233.16.1 userID(string): User Identifier sessionIP(string): Session IP address accessNetworkTunnel(string): accessNetworkTunnel has pairs that can be specified as 'TeID:IP' coreNetworkTunnel(string): coreNetworkTunnel has pairs that can be specified as 'TeID:IP'","title":"Create Session information"},{"location":"cmd/#session-yaml-example","text":"apiVersion: netlox/v1 kind: Session metadata: name: test spec: ident: user1 sessionIP: 88.88.88.88 accessNetworkTunnel: TeID: 1 tunnelIP: 11.11.11.11 coreNetworkTunnel: TeID: 1 tunnelIP: 22.22.22.22","title":"Session yaml example"},{"location":"cmd/#delete-session-information","text":"","title":"Delete Session information"},{"location":"cmd/#loxicmd-delete-session-user1","text":"","title":"loxicmd delete session user1"},{"location":"cmd/#sessionulcl","text":"","title":"SessionUlCl"},{"location":"cmd/#get-sessionulcl-information","text":"loxicmd get sessionulcl","title":"Get SessionUlCl information"},{"location":"cmd/#create-sessionulcl-information","text":"#loxicmd create sessionulcl <userID> --ulclArgs=<QFI>:<ulclIP>,... loxicmd create sessionulcl user1 --ulclArgs=16:192.33.125.1 userID(string): User Identifier ulclArgs(string): Port pairs can be specified as 'QFI:UlClIP'","title":"Create SessionUlCl information"},{"location":"cmd/#sessionulcl-yaml-example","text":"apiVersion: netlox/v1 kind: SessionULCL metadata: name: test spec: ulclIdent: user1 ulclArgument: qfi: 11 ulclIP: 8.8.8.8","title":"SessionUlCl yaml example"},{"location":"cmd/#delete-sessionulcl-information","text":"loxicmd delete sessionulcl --ulclArgs=192.33.125.1 ulclArgs(string): UlCl IP address can be specified as 'UlClIP'. It don't need QFI.","title":"Delete SessionUlCl information"},{"location":"cmd/#ipaddress","text":"","title":"IPaddress"},{"location":"cmd/#get-ipaddress-information","text":"loxicmd get ip","title":"Get IPaddress information"},{"location":"cmd/#create-ipaddress-information","text":"#loxicmd create ip <DeviceIPNet> <device> loxicmd create ip 192.168.0.1/24 eno7 DeviceIPNet(string): Actual IP address with mask device(string): name of the related device","title":"Create IPaddress information"},{"location":"cmd/#ipaddress-yaml-example","text":"apiVersion: netlox/v1 kind: IPaddress metadata: name: test spec: dev: eno8 ipAddress: 192.168.23.1/32","title":"IPaddress yaml example"},{"location":"cmd/#delete-ipaddress-information","text":"","title":"Delete IPaddress information"},{"location":"cmd/#loxicmd-delete-ip-deviceipnet-device-loxicmd-delete-ip-1921680124-eno7","text":"","title":"#loxicmd delete ip &lt;DeviceIPNet&gt; &lt;device&gt; loxicmd delete ip 192.168.0.1/24 eno7"},{"location":"cmd/#fdb","text":"","title":"FDB"},{"location":"cmd/#get-fdb-information","text":"loxicmd get fdb","title":"Get FDB information"},{"location":"cmd/#create-fdb-information","text":"#loxicmd create fdb <MacAddress> <DeviceName> loxicmd create fdb aa:aa:aa:aa:bb:bb eno7 MacAddress(string): mac address DeviceName(string): name of the related device","title":"Create FDB information"},{"location":"cmd/#fdb-yaml-example","text":"apiVersion: netlox/v1 kind: FDB metadata: name: test spec: dev: eno8 macAddress: aa:aa:aa:aa:aa:aa","title":"FDB yaml example"},{"location":"cmd/#delete-fdb-information","text":"#loxicmd delete fdb <MacAddress> <DeviceName> loxicmd delete fdb aa:aa:aa:aa:bb:bb eno7","title":"Delete FDB information"},{"location":"cmd/#route","text":"","title":"Route"},{"location":"cmd/#get-route-information","text":"loxicmd get route","title":"Get Route information"},{"location":"cmd/#create-route-information","text":"#loxicmd create route <DestinationIPNet> <gateway> loxicmd create route 192.168.212.0/24 172.17.0.254 DestinationIPNet(string): Actual IP address route with mask gateway(string): gateway information if any","title":"Create Route information"},{"location":"cmd/#route-yaml-example","text":"apiVersion: netlox/v1 kind: Route metadata: name: test spec: destinationIPNet: 192.168.30.0/24 gateway: 172.17.0.1","title":"Route yaml example"},{"location":"cmd/#delete-route-information","text":"","title":"Delete Route information"},{"location":"cmd/#loxicmd-delete-route-destinationipnet-loxicmd-delete-route-192168212024","text":"","title":"#loxicmd delete route &lt;DestinationIPNet&gt; loxicmd delete route 192.168.212.0/24"},{"location":"cmd/#neighbor","text":"","title":"Neighbor"},{"location":"cmd/#get-neighbor-information","text":"loxicmd get neighbor","title":"Get Neighbor information"},{"location":"cmd/#create-neighbor-information","text":"#loxicmd create neighbor <DeviceIP> <DeviceName> [--macAddress=aa:aa:aa:aa:aa:aa] loxicmd create neighbor 192.168.0.1 eno7 --macAddress=aa:aa:aa:aa:aa:aa DeviceIP(string): The IP address DeviceName(string): name of the related device macAddress(string): resolved hardware address if any","title":"Create Neighbor information"},{"location":"cmd/#neighbor-yaml-example","text":"apiVersion: netlox/v1 kind: Neighbor metadata: name: test spec: dev: eno8 macAddress: aa:aa:aa:aa:aa:aa ipAddress: 192.168.23.21","title":"Neighbor yaml example"},{"location":"cmd/#delete-neighbor-information","text":"#loxicmd delete neighbor <DeviceIP> <device> loxicmd delete neighbor 192.168.0.1 eno7","title":"Delete Neighbor information"},{"location":"cmd/#vlan","text":"","title":"Vlan"},{"location":"cmd/#get-vlan-and-vlan-member-information","text":"loxicmd get vlan loxicmd get vlanmember","title":"Get Vlan and Vlan Member information"},{"location":"cmd/#create-vlan-and-vlan-member-information","text":"#loxicmd create vlan <Vid> loxicmd create vlan 100 Vid(int): vlan identifier #loxicmd create vlanmember <Vid> <DeviceName> --tagged=<Tagged> loxicmd create vlanmember 100 eno7 --tagged=true loxicmd create vlanmember 100 eno7 Vid(int): vlan identifier DeviceName(string): name of the related device tagged(boolean): tagged or not (default is false)","title":"Create Vlan and Vlan Member information"},{"location":"cmd/#vlan-yaml-example","text":"apiVersion: netlox/v1 kind: Vlan metadata: name: test spec: vid: 100","title":"Vlan yaml example"},{"location":"cmd/#vlan-member-yaml-example","text":"apiVersion: netlox/v1 kind: VlanMember metadata: name: test vid: 100 spec: dev: eno8 Tagged: true","title":"Vlan Member yaml example"},{"location":"cmd/#delete-vlan-and-vlan-member-information","text":"#loxicmd delete vlan <Vid> loxicmd delete vlan 100 #loxicmd delete vlanmember <Vid> <DeviceName> --tagged=<Tagged> loxicmd delete vlanmember 100 eno7 --tagged=true loxicmd delete vlanmember 100 eno7","title":"Delete Vlan and Vlan Member information"},{"location":"cmd/#vxlan","text":"","title":"Vxlan"},{"location":"cmd/#get-vxlan-and-vxlan-peer-information","text":"loxicmd get vxlan loxicmd get vxlanpeer","title":"Get Vxlan and Vxlan Peer information"},{"location":"cmd/#create-vxlan-and-vxlan-peer-information","text":"#loxicmd create vxlan <VxlanID> <EndpointDeviceName> loxicmd create vxlan 100 eno7 VxlanID(int): Vxlan Identifier EndpointDeviceName(string): VTEP Device name(It must have own IP address for peering) #loxicmd create vxlanpeer <VxlanID> <PeerIP> loxicmd create vxlan-peer 100 30.1.3.1 VxlanID(int): Vxlan Identifier PeerIP(string): Vxlan peer device IP address","title":"Create Vxlan and Vxlan Peer information"},{"location":"cmd/#vxlan-yaml-example","text":"apiVersion: netlox/v1 kind: Vxlan metadata: name: test spec: epIntf: eno8 vxlanID: 100","title":"Vxlan yaml example"},{"location":"cmd/#vxlan-peer-yaml-example","text":"apiVersion: netlox/v1 kind: VxlanPeer metadata: name: test vxlanID: 100 spec: peerIP: 21.21.21.1","title":"Vxlan Peer yaml example"},{"location":"cmd/#delete-vxlan-and-vxlan-peer-information","text":"#loxicmd delete vxlan <VxlanID> loxicmd delete vxlan 100 #loxicmd delete vxlanpeer <VxlanID> <PeerIP> loxicmd delete vxlan-peer 100 30.1.3.1","title":"Delete Vxlan and Vxlan Peer information"},{"location":"cmd/#firewall","text":"","title":"Firewall"},{"location":"cmd/#get-firewall-information","text":"loxicmd get firewall","title":"Get Firewall information"},{"location":"cmd/#create-firewall-information","text":"#loxicmd create firewall --firewallRule=<ruleKey>:<ruleValue>, [--allow] [--drop] [--trap] [--redirect=<PortName>] [--setmark=<FwMark> loxicmd create firewall --firewallRule=\"sourceIP:1.2.3.2/32,destinationIP:2.3.1.2/32,preference:200\" --allow loxicmd create firewall --firewallRule=\"sourceIP:1.2.3.2/32,destinationIP:2.3.1.2/32,preference:200\" --allow --setmark=10 loxicmd create firewall --firewallRule=\"sourceIP:1.2.3.2/32,destinationIP:2.3.1.2/32,preference:200\" --drop loxicmd create firewall --firewallRule=\"sourceIP:1.2.3.2/32,destinationIP:2.3.1.2/32,preference:200\" --trap loxicmd create firewall --firewallRule=\"sourceIP:1.2.3.2/32,destinationIP:2.3.1.2/32,preference:200\" --redirect=ensp0 firewallRule sourceIP(string) - Source IP in CIDR notation destinationIP(string) - Destination IP in CIDR notation minSourcePort(int) - Minimum source port range maxSourcePort(int) - Maximum source port range minDestinationPort(int) - Minimum destination port range maxDestinationPort(int) - Maximum source port range protocol(int) - the protocol portName(string) - the incoming port preference(int) - User preference for ordering","title":"Create Firewall information"},{"location":"cmd/#firewall-yaml-example","text":"apiVersion: netlox/v1 kind: Firewall metadata: name: test spec: ruleArguments: sourceIP: 192.169.1.2/24 destinationIP: 192.169.2.1/24 preference: 200 opts: allow: true","title":"Firewall yaml example"},{"location":"cmd/#delete-firewall-information","text":"","title":"Delete Firewall information"},{"location":"cmd/#loxicmd-delete-firewall-firewallrulerulekeyrulevalue-loxicmd-delete-firewall-firewallrulesourceip123232destinationip231232preference200","text":"","title":"#loxicmd delete firewall --firewallRule=&lt;ruleKey&gt;:&lt;ruleValue&gt; loxicmd delete firewall --firewallRule=\"sourceIP:1.2.3.2/32,destinationIP:2.3.1.2/32,preference:200"},{"location":"cmd/#mirror","text":"","title":"Mirror"},{"location":"cmd/#get-mirror-information","text":"loxicmd get mirror","title":"Get Mirror information"},{"location":"cmd/#create-mirror-information","text":"#loxicmd create mirror <mirrorIdent> --mirrorInfo=<InfoOption>:<InfoValue>,... --targetObject=attachement:<port1,rule2>,mirrObjName:<ObjectName> loxicmd create mirror mirr-1 --mirrorInfo=\"type:0,port:ensp0\" --targetObject=\"attachement:1,mirrObjName:ensp1 mirrorIdent(string): Mirror identifier type(int) : Mirroring type as like 0 == SPAN, 1 == RSPAN, 2 == ERSPAN port(string) : The port where mirrored traffic needs to be sent vlan(int) : for RSPAN we may need to send tagged mirror traffic remoteIP(string) : For ERSPAN we may need to send tunnelled mirror traffic sourceIP(string): For ERSPAN we may need to send tunnelled mirror traffic tunnelID(int): For ERSPAN we may need to send tunnelled mirror traffic","title":"Create Mirror information"},{"location":"cmd/#mirror-yaml-example","text":"apiVersion: netlox/v1 kind: Mirror metadata: name: test spec: mirrorIdent: mirr-1 mirrorInfo: type: 0 port: eno1 targetObject: attachment: 1 mirrObjName: eno2","title":"Mirror yaml example"},{"location":"cmd/#delete-mirror-information","text":"#loxicmd delete mirror <mirrorIdent> loxicmd delete mirror mirr-1","title":"Delete Mirror information"},{"location":"cmd/#policy","text":"","title":"Policy"},{"location":"cmd/#get-policy-information","text":"loxicmd get policy","title":"Get Policy information"},{"location":"cmd/#create-policy-information","text":"#loxicmd create policy IDENT --rate=<Peak>:<Commited> --target=<ObjectName>:<Attachment> [--block-size=<Excess>:<Committed>] [--color] [--pol-type=<policy type>] loxicmd create policy pol-0 --rate=100:100 --target=ensp0:1 loxicmd create policy pol-1 --rate=100:100 --target=ensp0:1 --block-size=12000:6000 loxicmd create policy pol-1 --rate=100:100 --target=ensp0:1 --color loxicmd create policy pol-1 --rate=100:100 --target=ensp0:1 --color --pol-type 0 rate(string): Rate pairs can be specified as 'Peak:Commited'. rate unit : Mbps block-size(string): Block Size pairs can be specified as 'Excess:Committed'. block-size unit : bps target(string): Target Interface pairs can be specified as 'ObjectName:Attachment' color(boolean): Policy color enbale or not pol-type(int): Policy traffic control type. 0 : TrTCM, 1 : SrTCM","title":"Create Policy information"},{"location":"cmd/#policy-yaml-example","text":"apiVersion: netlox/v1 kind: Policy metadata: name: test spec: policyIdent: pol-eno8 policyInfo: type: 0 colorAware: false committedInfoRate: 100 peakInfoRate: 100 targetObject: attachment: 1 polObjName: eno8","title":"Policy yaml example"},{"location":"cmd/#delete-policy-information","text":"","title":"Delete Policy information"},{"location":"cmd/#loxicmd-delete-policy-polident-loxicmd-delete-policy-pol-1","text":"","title":"#loxicmd delete policy &lt;Polident&gt; loxicmd delete policy pol-1"},{"location":"cmd/#session-recorder","text":"","title":"Session Recorder"},{"location":"cmd/#set-n-tuple-policy-for-recording","text":"loxicmd create firewall --firewallRule=\"destinationIP:31.31.31.0/24,preference:200\" --allow --record loxilb will record any connection track entry which matches this policy (even for reverse direction) as a way to provide extended visibility for debugging","title":"Set n-tuple policy for recording"},{"location":"cmd/#check-or-record-with-tcpdump","text":"tcpdump -i llb0 -n Any valid tcpdump option can be used including saving to a pcap file","title":"Check or record with tcpdump"},{"location":"cmd/#get-live-connection-track-information","text":"loxicmd get conntrack","title":"Get live connection-track information"},{"location":"cmd/#get-port-dump-information","text":"loxicmd get port","title":"Get port-dump information"},{"location":"cmd/#save-all-loxilbs-operational-information-in-dbstore","text":"loxicmd save -a ** This will ensure that whenever loxilb restarts, it will start with last saved state from DBStore","title":"Save all loxilb's operational information in DBStore"},{"location":"cmd/#configure-loxicmd-with-yamlbeta","text":"The loxicmd support yaml based configuration. The format is same as Kubernetes. This beta version support only one configuraion per one file. That means \"Do not use --- in yaml file.\" . It will be supported at next release.","title":"Configure loxicmd with yaml(Beta)"},{"location":"cmd/#command","text":"#loxicmd apply -f <file.yaml> #loxicmd delete -f <file.yaml> loxicmd apply -f lb.yaml loxicmd delete -f lb.yaml","title":"Command"},{"location":"cmd/#file-examplelbyaml","text":"apiVersion: netlox/v1 kind: Loadbalancer metadata: name: load spec: serviceArguments: externalIP: 123.123.123.1 port: 80 protocol: tcp sel: 0 endpoints: - endpointIP: 4.3.2.1 weight: 1 targetPort: 8080 - endpointIP: 4.3.2.2 weight: 1 targetPort: 8080 - endpointIP: 4.3.2.3 weight: 1 targetPort: 8080 It reuse API's json body as a \"Spec\". If the API URL has no param, it don't need to use \"metadata\". For example, The body of load Balancer rule is shown below. { \"serviceArguments\": { \"externalIP\": \"123.123.123.1\", \"port\": 80, \"protocol\": \"tcp\", \"sel\": 0 }, \"endpoints\": [ { \"endpointIP\": \"4.3.2.1\", \"weight\": 1, \"targetPort\": 8080 }, { \"endpointIP\": \"4.3.2.2\", \"weight\": 1, \"targetPort\": 8080 }, { \"endpointIP\": \"4.3.2.3\", \"weight\": 1, \"targetPort\": 8080 } } This json format can be converted Yaml format as shown below. serviceArguments: externalIP: 123.123.123.1 port: 80 protocol: tcp sel: 0 endpoints: - endpointIP: 4.3.2.1 weight: 1 targetPort: 8080 - endpointIP: 4.3.2.2 weight: 1 targetPort: 8080 - endpointIP: 4.3.2.3 weight: 1 targetPort: 8080 Finally, this is located in the Spec of the entire configuration file as File example(lb.yaml) If you want to add Vlan bridge, IPaddress or something else. Just change the Kind value from Loadbalancer to VlanBridge, IPaddress as like below example. apiVersion: netlox/v1 kind: IPaddress metadata: name: test spec: dev: eno8 ipAddress: 192.168.23.1/32 If the URL has param such as adding vlan-member, it must have metadata . apiVersion: netlox/v1 kind: VlanMember metadata: name: test vid: 100 spec: dev: eno8 Tagged: true The example of all the settings below, so please refer to it.","title":"File example(lb.yaml)"},{"location":"cmd/#more-information","text":"There are tons of other commands, use help option! loxicmd help","title":"More information"},{"location":"code/","text":"loxilb is organized as below: \u251c\u2500\u2500 api \u2502 \u251c\u2500\u2500 certification \u2502 \u251c\u2500\u2500 cmd \u2502 \u2502 \u251c\u2500\u2500 loxilb-rest-api-server \u2502 \u251c\u2500\u2500 models \u2502 \u251c\u2500\u2500 restapi \u2502 \u251c\u2500\u2500 handler \u2502 \u251c\u2500\u2500 operations \u251c\u2500\u2500 common \u251c\u2500\u2500 ebpf \u2502 \u251c\u2500\u2500 common \u2502 \u251c\u2500\u2500 headers \u2502 \u2502 \u251c\u2500\u2500 linux \u2502 \u251c\u2500\u2500 kernel \u2502 \u251c\u2500\u2500 libbpf \u2502 \u2502 \u251c\u2500\u2500 include \u2502 \u2502 \u2502 \u251c\u2500\u2500 asm \u2502 \u2502 \u2502 \u251c\u2500\u2500 linux \u2502 \u2502 \u2502 \u251c\u2500\u2500 uapi \u2502 \u2502 \u2502 \u251c\u2500\u2500 linux \u2502 \u2502 \u251c\u2500\u2500 scripts \u2502 \u2502 \u251c\u2500\u2500 src \u2502 \u2502 \u2502 \u251c\u2500\u2500 build \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 usr \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 include \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 bpf \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 lib64 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 pkgconfig \u2502 \u2502 \u2502 \u251c\u2500\u2500 sharedobjs \u2502 \u2502 \u2502 \u251c\u2500\u2500 staticobjs \u2502 \u2502 \u251c\u2500\u2500 travis-ci \u2502 \u2502 \u251c\u2500\u2500 managers \u2502 \u2502 \u251c\u2500\u2500 vmtest \u2502 \u2502 \u251c\u2500\u2500 configs \u2502 \u2502 \u251c\u2500\u2500 blacklist \u2502 \u2502 \u251c\u2500\u2500 whitelist \u2502 \u251c\u2500\u2500 utils \u251c\u2500\u2500 loxinet \u251c\u2500\u2500 options \u251c\u2500\u2500 loxilib api This directory contains source code to host api server to handle CCM configuration requests. common Common api to configure which are exposed by loxinet are defined in this directory. loxinet This module implements the glue layer or the middle layer between eBPF datapath module and api modules. It defines functions for configuring networking and load balancing rules in the eBPF datapath. ebpf This directory contains source code for loxilb eBPF datapath. options This directory contains files for managing the command line options. loxilib This package contains common routines for logging, statistics and other utilities.","title":"Code organization"},{"location":"code/#loxilb-is-organized-as-below","text":"\u251c\u2500\u2500 api \u2502 \u251c\u2500\u2500 certification \u2502 \u251c\u2500\u2500 cmd \u2502 \u2502 \u251c\u2500\u2500 loxilb-rest-api-server \u2502 \u251c\u2500\u2500 models \u2502 \u251c\u2500\u2500 restapi \u2502 \u251c\u2500\u2500 handler \u2502 \u251c\u2500\u2500 operations \u251c\u2500\u2500 common \u251c\u2500\u2500 ebpf \u2502 \u251c\u2500\u2500 common \u2502 \u251c\u2500\u2500 headers \u2502 \u2502 \u251c\u2500\u2500 linux \u2502 \u251c\u2500\u2500 kernel \u2502 \u251c\u2500\u2500 libbpf \u2502 \u2502 \u251c\u2500\u2500 include \u2502 \u2502 \u2502 \u251c\u2500\u2500 asm \u2502 \u2502 \u2502 \u251c\u2500\u2500 linux \u2502 \u2502 \u2502 \u251c\u2500\u2500 uapi \u2502 \u2502 \u2502 \u251c\u2500\u2500 linux \u2502 \u2502 \u251c\u2500\u2500 scripts \u2502 \u2502 \u251c\u2500\u2500 src \u2502 \u2502 \u2502 \u251c\u2500\u2500 build \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 usr \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 include \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 bpf \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 lib64 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 pkgconfig \u2502 \u2502 \u2502 \u251c\u2500\u2500 sharedobjs \u2502 \u2502 \u2502 \u251c\u2500\u2500 staticobjs \u2502 \u2502 \u251c\u2500\u2500 travis-ci \u2502 \u2502 \u251c\u2500\u2500 managers \u2502 \u2502 \u251c\u2500\u2500 vmtest \u2502 \u2502 \u251c\u2500\u2500 configs \u2502 \u2502 \u251c\u2500\u2500 blacklist \u2502 \u2502 \u251c\u2500\u2500 whitelist \u2502 \u251c\u2500\u2500 utils \u251c\u2500\u2500 loxinet \u251c\u2500\u2500 options \u251c\u2500\u2500 loxilib","title":"loxilb is organized as below:"},{"location":"code/#api","text":"This directory contains source code to host api server to handle CCM configuration requests.","title":"api"},{"location":"code/#common","text":"Common api to configure which are exposed by loxinet are defined in this directory.","title":"common"},{"location":"code/#loxinet","text":"This module implements the glue layer or the middle layer between eBPF datapath module and api modules. It defines functions for configuring networking and load balancing rules in the eBPF datapath.","title":"loxinet"},{"location":"code/#ebpf","text":"This directory contains source code for loxilb eBPF datapath.","title":"ebpf"},{"location":"code/#options","text":"This directory contains files for managing the command line options.","title":"options"},{"location":"code/#loxilib","text":"This package contains common routines for logging, statistics and other utilities.","title":"loxilib"},{"location":"contribute/","text":"Contributing When contributing to any of loxilb's repositories, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change. Please note we have a code of conduct, please follow it in all your interactions with the project. Pull Request Process Ensure any install or build dependencies are removed before the end of the layer when doing a build. Update the README.md with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters. Increase the version numbers in any examples files and the README.md to the new version that this Pull Request would represent. The versioning scheme we use is SemVer . You may merge the Pull Request in once you have the sign-off of two other developers, or if you do not have permission to do that, you may request the second reviewer to merge it for you. For Pull Requests to be successfully merged to main branch : It has to be code-reviewed by the maintainer(s) Integrated Travis-CI runs should pass without errors Detailed instructions to help new developers setup the development/test environment can be found here Alternatively, they can email developers at [loxilb-devel@netlox.io]. , checkout existing issues in github , visit the loxilb forum or loxilb slack channel Sign Your Commits Instructions DCO Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project. You sign-off by adding the following to your commit messages. Your sign-off must match the git user and email associated with the commit. This is my commit message Signed-off-by: Your Name <your.name@example.com> Git has a -s command line option to do this automatically: git commit -s -m 'This is my commit message' If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running git commit --amend -s Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [loxilb-devel@netlox.io]. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4","title":"Contribution Guide"},{"location":"contribute/#contributing","text":"When contributing to any of loxilb's repositories, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change. Please note we have a code of conduct, please follow it in all your interactions with the project.","title":"Contributing"},{"location":"contribute/#pull-request-process","text":"Ensure any install or build dependencies are removed before the end of the layer when doing a build. Update the README.md with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters. Increase the version numbers in any examples files and the README.md to the new version that this Pull Request would represent. The versioning scheme we use is SemVer . You may merge the Pull Request in once you have the sign-off of two other developers, or if you do not have permission to do that, you may request the second reviewer to merge it for you. For Pull Requests to be successfully merged to main branch : It has to be code-reviewed by the maintainer(s) Integrated Travis-CI runs should pass without errors Detailed instructions to help new developers setup the development/test environment can be found here Alternatively, they can email developers at [loxilb-devel@netlox.io]. , checkout existing issues in github , visit the loxilb forum or loxilb slack channel","title":"Pull Request Process"},{"location":"contribute/#sign-your-commits","text":"Instructions","title":"Sign Your Commits"},{"location":"contribute/#dco","text":"Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project. You sign-off by adding the following to your commit messages. Your sign-off must match the git user and email associated with the commit. This is my commit message Signed-off-by: Your Name <your.name@example.com> Git has a -s command line option to do this automatically: git commit -s -m 'This is my commit message' If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running git commit --amend -s","title":"DCO"},{"location":"contribute/#code-of-conduct","text":"","title":"Code of Conduct"},{"location":"contribute/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"contribute/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"contribute/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"contribute/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"contribute/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [loxilb-devel@netlox.io]. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"contribute/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4","title":"Attribution"},{"location":"debugging/","text":"loxilb - How to debug and troubleshoot * loxilb docker or pod not coming in Running state ? Solution: Check the host machine kernel version. loxilb requires kernel version 5.8 or above . Make sure you are running the correct image as per your environment. * externalIP pending in \"kubectl get svc\" ? If this happens: 1. When running loxilb externally, there could be a connectivity issue. Solution: Check if loxiURL annotation in kube-loxilb.yaml was set correctly. Check for kube-loxilb(master node) connectivity with loxilb node. 2. When running loxilb in-cluster mode Solution: Make sure loxilb pods were spawned. 3. Make sure the annotation \"node.kubernetes.io/exclude-from-external-load-balancers\" is NOT present in the node's configuration. Solution: If present, then the node will not be considered as an endpoint by loxilb. You can remove it by editing \"kubectl edit \\<node-name>\" 4. Make sure these annotations are present in your service.yaml spec: loadBalancerClass: loxilb.io/loxilb type: LoadBalancer * SCTP packets dropping ? Usually, This happens due to SCTP checksum validation by host kernel and the possible scenarios are: 1. When workload and loxilb are scheduled in the same node. 2. Different CNI creates different types of interfaces i.e. CNIs creates bridges/tunnels/veth pairs and different network policies. These interfaces have different characteristics and implications on loxilb's checksum calculation logic. Solution: There are two ways to resolve this issue: Disable checksum calculation. echo 1 > /sys/module/sctp/parameters/no_checksums echo 0 > /proc/sys/net/netfilter/nf_conntrack_checksum Or, Let loxilb take care of the checksum calculation completely. For that, We need to install a utility(a kernel module) in all the nodes where loxilb is running. It will make sure the correct checksum is applied at the end. curl -sfL https://github.com/loxilb-io/loxilb-ebpf/raw/main/kprobe/install.sh | sh - * ABORT in SCTP ? SCTP ABORT can be seen in many scenarios: 1. When Service IP is same as loxilb IP and SCTP packets does not match the rules. Solution: Check if the rule is installed properly loxicmd get lb Make sure the client is connecting to the same IP and port as per the configured service LB rule. 2. In one-arm/fullnat mode, loxilb sends SCTP ABORT after receiving SCTP INIT ACK packet. Solution: Check the underlying hypervisor interface driver. Some drivers does not provide enough metadata for ebpf processing which makes the packet to follow fallback path to kernel and kernel being unaware of the SCTP connection sends SCTP ABORT. Emulated interfaces in bridge mode are preferred for smooth networking. 3. ABORT after few seconds(Heartbeat re-transmisions) When initiating the SCTP connection, if the application is not binded with a particular IP then SCTP stack uses all the IPs in the SCTP INIT message. After the successful connection, both endpoints start health check for each network path. As loxilb is in between and unaware of all the endpoint IPs, drops all those packets, which leads to sending SCTP ABORT from the endpoint. Solution: In SCTP uni-homing case, it is absolutely necessary to make sure the applications are binded to only one IP to avoid this case. 4. ABORT after few seconds(SCTP Multihoming) Solution: Currently, SCTP Multihoming service works only with fullnat mode and externalTrafficPolicy set to \"Local\" * Check loxilb logs loxilb logs its various important events and logs in the file /var/log/loxilb.log. Users can check it by using tail -f or any other command of choice. root@752531364e2c:/# tail -f /var/log/loxilb.log DBG: 2022/07/10 12:49:27 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:49:37 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:49:47 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:49:57 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:50:07 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:50:17 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:50:27 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:50:37 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:50:47 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:50:57 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 * Check loxicmd to debug loxilb's internal state ``` Spawn a bash shell of loxilb docker docker exec -it loxilb bash root@752531364e2c:/# loxicmd get lb | EXTERNALIP | PORT | PROTOCOL | SELECT | # OF ENDPOINTS | |------------|------|----------|--------|----------------| | 10.10.10.1 | 2020 | tcp | 0 | 3 | root@752531364e2c:/# loxicmd get lb -o wide | EXTERNALIP | PORT | PROTOCOL | SELECT | ENDPOINTIP | TARGETPORT | WEIGHT | |------------|------|----------|--------|---------------|------------|--------| | 10.10.10.1 | 2020 | tcp | 0 | 31.31.31.1 | 5001 | 1 | | | | | | 32.32.32.1 | 5001 | 2 | | | | | | 100.100.100.1 | 5001 | 2 | root@0c4f9175c983:/# loxicmd get conntrack | DESTINATIONIP | SOURCEIP | DESTINATIONPORT | SOURCEPORT | PROTOCOL | STATE | ACT | |---------------|------------|-----------------|------------|----------|-------------|-----| | 127.0.0.1 | 127.0.0.1 | 11111 | 47180 | tcp | closed-wait | | | 127.0.0.1 | 127.0.0.1 | 11111 | 47182 | tcp | est | | | 32.32.32.1 | 31.31.31.1 | 35068 | 35068 | icmp | bidir | | root@65ad9b2f1b7f:/# loxicmd get port | INDEX | PORTNAME | MAC | LINK/STATE | L3INFO | L2INFO | |-------|----------|-------------------|-------------|---------------|---------------| | 1 | lo | 00:00:00:00:00:00 | true/false | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3801 | | | | | | IPv6 : [] | | | 2 | vlan3801 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3801 | | | | | | IPv6 : [] | | | 3 | llb0 | 42:6e:9b:7f:ff:36 | true/false | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3803 | | | | | | IPv6 : [] | | | 4 | vlan3803 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3803 | | | | | | IPv6 : [] | | | 5 | eth0 | 02:42:ac:1e:01:c1 | true/true | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3805 | | | | | | IPv6 : [] | | | 6 | vlan3805 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3805 | | | | | | IPv6 : [] | | | 7 | enp1 | fe:84:23:ac:41:31 | false/false | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3807 | | | | | | IPv6 : [] | | | 8 | vlan3807 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3807 | | | | | | IPv6 : [] | | | 9 | enp2 | d6:3c:7f:9e:58:5c | false/false | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3809 | | | | | | IPv6 : [] | | | 10 | vlan3809 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3809 | | | | | | IPv6 : [] | | | 11 | enp2v15 | 8a:9e:99:aa:f9:c3 | false/false | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3811 | | | | | | IPv6 : [] | | | 12 | vlan3811 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3811 | | | | | | IPv6 : [] | | | 13 | enp3 | f2:c7:4b:ac:fd:3e | false/false | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3813 | | | | | | IPv6 : [] | | | 14 | vlan3813 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3813 | | | | | | IPv6 : [] | | | 15 | enp4 | 12:d2:c3:79:f3:6a | false/false | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3815 | | | | | | IPv6 : [] | | | 16 | vlan3815 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3815 | | | | | | IPv6 : [] | | | 17 | vlan100 | 56:2e:76:b2:71:48 | false/false | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 100 | | | | | | IPv6 : [] | |","title":"Debugging loxilb"},{"location":"debugging/#loxilb-how-to-debug-and-troubleshoot","text":"","title":"loxilb - How to debug and troubleshoot"},{"location":"debugging/#loxilb-docker-or-pod-not-coming-in-running-state","text":"Solution: Check the host machine kernel version. loxilb requires kernel version 5.8 or above . Make sure you are running the correct image as per your environment.","title":"* loxilb docker or pod not coming in Running state ?"},{"location":"debugging/#externalip-pending-in-kubectl-get-svc","text":"If this happens: 1. When running loxilb externally, there could be a connectivity issue. Solution: Check if loxiURL annotation in kube-loxilb.yaml was set correctly. Check for kube-loxilb(master node) connectivity with loxilb node. 2. When running loxilb in-cluster mode Solution: Make sure loxilb pods were spawned. 3. Make sure the annotation \"node.kubernetes.io/exclude-from-external-load-balancers\" is NOT present in the node's configuration. Solution: If present, then the node will not be considered as an endpoint by loxilb. You can remove it by editing \"kubectl edit \\<node-name>\" 4. Make sure these annotations are present in your service.yaml spec: loadBalancerClass: loxilb.io/loxilb type: LoadBalancer","title":"* externalIP pending in \"kubectl get svc\" ?"},{"location":"debugging/#sctp-packets-dropping","text":"Usually, This happens due to SCTP checksum validation by host kernel and the possible scenarios are: 1. When workload and loxilb are scheduled in the same node. 2. Different CNI creates different types of interfaces i.e. CNIs creates bridges/tunnels/veth pairs and different network policies. These interfaces have different characteristics and implications on loxilb's checksum calculation logic. Solution: There are two ways to resolve this issue: Disable checksum calculation. echo 1 > /sys/module/sctp/parameters/no_checksums echo 0 > /proc/sys/net/netfilter/nf_conntrack_checksum Or, Let loxilb take care of the checksum calculation completely. For that, We need to install a utility(a kernel module) in all the nodes where loxilb is running. It will make sure the correct checksum is applied at the end. curl -sfL https://github.com/loxilb-io/loxilb-ebpf/raw/main/kprobe/install.sh | sh -","title":"* SCTP packets dropping ?"},{"location":"debugging/#abort-in-sctp","text":"SCTP ABORT can be seen in many scenarios: 1. When Service IP is same as loxilb IP and SCTP packets does not match the rules. Solution: Check if the rule is installed properly loxicmd get lb Make sure the client is connecting to the same IP and port as per the configured service LB rule. 2. In one-arm/fullnat mode, loxilb sends SCTP ABORT after receiving SCTP INIT ACK packet. Solution: Check the underlying hypervisor interface driver. Some drivers does not provide enough metadata for ebpf processing which makes the packet to follow fallback path to kernel and kernel being unaware of the SCTP connection sends SCTP ABORT. Emulated interfaces in bridge mode are preferred for smooth networking. 3. ABORT after few seconds(Heartbeat re-transmisions) When initiating the SCTP connection, if the application is not binded with a particular IP then SCTP stack uses all the IPs in the SCTP INIT message. After the successful connection, both endpoints start health check for each network path. As loxilb is in between and unaware of all the endpoint IPs, drops all those packets, which leads to sending SCTP ABORT from the endpoint. Solution: In SCTP uni-homing case, it is absolutely necessary to make sure the applications are binded to only one IP to avoid this case. 4. ABORT after few seconds(SCTP Multihoming) Solution: Currently, SCTP Multihoming service works only with fullnat mode and externalTrafficPolicy set to \"Local\"","title":"* ABORT in SCTP ?"},{"location":"debugging/#check-loxilb-logs","text":"loxilb logs its various important events and logs in the file /var/log/loxilb.log. Users can check it by using tail -f or any other command of choice. root@752531364e2c:/# tail -f /var/log/loxilb.log DBG: 2022/07/10 12:49:27 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:49:37 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:49:47 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:49:57 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:50:07 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:50:17 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:50:27 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:50:37 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:50:47 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0 DBG: 2022/07/10 12:50:57 1:dst-10.10.10.1/32,proto-6,dport-2020,,do-dnat:eip-31.31.31.1,ep-5001,w-1,alive|eip-32.32.32.1,ep-5001,w-2,alive|eip-100.100.100.1,ep-5001,w-2,alive| pc 0 bc 0","title":"* Check loxilb logs"},{"location":"debugging/#check-loxicmd-to-debug-loxilbs-internal-state","text":"```","title":"* Check loxicmd to debug loxilb's internal state"},{"location":"debugging/#spawn-a-bash-shell-of-loxilb-docker","text":"docker exec -it loxilb bash root@752531364e2c:/# loxicmd get lb | EXTERNALIP | PORT | PROTOCOL | SELECT | # OF ENDPOINTS | |------------|------|----------|--------|----------------| | 10.10.10.1 | 2020 | tcp | 0 | 3 | root@752531364e2c:/# loxicmd get lb -o wide | EXTERNALIP | PORT | PROTOCOL | SELECT | ENDPOINTIP | TARGETPORT | WEIGHT | |------------|------|----------|--------|---------------|------------|--------| | 10.10.10.1 | 2020 | tcp | 0 | 31.31.31.1 | 5001 | 1 | | | | | | 32.32.32.1 | 5001 | 2 | | | | | | 100.100.100.1 | 5001 | 2 | root@0c4f9175c983:/# loxicmd get conntrack | DESTINATIONIP | SOURCEIP | DESTINATIONPORT | SOURCEPORT | PROTOCOL | STATE | ACT | |---------------|------------|-----------------|------------|----------|-------------|-----| | 127.0.0.1 | 127.0.0.1 | 11111 | 47180 | tcp | closed-wait | | | 127.0.0.1 | 127.0.0.1 | 11111 | 47182 | tcp | est | | | 32.32.32.1 | 31.31.31.1 | 35068 | 35068 | icmp | bidir | | root@65ad9b2f1b7f:/# loxicmd get port | INDEX | PORTNAME | MAC | LINK/STATE | L3INFO | L2INFO | |-------|----------|-------------------|-------------|---------------|---------------| | 1 | lo | 00:00:00:00:00:00 | true/false | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3801 | | | | | | IPv6 : [] | | | 2 | vlan3801 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3801 | | | | | | IPv6 : [] | | | 3 | llb0 | 42:6e:9b:7f:ff:36 | true/false | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3803 | | | | | | IPv6 : [] | | | 4 | vlan3803 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3803 | | | | | | IPv6 : [] | | | 5 | eth0 | 02:42:ac:1e:01:c1 | true/true | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3805 | | | | | | IPv6 : [] | | | 6 | vlan3805 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3805 | | | | | | IPv6 : [] | | | 7 | enp1 | fe:84:23:ac:41:31 | false/false | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3807 | | | | | | IPv6 : [] | | | 8 | vlan3807 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3807 | | | | | | IPv6 : [] | | | 9 | enp2 | d6:3c:7f:9e:58:5c | false/false | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3809 | | | | | | IPv6 : [] | | | 10 | vlan3809 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3809 | | | | | | IPv6 : [] | | | 11 | enp2v15 | 8a:9e:99:aa:f9:c3 | false/false | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3811 | | | | | | IPv6 : [] | | | 12 | vlan3811 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3811 | | | | | | IPv6 : [] | | | 13 | enp3 | f2:c7:4b:ac:fd:3e | false/false | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3813 | | | | | | IPv6 : [] | | | 14 | vlan3813 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3813 | | | | | | IPv6 : [] | | | 15 | enp4 | 12:d2:c3:79:f3:6a | false/false | Routed: false | IsPVID: true | | | | | | IPv4 : [] | VID : 3815 | | | | | | IPv6 : [] | | | 16 | vlan3815 | aa:bb:cc:dd:ee:ff | true/true | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 3815 | | | | | | IPv6 : [] | | | 17 | vlan100 | 56:2e:76:b2:71:48 | false/false | Routed: false | IsPVID: false | | | | | | IPv4 : [] | VID : 100 | | | | | | IPv6 : [] | |","title":"Spawn a bash shell of loxilb docker"},{"location":"ebpf/","text":"What is eBPF ?? eBPF has been making quite some news lately. An elegant way to extend the linux kernel (or windows) has far reaching implications. Although initially, eBPF was used to enhance system observability beyond existing tools, we will explore in this post how eBPF can be used for enhancing Linux networking performance. There are a lot of additional resources about eBPF in the eBPF project page . A quick recap The hooks that are of particular interest for this discussion are NIC hook (invoked just after packet is received at NIC) and TC hook (invoked just before Linux starts processing packet with its TCP/IP stack). Programs loaded to the former hook are also known as XDP programs and to the latter are called eBPF TC. Although both use eBPF restricted C syntax, there are significant differences between these types. (We will cover it in a separate blog later). For now, we just need to remember that when dealing with container-to-container or container-to-outside communication eBPF-TC makes much more sense since memory allocation (for skb) will happen either way in such scenarios. The performance bottlenecks Coming back to the focus of our discussion which is of course performance, let us step back and take a look at why Linux sucks at networking performance (or rather why it could perform much faster). Linux networking evolved from the days of dial up modem networking when speed was not of utmost importance. Down the lane, code kept accumulating. Although it is extremely feature rich and RFC compliant, it hardly resembles a powerful data-path networking engine. The following figure shows a call-trace of Linux kernel networking stack: The point is it has become incredibly complex over the years. Once features like NAT, VXLAN, conntrack etc come into play, Linux networking stops scaling due to cache degradation, lock contention etc. One problem leads to the another To avoid performance penalties, many user-space frameworks like DPDK have been widely used, which completely skip the linux kernel networking and directly process packets in the user-space. As simple as that may sound, there are some serious drawbacks in using such frameworks e.g need to dedicate cores (can\u2019t multitask), applications written on a specific user-space driver (PMD) might not run on another as it is, apps are also rendered incompatible across different DPDK releases frequently. Finally, there is a need to redo various parts of the TCP/IP stack and the provisioning involved. In short, it leads to a massive and completely unnecessary need of reinventing the wheel. We will have a detailed post later to discuss these factors. But for now, in short, if we are looking to get more out of a box than doing only networking, DPDK is not the right choice. In the age of distributed edge computing and immersive metaverse, the need to do more out of less is of utmost importance. eBPF comes to the rescue Now, eBPF changes all of this. eBPF is hosted inside the kernel so the biggest advantage of eBPF is it can co-exist with Linux/OS without the need of using dedicated cores, skipping the Kernel stack or breaking tools used for ages by the community. Handling of new protocols and functionality can be done in the fly without waiting for kernel development to catch up.","title":"What is eBPF"},{"location":"ebpf/#what-is-ebpf","text":"eBPF has been making quite some news lately. An elegant way to extend the linux kernel (or windows) has far reaching implications. Although initially, eBPF was used to enhance system observability beyond existing tools, we will explore in this post how eBPF can be used for enhancing Linux networking performance. There are a lot of additional resources about eBPF in the eBPF project page .","title":"What is eBPF ??"},{"location":"ebpf/#a-quick-recap","text":"The hooks that are of particular interest for this discussion are NIC hook (invoked just after packet is received at NIC) and TC hook (invoked just before Linux starts processing packet with its TCP/IP stack). Programs loaded to the former hook are also known as XDP programs and to the latter are called eBPF TC. Although both use eBPF restricted C syntax, there are significant differences between these types. (We will cover it in a separate blog later). For now, we just need to remember that when dealing with container-to-container or container-to-outside communication eBPF-TC makes much more sense since memory allocation (for skb) will happen either way in such scenarios.","title":"A quick recap"},{"location":"ebpf/#the-performance-bottlenecks","text":"Coming back to the focus of our discussion which is of course performance, let us step back and take a look at why Linux sucks at networking performance (or rather why it could perform much faster). Linux networking evolved from the days of dial up modem networking when speed was not of utmost importance. Down the lane, code kept accumulating. Although it is extremely feature rich and RFC compliant, it hardly resembles a powerful data-path networking engine. The following figure shows a call-trace of Linux kernel networking stack: The point is it has become incredibly complex over the years. Once features like NAT, VXLAN, conntrack etc come into play, Linux networking stops scaling due to cache degradation, lock contention etc.","title":"The performance bottlenecks"},{"location":"ebpf/#one-problem-leads-to-the-another","text":"To avoid performance penalties, many user-space frameworks like DPDK have been widely used, which completely skip the linux kernel networking and directly process packets in the user-space. As simple as that may sound, there are some serious drawbacks in using such frameworks e.g need to dedicate cores (can\u2019t multitask), applications written on a specific user-space driver (PMD) might not run on another as it is, apps are also rendered incompatible across different DPDK releases frequently. Finally, there is a need to redo various parts of the TCP/IP stack and the provisioning involved. In short, it leads to a massive and completely unnecessary need of reinventing the wheel. We will have a detailed post later to discuss these factors. But for now, in short, if we are looking to get more out of a box than doing only networking, DPDK is not the right choice. In the age of distributed edge computing and immersive metaverse, the need to do more out of less is of utmost importance.","title":"One problem leads to the another"},{"location":"ebpf/#ebpf-comes-to-the-rescue","text":"Now, eBPF changes all of this. eBPF is hosted inside the kernel so the biggest advantage of eBPF is it can co-exist with Linux/OS without the need of using dedicated cores, skipping the Kernel stack or breaking tools used for ages by the community. Handling of new protocols and functionality can be done in the fly without waiting for kernel development to catch up.","title":"eBPF comes to the rescue"},{"location":"eks-external/","text":"Create an EKS cluster with ingress access enabled by loxilb (external-mode) This document details the steps to create an EKS cluster and allow external ingress access using loxilb running in external mode. loxilb will run as EC2 instances in EKS cluster's VPC while loxilb's operator, kube-loxilb, will run as a replica-set inside EKS cluster. Create EKS cluster with 4 worker nodes from a bastion node inside your VPC It is assumed that aws-cli, kubectl and eksctl are installed in a bastion node $ eksctl create cluster --version 1.24 --name loxilb-demo --vpc-nat-mode Single --region ap-northeast-2 --node-type t3.small --nodes 4 --with-oidc --managed Create kube config for kubectl access $ aws eks update-kubeconfig --region ap-northeast-2 --name loxilb-demo Double confirm the cluster created $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-2fpm4 2/2 Running 0 14m kube-system aws-node-6vhlr 2/2 Running 0 14m kube-system aws-node-9kzb2 2/2 Running 0 14m kube-system aws-node-vvkq5 2/2 Running 0 14m kube-system coredns-5ff5b8d45c-gj9kj 1/1 Running 0 21m kube-system coredns-5ff5b8d45c-p64fd 1/1 Running 0 21m kube-system kube-proxy-5j9gf 1/1 Running 0 14m kube-system kube-proxy-5tm8w 1/1 Running 0 14m kube-system kube-proxy-894k9 1/1 Running 0 14m kube-system kube-proxy-xgfb8 1/1 Running 0 14m Deploy loxilb as EC2 instances in EKS's VPC Create a file launch-loxilb.sh with the following contents (in bastion node) sudo apt-get update && apt-get install -y snapd sudo snap install docker sleep 30 sudo docker run -u root --cap-add SYS_ADMIN --net=host --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest Deploy loxilb ec2 instance(s) using the above init-script $ aws ec2 run-instances --image-id ami-01ed8ade75d4eee2f --count 1 --instance-type t3.medium --key-name aws-netlox --security-group-ids sg-0e2638db05b256476 --subnet-id subnet-0109b973f5f674f99 --associate-public-ip-address --user-data file://launch-loxilb.sh Note : subnet-id should be any subnet with public access enabled from the EKS cluster. Rest of the args can be changed as applicable Double confirm loxilb EC2 instances are running properly in amazon aws console or using aws cli. Disable source/dest check of the loxilb EC2 instances aws ec2 modify-network-interface-attribute --network-interface-id eni-02e1cbfa022eb0901 --no-source-dest-check Deploy loxilb's operator (kube-loxilb) Create a file kube-loxilb.yml with the following contents --- apiVersion: v1 kind: ServiceAccount metadata: name: kube-loxilb namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb rules: - apiGroups: - \"\" resources: - nodes verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - pods verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - endpoints - services - services/status verbs: - get - watch - list - patch - update - apiGroups: - gateway.networking.k8s.io resources: - gatewayclasses - gatewayclasses/status - gateways - gateways/status - tcproutes - udproutes verbs: [\"get\", \"watch\", \"list\", \"patch\", \"update\"] - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - get - watch - list - apiGroups: - authentication.k8s.io resources: - tokenreviews verbs: - create - apiGroups: - authorization.k8s.io resources: - subjectaccessreviews verbs: - create - apiGroups: - bgppeer.loxilb.io resources: - bgppeerservices verbs: - get - watch - list - create - update - delete --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-loxilb subjects: - kind: ServiceAccount name: kube-loxilb namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: kube-loxilb namespace: kube-system labels: app: kube-loxilb-app spec: replicas: 1 selector: matchLabels: app: kube-loxilb-app template: metadata: labels: app: kube-loxilb-app spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet tolerations: # Mark the pod as a critical add-on for rescheduling. - key: CriticalAddonsOnly operator: Exists priorityClassName: system-node-critical serviceAccountName: kube-loxilb terminationGracePeriodSeconds: 0 containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: - --loxiURL=http://192.168.31.175:11111 - --externalCIDR=0.0.0.0/32 - --setLBMode=5 #- --setRoles:0.0.0.0 resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"] Note1: --externalCIDR args can be set to any Public IP address via which any of the worker nodes can be accessed. It can be also set to simply 0.0.0.0/32 which means LB will be performed on any of the nodes where loxilb runs. The decision of which loxilb node/instance will be chosen as ingress in this case can be done by Route53/DNS. Note2: --loxiURL args should be set to privateIP address(es) of the loxilb ec2 instances accessible from the EKS cluster. Currently, kube-loxilb can't autodetect the EC2 instances running loxilb in external mode. Deploy kube-loxilb to EKS cluster $ kubectl apply -f kube-loxilb.yml serviceaccount/kube-loxilb created clusterrole.rbac.authorization.k8s.io/kube-loxilb created deployment.apps/kube-loxilb created Check the state of the EKS cluster $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-2fpm4 2/2 Running 0 14m kube-system aws-node-6vhlr 2/2 Running 0 14m kube-system aws-node-9kzb2 2/2 Running 0 14m kube-system aws-node-vvkq5 2/2 Running 0 14m kube-system coredns-5ff5b8d45c-gj9kj 1/1 Running 0 21m kube-system coredns-5ff5b8d45c-p64fd 1/1 Running 0 21m kube-system kube-proxy-5j9gf 1/1 Running 0 14m kube-system kube-proxy-5tm8w 1/1 Running 0 14m kube-system kube-proxy-894k9 1/1 Running 0 14m kube-system kube-proxy-xgfb8 1/1 Running 0 14m kube-system kube-loxilb-6477d6897f-vz74f 1/1 Running 0 5m Install a test service Create a file nginx.yml with the following contents: apiVersion: v1 kind: Service metadata: name: nginx-lb1 spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: nginx-test ports: - port: 55002 targetPort: 80 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: nginx-test labels: what: nginx-test spec: containers: - name: nginx-test image: nginx:stable ports: - containerPort: 80 Deploy test nginx service to EKS $ kubectl apply -f nginx.yml service/nginx-lb1 created Check the state of the EKS cluster $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default nginx-test 1/1 Running 0 50s kube-system aws-node-2fpm4 2/2 Running 0 14m kube-system aws-node-6vhlr 2/2 Running 0 14m kube-system aws-node-9kzb2 2/2 Running 0 14m kube-system aws-node-vvkq5 2/2 Running 0 14m kube-system coredns-5ff5b8d45c-gj9kj 1/1 Running 0 21m kube-system coredns-5ff5b8d45c-p64fd 1/1 Running 0 21m kube-system kube-proxy-5j9gf 1/1 Running 0 14m kube-system kube-proxy-5tm8w 1/1 Running 0 14m kube-system kube-proxy-894k9 1/1 Running 0 14m kube-system kube-proxy-xgfb8 1/1 Running 0 14m kube-system kube-loxilb-6477d6897f-vz74f 1/1 Running 0 5m Check the external service for service ingress (via loxilb) $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 10h nginx-lb1 LoadBalancer 10.100.244.105 llbanyextip 55005:30055/TCP 24s Test the service Try to access the service from outside (internet). We can use any public IP associated with any of the loxilb ec2 instances $ curl http://3.37.191.xx:55005 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> Note - We would need to make sure AWS security groups are setup properly to allow access for ingress traffic. Restricting loxilb service for a local-zone node-group For limiting loxilb services to a specific node group of a local-zone, we can use kubenetes node-labels to limit the endpoints of that service to that node-group only. For example, if all the nodes in a local-zone node-groups have a label node.kubernetes.io/local-zone2=true , then we can create a loxilb service with a following annotation : apiVersion: v1 kind: Service metadata: name: nginx-lb1 annotations: loxilb.io/nodelabel: \"node.kubernetes.io/local-zone2\" spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: nginx-test ports: - port: 55002 targetPort: 80 type: LoadBalancer This will make sure that loxilb will pick only the endpoint nodes which belong to that node-group only.","title":"Eks external"},{"location":"eks-external/#create-an-eks-cluster-with-ingress-access-enabled-by-loxilb-external-mode","text":"This document details the steps to create an EKS cluster and allow external ingress access using loxilb running in external mode. loxilb will run as EC2 instances in EKS cluster's VPC while loxilb's operator, kube-loxilb, will run as a replica-set inside EKS cluster.","title":"Create an EKS cluster with ingress access enabled by loxilb (external-mode)"},{"location":"eks-external/#create-eks-cluster-with-4-worker-nodes-from-a-bastion-node-inside-your-vpc","text":"It is assumed that aws-cli, kubectl and eksctl are installed in a bastion node $ eksctl create cluster --version 1.24 --name loxilb-demo --vpc-nat-mode Single --region ap-northeast-2 --node-type t3.small --nodes 4 --with-oidc --managed Create kube config for kubectl access $ aws eks update-kubeconfig --region ap-northeast-2 --name loxilb-demo Double confirm the cluster created $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-2fpm4 2/2 Running 0 14m kube-system aws-node-6vhlr 2/2 Running 0 14m kube-system aws-node-9kzb2 2/2 Running 0 14m kube-system aws-node-vvkq5 2/2 Running 0 14m kube-system coredns-5ff5b8d45c-gj9kj 1/1 Running 0 21m kube-system coredns-5ff5b8d45c-p64fd 1/1 Running 0 21m kube-system kube-proxy-5j9gf 1/1 Running 0 14m kube-system kube-proxy-5tm8w 1/1 Running 0 14m kube-system kube-proxy-894k9 1/1 Running 0 14m kube-system kube-proxy-xgfb8 1/1 Running 0 14m","title":"Create EKS cluster with 4 worker nodes from a bastion node inside your VPC"},{"location":"eks-external/#deploy-loxilb-as-ec2-instances-in-ekss-vpc","text":"Create a file launch-loxilb.sh with the following contents (in bastion node) sudo apt-get update && apt-get install -y snapd sudo snap install docker sleep 30 sudo docker run -u root --cap-add SYS_ADMIN --net=host --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest Deploy loxilb ec2 instance(s) using the above init-script $ aws ec2 run-instances --image-id ami-01ed8ade75d4eee2f --count 1 --instance-type t3.medium --key-name aws-netlox --security-group-ids sg-0e2638db05b256476 --subnet-id subnet-0109b973f5f674f99 --associate-public-ip-address --user-data file://launch-loxilb.sh","title":"Deploy loxilb as EC2 instances in EKS's VPC"},{"location":"eks-external/#note-subnet-id-should-be-any-subnet-with-public-access-enabled-from-the-eks-cluster-rest-of-the-args-can-be-changed-as-applicable","text":"Double confirm loxilb EC2 instances are running properly in amazon aws console or using aws cli. Disable source/dest check of the loxilb EC2 instances aws ec2 modify-network-interface-attribute --network-interface-id eni-02e1cbfa022eb0901 --no-source-dest-check","title":"Note : subnet-id should be any subnet with public access enabled from the EKS cluster. Rest of the args can be changed as applicable"},{"location":"eks-external/#deploy-loxilbs-operator-kube-loxilb","text":"Create a file kube-loxilb.yml with the following contents --- apiVersion: v1 kind: ServiceAccount metadata: name: kube-loxilb namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb rules: - apiGroups: - \"\" resources: - nodes verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - pods verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - endpoints - services - services/status verbs: - get - watch - list - patch - update - apiGroups: - gateway.networking.k8s.io resources: - gatewayclasses - gatewayclasses/status - gateways - gateways/status - tcproutes - udproutes verbs: [\"get\", \"watch\", \"list\", \"patch\", \"update\"] - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - get - watch - list - apiGroups: - authentication.k8s.io resources: - tokenreviews verbs: - create - apiGroups: - authorization.k8s.io resources: - subjectaccessreviews verbs: - create - apiGroups: - bgppeer.loxilb.io resources: - bgppeerservices verbs: - get - watch - list - create - update - delete --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-loxilb subjects: - kind: ServiceAccount name: kube-loxilb namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: kube-loxilb namespace: kube-system labels: app: kube-loxilb-app spec: replicas: 1 selector: matchLabels: app: kube-loxilb-app template: metadata: labels: app: kube-loxilb-app spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet tolerations: # Mark the pod as a critical add-on for rescheduling. - key: CriticalAddonsOnly operator: Exists priorityClassName: system-node-critical serviceAccountName: kube-loxilb terminationGracePeriodSeconds: 0 containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: - --loxiURL=http://192.168.31.175:11111 - --externalCIDR=0.0.0.0/32 - --setLBMode=5 #- --setRoles:0.0.0.0 resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"]","title":"Deploy loxilb's operator (kube-loxilb)"},{"location":"eks-external/#note1-externalcidr-args-can-be-set-to-any-public-ip-address-via-which-any-of-the-worker-nodes-can-be-accessed-it-can-be-also-set-to-simply-000032-which-means-lb-will-be-performed-on-any-of-the-nodes-where-loxilb-runs-the-decision-of-which-loxilb-nodeinstance-will-be-chosen-as-ingress-in-this-case-can-be-done-by-route53dns","text":"","title":"Note1: --externalCIDR args can be set to any Public IP address via which any of the worker nodes can be accessed. It can be also set to simply 0.0.0.0/32 which means LB will be performed on any of the nodes where loxilb runs. The decision of which loxilb node/instance will be chosen as ingress in this case can be done by Route53/DNS."},{"location":"eks-external/#note2-loxiurl-args-should-be-set-to-privateip-addresses-of-the-loxilb-ec2-instances-accessible-from-the-eks-cluster-currently-kube-loxilb-cant-autodetect-the-ec2-instances-running-loxilb-in-external-mode","text":"Deploy kube-loxilb to EKS cluster $ kubectl apply -f kube-loxilb.yml serviceaccount/kube-loxilb created clusterrole.rbac.authorization.k8s.io/kube-loxilb created deployment.apps/kube-loxilb created Check the state of the EKS cluster $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-2fpm4 2/2 Running 0 14m kube-system aws-node-6vhlr 2/2 Running 0 14m kube-system aws-node-9kzb2 2/2 Running 0 14m kube-system aws-node-vvkq5 2/2 Running 0 14m kube-system coredns-5ff5b8d45c-gj9kj 1/1 Running 0 21m kube-system coredns-5ff5b8d45c-p64fd 1/1 Running 0 21m kube-system kube-proxy-5j9gf 1/1 Running 0 14m kube-system kube-proxy-5tm8w 1/1 Running 0 14m kube-system kube-proxy-894k9 1/1 Running 0 14m kube-system kube-proxy-xgfb8 1/1 Running 0 14m kube-system kube-loxilb-6477d6897f-vz74f 1/1 Running 0 5m","title":"Note2: --loxiURL args should be set to privateIP address(es) of the loxilb ec2 instances accessible from the EKS cluster. Currently, kube-loxilb can't autodetect the EC2 instances running loxilb in external mode."},{"location":"eks-external/#install-a-test-service","text":"Create a file nginx.yml with the following contents: apiVersion: v1 kind: Service metadata: name: nginx-lb1 spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: nginx-test ports: - port: 55002 targetPort: 80 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: nginx-test labels: what: nginx-test spec: containers: - name: nginx-test image: nginx:stable ports: - containerPort: 80 Deploy test nginx service to EKS $ kubectl apply -f nginx.yml service/nginx-lb1 created Check the state of the EKS cluster $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default nginx-test 1/1 Running 0 50s kube-system aws-node-2fpm4 2/2 Running 0 14m kube-system aws-node-6vhlr 2/2 Running 0 14m kube-system aws-node-9kzb2 2/2 Running 0 14m kube-system aws-node-vvkq5 2/2 Running 0 14m kube-system coredns-5ff5b8d45c-gj9kj 1/1 Running 0 21m kube-system coredns-5ff5b8d45c-p64fd 1/1 Running 0 21m kube-system kube-proxy-5j9gf 1/1 Running 0 14m kube-system kube-proxy-5tm8w 1/1 Running 0 14m kube-system kube-proxy-894k9 1/1 Running 0 14m kube-system kube-proxy-xgfb8 1/1 Running 0 14m kube-system kube-loxilb-6477d6897f-vz74f 1/1 Running 0 5m Check the external service for service ingress (via loxilb) $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 10h nginx-lb1 LoadBalancer 10.100.244.105 llbanyextip 55005:30055/TCP 24s","title":"Install a test service"},{"location":"eks-external/#test-the-service","text":"Try to access the service from outside (internet). We can use any public IP associated with any of the loxilb ec2 instances $ curl http://3.37.191.xx:55005 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html>","title":"Test the service"},{"location":"eks-external/#note-we-would-need-to-make-sure-aws-security-groups-are-setup-properly-to-allow-access-for-ingress-traffic","text":"","title":"Note - We would need to make sure AWS security groups are setup properly to allow access for ingress traffic."},{"location":"eks-external/#restricting-loxilb-service-for-a-local-zone-node-group","text":"For limiting loxilb services to a specific node group of a local-zone, we can use kubenetes node-labels to limit the endpoints of that service to that node-group only. For example, if all the nodes in a local-zone node-groups have a label node.kubernetes.io/local-zone2=true , then we can create a loxilb service with a following annotation : apiVersion: v1 kind: Service metadata: name: nginx-lb1 annotations: loxilb.io/nodelabel: \"node.kubernetes.io/local-zone2\" spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: nginx-test ports: - port: 55002 targetPort: 80 type: LoadBalancer This will make sure that loxilb will pick only the endpoint nodes which belong to that node-group only.","title":"Restricting loxilb service for a local-zone node-group"},{"location":"eks-incluster/","text":"Create an EKS cluster with ingress access enabled by loxilb (incluster-mode) This document details the steps to create an EKS cluster and allow external ingress access using loxilb running in incluster mode. loxilb will run as a daemon-set in all the worker nodes while loxilb's operator, kube-loxilb, will run as a replica-set. Although loxilb has built-in support for associating (floating) AWS EIPs to private subnet addresses of EC2 instances, this is not considered in this particular scenario. But if it is needed, the functionality can be enabled by changing a few parameters in yaml config files. Create EKS cluster with 4 worker nodes from a bastion node inside your VPC It is assumed that aws-cli, kubectl and eksctl are installed in a bastion node $ eksctl create cluster --version 1.24 --name loxilb-demo --vpc-nat-mode Single --region ap-northeast-2 --node-type t3.small --nodes 4 --with-oidc --managed Create kube config for kubectl access $ aws eks update-kubeconfig --region ap-northeast-2 --name loxilb-demo Double confirm the cluster created $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-2fpm4 2/2 Running 0 14m kube-system aws-node-6vhlr 2/2 Running 0 14m kube-system aws-node-9kzb2 2/2 Running 0 14m kube-system aws-node-vvkq5 2/2 Running 0 14m kube-system coredns-5ff5b8d45c-gj9kj 1/1 Running 0 21m kube-system coredns-5ff5b8d45c-p64fd 1/1 Running 0 21m kube-system kube-proxy-5j9gf 1/1 Running 0 14m kube-system kube-proxy-5tm8w 1/1 Running 0 14m kube-system kube-proxy-894k9 1/1 Running 0 14m kube-system kube-proxy-xgfb8 1/1 Running 0 14m Deploy loxilb as a daemon-set Create a file loxilb.yml with the following contents apiVersion: apps/v1 kind: DaemonSet metadata: name: loxilb-lb namespace: kube-system spec: selector: matchLabels: app: loxilb-app template: metadata: name: loxilb-lb labels: app: loxilb-app spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet containers: - name: loxilb-app image: \"ghcr.io/loxilb-io/loxilb:latest\" imagePullPolicy: Always command: [ \"/root/loxilb-io/loxilb/loxilb\", \"--egr-hooks\", \"--blacklist=cni[0-9a-z]|veth.|flannel.|eni.\" ] ports: - containerPort: 11111 - containerPort: 179 securityContext: privileged: true capabilities: add: - SYS_ADMIN --- apiVersion: v1 kind: Service metadata: name: loxilb-lb-service namespace: kube-system spec: clusterIP: None selector: app: loxilb-app ports: - name: loxilb-app port: 11111 targetPort: 11111 protocol: TCP - name: loxilb-app-bgp port: 179 targetPort: 179 protocol: TCP Deploy loxilb $ kubectl apply -f loxilb.yml daemonset.apps/loxilb-lb created service/loxilb-lb-service created Double confirm loxilb pods are running properly as a daemonset $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-2fpm4 2/2 Running 0 19m kube-system aws-node-6vhlr 2/2 Running 0 19m kube-system aws-node-9kzb2 2/2 Running 0 19m kube-system aws-node-vvkq5 2/2 Running 0 19m kube-system coredns-5ff5b8d45c-gj9kj 1/1 Running 0 26m kube-system coredns-5ff5b8d45c-p64fd 1/1 Running 0 26m kube-system kube-proxy-5j9gf 1/1 Running 0 19m kube-system kube-proxy-5tm8w 1/1 Running 0 19m kube-system kube-proxy-894k9 1/1 Running 0 19m kube-system kube-proxy-xgfb8 1/1 Running 0 19m kube-system loxilb-lb-7s45t 1/1 Running 0 18s kube-system loxilb-lb-fp6nv 1/1 Running 0 18s kube-system loxilb-lb-pbzql 1/1 Running 0 18s kube-system loxilb-lb-zzth8 1/1 Running 0 18s Deploy loxilb's operator (kube-loxilb) Create a file kube-loxilb.yml with the following contents --- apiVersion: v1 kind: ServiceAccount metadata: name: kube-loxilb namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb rules: - apiGroups: - \"\" resources: - nodes verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - pods verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - endpoints - services - services/status verbs: - get - watch - list - patch - update - apiGroups: - gateway.networking.k8s.io resources: - gatewayclasses - gatewayclasses/status - gateways - gateways/status - tcproutes - udproutes verbs: [\"get\", \"watch\", \"list\", \"patch\", \"update\"] - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - get - watch - list - apiGroups: - authentication.k8s.io resources: - tokenreviews verbs: - create - apiGroups: - authorization.k8s.io resources: - subjectaccessreviews verbs: - create - apiGroups: - bgppeer.loxilb.io resources: - bgppeerservices verbs: - get - watch - list - create - update - delete --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-loxilb subjects: - kind: ServiceAccount name: kube-loxilb namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: kube-loxilb namespace: kube-system labels: app: kube-loxilb-app spec: replicas: 1 selector: matchLabels: app: kube-loxilb-app template: metadata: labels: app: kube-loxilb-app spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet tolerations: # Mark the pod as a critical add-on for rescheduling. - key: CriticalAddonsOnly operator: Exists priorityClassName: system-node-critical serviceAccountName: kube-loxilb terminationGracePeriodSeconds: 0 containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: - --externalCIDR=0.0.0.0/32 - --setLBMode=5 #- --setRoles:0.0.0.0 resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"] Note: --externalCIDR args can be set to any Public IP address via which any of the worker nodes can be accessed. It can be also set to simply 0.0.0.0/32 which means LB will be performed on any of the nodes where loxilb runs. The decision of which loxilb node/instance will be chosen as ingress in this case can be done by Route53/DNS. Deploy kube-loxilb to EKS cluster $ kubectl apply -f kube-loxilb.yml serviceaccount/kube-loxilb created clusterrole.rbac.authorization.k8s.io/kube-loxilb created deployment.apps/kube-loxilb created Check the state of the EKS cluster $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-2fpm4 2/2 Running 0 35m kube-system aws-node-6vhlr 2/2 Running 0 35m kube-system aws-node-9kzb2 2/2 Running 0 35m kube-system aws-node-vvkq5 2/2 Running 0 35m kube-system coredns-5ff5b8d45c-gj9kj 1/1 Running 0 42m kube-system coredns-5ff5b8d45c-p64fd 1/1 Running 0 42m kube-system kube-loxilb-c7cd4fccd-hjg8w 1/1 Running 0 116s kube-system kube-proxy-5j9gf 1/1 Running 0 35m kube-system kube-proxy-5tm8w 1/1 Running 0 35m kube-system kube-proxy-894k9 1/1 Running 0 35m kube-system kube-proxy-xgfb8 1/1 Running 0 35m kube-system loxilb-lb-7s45t 1/1 Running 0 16m kube-system loxilb-lb-fp6nv 1/1 Running 0 16m kube-system loxilb-lb-pbzql 1/1 Running 0 16m kube-system loxilb-lb-zzth8 1/1 Running 0 16m Install a test service Create a file nginx.yml with the following contents: apiVersion: v1 kind: Service metadata: name: nginx-lb1 spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: nginx-test ports: - port: 55002 targetPort: 80 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: nginx-test labels: what: nginx-test spec: containers: - name: nginx-test image: nginx:stable ports: - containerPort: 80 Deploy test nginx service to EKS $ kubectl apply -f nginx.yml service/nginx-lb1 created Check the state of the EKS cluster $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default nginx-test 1/1 Running 0 50s kube-system aws-node-2fpm4 2/2 Running 0 39m kube-system aws-node-6vhlr 2/2 Running 0 39m kube-system aws-node-9kzb2 2/2 Running 0 39m kube-system aws-node-vvkq5 2/2 Running 0 39m kube-system coredns-5ff5b8d45c-gj9kj 1/1 Running 0 46m kube-system coredns-5ff5b8d45c-p64fd 1/1 Running 0 46m kube-system kube-loxilb-c7cd4fccd-hjg8w 1/1 Running 0 6m13s kube-system kube-proxy-5j9gf 1/1 Running 0 39m kube-system kube-proxy-5tm8w 1/1 Running 0 39m kube-system kube-proxy-894k9 1/1 Running 0 39m kube-system kube-proxy-xgfb8 1/1 Running 0 39m kube-system loxilb-lb-7s45t 1/1 Running 0 20m kube-system loxilb-lb-fp6nv 1/1 Running 0 20m kube-system loxilb-lb-pbzql 1/1 Running 0 20m kube-system loxilb-lb-zzth8 1/1 Running 0 20m Check the external service for service ingress (via loxilb) $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 6h19m nginx-lb1 LoadBalancer 10.100.63.175 llbanyextip 55002:32704/TCP 4hs Test the service Try to access the service from outside (internet). We can use any public IP associated with the cluster (worker) nodes $ curl http://43.201.76.xx:55002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> Note - We would need to make sure AWS security groups are setup properly to allow access for ingress traffic. Restricting loxilb service for a local-zone node-group For limiting loxilb services to a specific node group of a local-zone, we can use kubenetes node-labels to limit the endpoints of that service to that node-group only. For example, if all the nodes in a local-zone node-groups have a label node.kubernetes.io/local-zone2=true , then we can create a loxilb service with a following annotation : apiVersion: v1 kind: Service metadata: name: nginx-lb1 loxilb.io/nodelabel: \"node.kubernetes.io/local-zone2\" spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: nginx-test ports: - port: 55002 targetPort: 80 type: LoadBalancer This will make sure that loxilb will pick only the endpoint nodes which belong to that node-group only.","title":"Eks incluster"},{"location":"eks-incluster/#create-an-eks-cluster-with-ingress-access-enabled-by-loxilb-incluster-mode","text":"This document details the steps to create an EKS cluster and allow external ingress access using loxilb running in incluster mode. loxilb will run as a daemon-set in all the worker nodes while loxilb's operator, kube-loxilb, will run as a replica-set. Although loxilb has built-in support for associating (floating) AWS EIPs to private subnet addresses of EC2 instances, this is not considered in this particular scenario. But if it is needed, the functionality can be enabled by changing a few parameters in yaml config files.","title":"Create an EKS cluster with ingress access enabled by loxilb (incluster-mode)"},{"location":"eks-incluster/#create-eks-cluster-with-4-worker-nodes-from-a-bastion-node-inside-your-vpc","text":"It is assumed that aws-cli, kubectl and eksctl are installed in a bastion node $ eksctl create cluster --version 1.24 --name loxilb-demo --vpc-nat-mode Single --region ap-northeast-2 --node-type t3.small --nodes 4 --with-oidc --managed Create kube config for kubectl access $ aws eks update-kubeconfig --region ap-northeast-2 --name loxilb-demo Double confirm the cluster created $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-2fpm4 2/2 Running 0 14m kube-system aws-node-6vhlr 2/2 Running 0 14m kube-system aws-node-9kzb2 2/2 Running 0 14m kube-system aws-node-vvkq5 2/2 Running 0 14m kube-system coredns-5ff5b8d45c-gj9kj 1/1 Running 0 21m kube-system coredns-5ff5b8d45c-p64fd 1/1 Running 0 21m kube-system kube-proxy-5j9gf 1/1 Running 0 14m kube-system kube-proxy-5tm8w 1/1 Running 0 14m kube-system kube-proxy-894k9 1/1 Running 0 14m kube-system kube-proxy-xgfb8 1/1 Running 0 14m","title":"Create EKS cluster with 4 worker nodes from a bastion node inside your VPC"},{"location":"eks-incluster/#deploy-loxilb-as-a-daemon-set","text":"Create a file loxilb.yml with the following contents apiVersion: apps/v1 kind: DaemonSet metadata: name: loxilb-lb namespace: kube-system spec: selector: matchLabels: app: loxilb-app template: metadata: name: loxilb-lb labels: app: loxilb-app spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet containers: - name: loxilb-app image: \"ghcr.io/loxilb-io/loxilb:latest\" imagePullPolicy: Always command: [ \"/root/loxilb-io/loxilb/loxilb\", \"--egr-hooks\", \"--blacklist=cni[0-9a-z]|veth.|flannel.|eni.\" ] ports: - containerPort: 11111 - containerPort: 179 securityContext: privileged: true capabilities: add: - SYS_ADMIN --- apiVersion: v1 kind: Service metadata: name: loxilb-lb-service namespace: kube-system spec: clusterIP: None selector: app: loxilb-app ports: - name: loxilb-app port: 11111 targetPort: 11111 protocol: TCP - name: loxilb-app-bgp port: 179 targetPort: 179 protocol: TCP Deploy loxilb $ kubectl apply -f loxilb.yml daemonset.apps/loxilb-lb created service/loxilb-lb-service created Double confirm loxilb pods are running properly as a daemonset $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-2fpm4 2/2 Running 0 19m kube-system aws-node-6vhlr 2/2 Running 0 19m kube-system aws-node-9kzb2 2/2 Running 0 19m kube-system aws-node-vvkq5 2/2 Running 0 19m kube-system coredns-5ff5b8d45c-gj9kj 1/1 Running 0 26m kube-system coredns-5ff5b8d45c-p64fd 1/1 Running 0 26m kube-system kube-proxy-5j9gf 1/1 Running 0 19m kube-system kube-proxy-5tm8w 1/1 Running 0 19m kube-system kube-proxy-894k9 1/1 Running 0 19m kube-system kube-proxy-xgfb8 1/1 Running 0 19m kube-system loxilb-lb-7s45t 1/1 Running 0 18s kube-system loxilb-lb-fp6nv 1/1 Running 0 18s kube-system loxilb-lb-pbzql 1/1 Running 0 18s kube-system loxilb-lb-zzth8 1/1 Running 0 18s","title":"Deploy loxilb as a daemon-set"},{"location":"eks-incluster/#deploy-loxilbs-operator-kube-loxilb","text":"Create a file kube-loxilb.yml with the following contents --- apiVersion: v1 kind: ServiceAccount metadata: name: kube-loxilb namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb rules: - apiGroups: - \"\" resources: - nodes verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - pods verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - endpoints - services - services/status verbs: - get - watch - list - patch - update - apiGroups: - gateway.networking.k8s.io resources: - gatewayclasses - gatewayclasses/status - gateways - gateways/status - tcproutes - udproutes verbs: [\"get\", \"watch\", \"list\", \"patch\", \"update\"] - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - get - watch - list - apiGroups: - authentication.k8s.io resources: - tokenreviews verbs: - create - apiGroups: - authorization.k8s.io resources: - subjectaccessreviews verbs: - create - apiGroups: - bgppeer.loxilb.io resources: - bgppeerservices verbs: - get - watch - list - create - update - delete --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-loxilb subjects: - kind: ServiceAccount name: kube-loxilb namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: kube-loxilb namespace: kube-system labels: app: kube-loxilb-app spec: replicas: 1 selector: matchLabels: app: kube-loxilb-app template: metadata: labels: app: kube-loxilb-app spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet tolerations: # Mark the pod as a critical add-on for rescheduling. - key: CriticalAddonsOnly operator: Exists priorityClassName: system-node-critical serviceAccountName: kube-loxilb terminationGracePeriodSeconds: 0 containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: - --externalCIDR=0.0.0.0/32 - --setLBMode=5 #- --setRoles:0.0.0.0 resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"]","title":"Deploy loxilb's operator (kube-loxilb)"},{"location":"eks-incluster/#note-externalcidr-args-can-be-set-to-any-public-ip-address-via-which-any-of-the-worker-nodes-can-be-accessed-it-can-be-also-set-to-simply-000032-which-means-lb-will-be-performed-on-any-of-the-nodes-where-loxilb-runs-the-decision-of-which-loxilb-nodeinstance-will-be-chosen-as-ingress-in-this-case-can-be-done-by-route53dns","text":"Deploy kube-loxilb to EKS cluster $ kubectl apply -f kube-loxilb.yml serviceaccount/kube-loxilb created clusterrole.rbac.authorization.k8s.io/kube-loxilb created deployment.apps/kube-loxilb created Check the state of the EKS cluster $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-2fpm4 2/2 Running 0 35m kube-system aws-node-6vhlr 2/2 Running 0 35m kube-system aws-node-9kzb2 2/2 Running 0 35m kube-system aws-node-vvkq5 2/2 Running 0 35m kube-system coredns-5ff5b8d45c-gj9kj 1/1 Running 0 42m kube-system coredns-5ff5b8d45c-p64fd 1/1 Running 0 42m kube-system kube-loxilb-c7cd4fccd-hjg8w 1/1 Running 0 116s kube-system kube-proxy-5j9gf 1/1 Running 0 35m kube-system kube-proxy-5tm8w 1/1 Running 0 35m kube-system kube-proxy-894k9 1/1 Running 0 35m kube-system kube-proxy-xgfb8 1/1 Running 0 35m kube-system loxilb-lb-7s45t 1/1 Running 0 16m kube-system loxilb-lb-fp6nv 1/1 Running 0 16m kube-system loxilb-lb-pbzql 1/1 Running 0 16m kube-system loxilb-lb-zzth8 1/1 Running 0 16m","title":"Note: --externalCIDR args can be set to any Public IP address via which any of the worker nodes can be accessed. It can be also set to simply 0.0.0.0/32 which means LB will be performed on any of the nodes where loxilb runs. The decision of which loxilb node/instance will be chosen as ingress in this case can be done by Route53/DNS."},{"location":"eks-incluster/#install-a-test-service","text":"Create a file nginx.yml with the following contents: apiVersion: v1 kind: Service metadata: name: nginx-lb1 spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: nginx-test ports: - port: 55002 targetPort: 80 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: nginx-test labels: what: nginx-test spec: containers: - name: nginx-test image: nginx:stable ports: - containerPort: 80 Deploy test nginx service to EKS $ kubectl apply -f nginx.yml service/nginx-lb1 created Check the state of the EKS cluster $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default nginx-test 1/1 Running 0 50s kube-system aws-node-2fpm4 2/2 Running 0 39m kube-system aws-node-6vhlr 2/2 Running 0 39m kube-system aws-node-9kzb2 2/2 Running 0 39m kube-system aws-node-vvkq5 2/2 Running 0 39m kube-system coredns-5ff5b8d45c-gj9kj 1/1 Running 0 46m kube-system coredns-5ff5b8d45c-p64fd 1/1 Running 0 46m kube-system kube-loxilb-c7cd4fccd-hjg8w 1/1 Running 0 6m13s kube-system kube-proxy-5j9gf 1/1 Running 0 39m kube-system kube-proxy-5tm8w 1/1 Running 0 39m kube-system kube-proxy-894k9 1/1 Running 0 39m kube-system kube-proxy-xgfb8 1/1 Running 0 39m kube-system loxilb-lb-7s45t 1/1 Running 0 20m kube-system loxilb-lb-fp6nv 1/1 Running 0 20m kube-system loxilb-lb-pbzql 1/1 Running 0 20m kube-system loxilb-lb-zzth8 1/1 Running 0 20m Check the external service for service ingress (via loxilb) $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 6h19m nginx-lb1 LoadBalancer 10.100.63.175 llbanyextip 55002:32704/TCP 4hs","title":"Install a test service"},{"location":"eks-incluster/#test-the-service","text":"Try to access the service from outside (internet). We can use any public IP associated with the cluster (worker) nodes $ curl http://43.201.76.xx:55002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html>","title":"Test the service"},{"location":"eks-incluster/#note-we-would-need-to-make-sure-aws-security-groups-are-setup-properly-to-allow-access-for-ingress-traffic","text":"","title":"Note - We would need to make sure AWS security groups are setup properly to allow access for ingress traffic."},{"location":"eks-incluster/#restricting-loxilb-service-for-a-local-zone-node-group","text":"For limiting loxilb services to a specific node group of a local-zone, we can use kubenetes node-labels to limit the endpoints of that service to that node-group only. For example, if all the nodes in a local-zone node-groups have a label node.kubernetes.io/local-zone2=true , then we can create a loxilb service with a following annotation : apiVersion: v1 kind: Service metadata: name: nginx-lb1 loxilb.io/nodelabel: \"node.kubernetes.io/local-zone2\" spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: nginx-test ports: - port: 55002 targetPort: 80 type: LoadBalancer This will make sure that loxilb will pick only the endpoint nodes which belong to that node-group only.","title":"Restricting loxilb service for a local-zone node-group"},{"location":"ext-ep/","text":"In Kubernetes, there are two key concepts - Service and Endpoint . What is Service? A \"Service\" is a method that exposes an application running in one or more pods. What is an Endpoint? An \"Endpoint\" defines a list of network endpoints(IP address and port), typically referenced by a Service to define which Pods the traffic can be sent to. When we create a service in Kubernetes, usually we do not have to worry about the Endpoints' management as it is taken care by Kubernetes itself. But, sometimes not all the services run in a single cluster, some of them are hosted in other cluster(s) e.g. DB, storage, web services, trancoder etc. When endpoints are outside of the Kubernetes cluster, Endpoint objects can still be used to define and manage those external endpoints. This scenario is common when Kubernetes services need to interact with external systems, APIs, or services located outside of the cluster. Here's a practical example: Suppose you have a Kubernetes cluster hosting a microservices-based application, and one of the services needs to communicate with an external database hosted outside of the cluster. In this case, you can use an Endpoint object to define the external database endpoint within Kubernetes. In that case, your cloud-native apps needs to connect to the external services with external endpoints. Service with External Endpoint You can create an external service with loxilb as well. For this, You can simply create an Endpoint Object and then create a service using this endpoint object: endpoint.yml apiVersion: v1 kind: Endpoints metadata: name: ext-tcp-lb subsets: - addresses: - ip: 192.168.82.2 ports: - port: 80 Create endpoint object: $ kubectl apply -f endpoint.yml View endpoints: $ kubectl get ep NAME ENDPOINTS AGE kubernetes 10.0.2.15:6443 16m ext-tcp-lb 192.168.82.2:80 16m service.yml apiVersion: v1 kind: Service metadata: name: ext-tcp-lb spec: loadBalancerClass: loxilb.io/loxilb type: LoadBalancer ports: - protocol: TCP port: 8000 targetPort: 80 Create Service: $ kubectl apply -f service.yml View Service: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 16m ext-tcp-lb LoadBalancer 10.43.164.108 llb-20.20.20.1 8000:30355/TCP 15m","title":"How-To - access end-points outside K8s"},{"location":"ext-ep/#what-is-service","text":"A \"Service\" is a method that exposes an application running in one or more pods.","title":"What is Service?"},{"location":"ext-ep/#what-is-an-endpoint","text":"An \"Endpoint\" defines a list of network endpoints(IP address and port), typically referenced by a Service to define which Pods the traffic can be sent to. When we create a service in Kubernetes, usually we do not have to worry about the Endpoints' management as it is taken care by Kubernetes itself. But, sometimes not all the services run in a single cluster, some of them are hosted in other cluster(s) e.g. DB, storage, web services, trancoder etc. When endpoints are outside of the Kubernetes cluster, Endpoint objects can still be used to define and manage those external endpoints. This scenario is common when Kubernetes services need to interact with external systems, APIs, or services located outside of the cluster. Here's a practical example: Suppose you have a Kubernetes cluster hosting a microservices-based application, and one of the services needs to communicate with an external database hosted outside of the cluster. In this case, you can use an Endpoint object to define the external database endpoint within Kubernetes. In that case, your cloud-native apps needs to connect to the external services with external endpoints.","title":"What is an Endpoint?"},{"location":"ext-ep/#service-with-external-endpoint","text":"You can create an external service with loxilb as well. For this, You can simply create an Endpoint Object and then create a service using this endpoint object: endpoint.yml apiVersion: v1 kind: Endpoints metadata: name: ext-tcp-lb subsets: - addresses: - ip: 192.168.82.2 ports: - port: 80 Create endpoint object: $ kubectl apply -f endpoint.yml View endpoints: $ kubectl get ep NAME ENDPOINTS AGE kubernetes 10.0.2.15:6443 16m ext-tcp-lb 192.168.82.2:80 16m service.yml apiVersion: v1 kind: Service metadata: name: ext-tcp-lb spec: loadBalancerClass: loxilb.io/loxilb type: LoadBalancer ports: - protocol: TCP port: 8000 targetPort: 80 Create Service: $ kubectl apply -f service.yml View Service: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 16m ext-tcp-lb LoadBalancer 10.43.164.108 llb-20.20.20.1 8000:30355/TCP 15m","title":"Service with External Endpoint"},{"location":"faq/","text":"loxilb FAQs Does loxilb depend on what kind of CNI is deployed in the cluster ? Yes, loxilb configuration and operation might be related to which CNI (Calico, Cilium etc) is in use. loxilb just needs a way to find a route to its end-points. This also depends on how the network topology is laid out. For example, if a separated network for nodePort and external LB services is in effect or not. We will have a detailed guide on best practices for loxilb deployment soon. In the meantime, kindly reach out to us via github or loxilb forum Can loxilb be possibly run outside the released docker image ? Yes, loxilb can be run outside the provided docker image. Docker image gives it good portability across various linux like OS's without any performance impact. However, if need is to run outside its own docker, kindly follow README of various loxilb-io repositories. Can loxilb also act as a CNI ? loxilb supports many functionalities of a CNI but loxilb dev team is happy solving external LB and related connectivity problems for the time being. If there is a future requirement not met by currently available CNIs, we might chip in as well Is there a commercially supported version of loxilb ? At this point of time, loxilb-team is working hard to provide a high-quality open-source product. If users need commercial support, kindly get in touch with us Can loxilb run in a standalone mode (without Kubernetes) ? Very much so. loxilb can run in a standalone mode. Please follow various guides available in loxilb repo to run loxilb in a standalone mode. How loxilb ensures conformance wtih Kubernetes ? loxilb uses kubetest/kubetest2 plus various other test-utilities as part of its CI/CD workflows. We are also planning to get ourselves officially supported by distros like RedHat Openshift Where is loxilb deployed so far ? loxilb is currently used in academia for R&D and various organizations are in process of using it for PoCs. We will update the list of deployments as and when they are officially known","title":"Frequenctly Asked Questions- FAQs"},{"location":"faq/#loxilb-faqs","text":"Does loxilb depend on what kind of CNI is deployed in the cluster ? Yes, loxilb configuration and operation might be related to which CNI (Calico, Cilium etc) is in use. loxilb just needs a way to find a route to its end-points. This also depends on how the network topology is laid out. For example, if a separated network for nodePort and external LB services is in effect or not. We will have a detailed guide on best practices for loxilb deployment soon. In the meantime, kindly reach out to us via github or loxilb forum Can loxilb be possibly run outside the released docker image ? Yes, loxilb can be run outside the provided docker image. Docker image gives it good portability across various linux like OS's without any performance impact. However, if need is to run outside its own docker, kindly follow README of various loxilb-io repositories. Can loxilb also act as a CNI ? loxilb supports many functionalities of a CNI but loxilb dev team is happy solving external LB and related connectivity problems for the time being. If there is a future requirement not met by currently available CNIs, we might chip in as well Is there a commercially supported version of loxilb ? At this point of time, loxilb-team is working hard to provide a high-quality open-source product. If users need commercial support, kindly get in touch with us Can loxilb run in a standalone mode (without Kubernetes) ? Very much so. loxilb can run in a standalone mode. Please follow various guides available in loxilb repo to run loxilb in a standalone mode. How loxilb ensures conformance wtih Kubernetes ? loxilb uses kubetest/kubetest2 plus various other test-utilities as part of its CI/CD workflows. We are also planning to get ourselves officially supported by distros like RedHat Openshift Where is loxilb deployed so far ? loxilb is currently used in academia for R&D and various organizations are in process of using it for PoCs. We will update the list of deployments as and when they are officially known","title":"loxilb FAQs"},{"location":"gtp/","text":"Creating a simple test topology for testing GTP with loxilb To test loxilb in a completely virtual environment, it is possible to quickly create a virtual test topology. We will explain the steps required to create a very simple topology (more complex topologies can be built using this example) : graph LR; UE1[UE1<br>32.32.32.1]-->B[llb1<br>10.10.10.59/24]; UE2[UE2<br>31.31.31.1]-->B; B-- GTP Tunnel ---C[llb2<br>10.10.10.56/24] C-->D[EP1<br>31.31.31.1]; C-->E[EP2<br>32.32.32.1]; C-->F[EP3<br>17.17.17.1]; Prerequisites : The system should be x86 based (bare-metal or virtual) Docker should be preinstalled Next step is to run the following script to create and configure the above topology. Please refer scenario3 in loxilb/cicd script Script will spawn dockers for UEs, loxilbs and endpoints. In the script, UEs will try to access service IP(88.88.88.88). We are creating sessions and configuring load-balancer rule inside loxilb docker as follows : dexec=\"docker exec -it \" ##llb1 config #Creating session for ue1 $dexec llb1 loxicmd create session user1 88.88.88.88 --accessNetworkTunnel 1:10.10.10.56 --coreNetworkTunnel=1:10.10.10.59 #Creating ULCL filter with ue1 IP $dexec llb1 loxicmd create sessionulcl user1 --ulclArgs=11:32.32.32.1 #Creating session for ue2 $dexec llb1 loxicmd create session user2 88.88.88.88 --accessNetworkTunnel 2:10.10.10.56 --coreNetworkTunnel=2:10.10.10.59 #Creating ULCL filter with ue2 IP $dexec llb1 loxicmd create sessionulcl user2 --ulclArgs=12:31.31.31.1 ##llb2 config #Creating session for ue1 $dexec llb2 loxicmd create session user1 32.32.32.1 --accessNetworkTunnel 1:10.10.10.59 --coreNetworkTunnel=1:10.10.10.56 #Creating ULCL filter with service IP for ue1 $dexec llb2 loxicmd create sessionulcl user1 --ulclArgs=11:88.88.88.88 #Creating session for ue1 $dexec llb2 loxicmd create session user2 31.31.31.1 --accessNetworkTunnel 2:10.10.10.59 --coreNetworkTunnel=2:10.10.10.56 #Creating ULCL filter with service IP for ue2 $dexec llb2 loxicmd create sessionulcl user2 --ulclArgs=12:88.88.88.88 ##Create LB rule $dexec llb2 loxicmd create lb 88.88.88.88 --tcp=2020:8080 --endpoints=25.25.25.1:1,26.26.26.1:1,27.27.27.1:1 So, we now have two instances loxilb running as a docker. First instance of loxilb, llb1 is simulated as gNB as it is used to encap the incoming traffic from UE1 or UE2. Breakout, forward or loadbalancer rule can be configured on second instance llb2 We can run any workloads as we wish inside the host containers and start testing loxilb.","title":"Creating a simple test topology for testing GTP with loxilb"},{"location":"gtp/#creating-a-simple-test-topology-for-testing-gtp-with-loxilb","text":"To test loxilb in a completely virtual environment, it is possible to quickly create a virtual test topology. We will explain the steps required to create a very simple topology (more complex topologies can be built using this example) : graph LR; UE1[UE1<br>32.32.32.1]-->B[llb1<br>10.10.10.59/24]; UE2[UE2<br>31.31.31.1]-->B; B-- GTP Tunnel ---C[llb2<br>10.10.10.56/24] C-->D[EP1<br>31.31.31.1]; C-->E[EP2<br>32.32.32.1]; C-->F[EP3<br>17.17.17.1]; Prerequisites : The system should be x86 based (bare-metal or virtual) Docker should be preinstalled","title":"Creating a simple test topology for testing GTP with loxilb"},{"location":"gtp/#next-step-is-to-run-the-following-script-to-create-and-configure-the-above-topology","text":"Please refer scenario3 in loxilb/cicd script Script will spawn dockers for UEs, loxilbs and endpoints. In the script, UEs will try to access service IP(88.88.88.88). We are creating sessions and configuring load-balancer rule inside loxilb docker as follows : dexec=\"docker exec -it \" ##llb1 config #Creating session for ue1 $dexec llb1 loxicmd create session user1 88.88.88.88 --accessNetworkTunnel 1:10.10.10.56 --coreNetworkTunnel=1:10.10.10.59 #Creating ULCL filter with ue1 IP $dexec llb1 loxicmd create sessionulcl user1 --ulclArgs=11:32.32.32.1 #Creating session for ue2 $dexec llb1 loxicmd create session user2 88.88.88.88 --accessNetworkTunnel 2:10.10.10.56 --coreNetworkTunnel=2:10.10.10.59 #Creating ULCL filter with ue2 IP $dexec llb1 loxicmd create sessionulcl user2 --ulclArgs=12:31.31.31.1 ##llb2 config #Creating session for ue1 $dexec llb2 loxicmd create session user1 32.32.32.1 --accessNetworkTunnel 1:10.10.10.59 --coreNetworkTunnel=1:10.10.10.56 #Creating ULCL filter with service IP for ue1 $dexec llb2 loxicmd create sessionulcl user1 --ulclArgs=11:88.88.88.88 #Creating session for ue1 $dexec llb2 loxicmd create session user2 31.31.31.1 --accessNetworkTunnel 2:10.10.10.59 --coreNetworkTunnel=2:10.10.10.56 #Creating ULCL filter with service IP for ue2 $dexec llb2 loxicmd create sessionulcl user2 --ulclArgs=12:88.88.88.88 ##Create LB rule $dexec llb2 loxicmd create lb 88.88.88.88 --tcp=2020:8080 --endpoints=25.25.25.1:1,26.26.26.1:1,27.27.27.1:1 So, we now have two instances loxilb running as a docker. First instance of loxilb, llb1 is simulated as gNB as it is used to encap the incoming traffic from UE1 or UE2. Breakout, forward or loadbalancer rule can be configured on second instance llb2 We can run any workloads as we wish inside the host containers and start testing loxilb.","title":"Next step is to run the following script to create and configure the above topology."},{"location":"ha-deploy/","text":"How to deploy loxilb with High Availability This article describes different scenarios about how to deploy loxilb with High Availability. Before continuing to this page, all readers are advised to have a basic understanding about kube-loxilb and the different NAT modes supported by loxilb. loxilb can run in-cluster or external to kubernetes cluster depending on architectural choices. For this documentation, we have assumed an incluster deployment wherever applicable but similar configuration should suffice for an external deployment as well. Scenario 1 - Flat L2 Networking (active-backup) Scenario 2 - L3 network (active-backup mode using BGP) Scenario 3 - L3 network (active-active with BGP ECMP) Scenario 4 - ACTIVE-BACKUP with Connection Sync Scenario 5 - ACTIVE-BACKUP with Fast Failover Detection(BFD) Scenario 1 - Flat L2 Networking (active-backup) Setup For this deployment scenario, kubernetes and loxilb are setup as follows: Kubernetes uses a cluster with 2 Master Nodes and 2 Worker Nodes, all the nodes use the same 192.168.80.0/24 subnet. In this scenario, loxilb will be deployed as a DaemonSet in all the master nodes. And, kube-loxilb will be deployed as Deployment. Ideal for use when Clients and services need to be in same-subnet. End-points may or may not be in same subnet. Simpler deployment is desired. Roles and Responsiblities for kube-loxilb: Choose CIDR from local subnet. Choose SetRoles option so it can choose active loxilb pod. Monitors loxilb's health and elect new master on failover. Sets up loxilb in one-arm svc mode towards end-points. Configuration options containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: .... .... - --externalCIDR=192.168.80.200/24 - --setRoles=0.0.0.0 - --setLBMode=1 \"--externalCIDR=192.168.80.200/24\" - The external service IP for a svc is chosen from the externalCIDR range. In this scenario, the Client, svc and cluster are in the same subnet. \"--setRoles=0.0.0.0\" - This option will enable kube-loxilb to choose active-backup amongst the loxilb instance and the svc IP to be configured on the active loxilb node. \"--setLBMode=1\" - This option will enable kube-loxilb to configure svc in one-arm mode towards the endpoints. Sample kube-loxilb.yaml can be found here . Roles and Responsiblities for loxilb: Tracks and directs the external traffic destined to svc to the endpoints. Monitors endpoint's health and chooses active endpoints, if configured. Configuration options containers: - name: loxilb-app image: \"ghcr.io/loxilb-io/loxilb:latest\" imagePullPolicy: Always command: [ \"/root/loxilb-io/loxilb/loxilb\", \"--egr-hooks\", \"--blacklist=cni[0-9a-z]|veth.|flannel.\" ] ports: - containerPort: 11111 - containerPort: 179 - containerPort: 50051 securityContext: privileged: true capabilities: add: - SYS_ADMIN \"--egr-hooks\" - required for those cases in which workloads can be scheduled in the master nodes. No need to mention this argument when you are managing the workload scheduling to worker nodes. \"--blacklist=cni[0-9a-z]|veth.|flannel.\" - mandatory for running in in-cluster mode. As loxilb attaches it's ebpf programs on all the interfaces but since we running it in the default namespace then all the interfaces including CNI interfaces will be exposed and loxilb will attach it's ebpf program in those interfaces which is definitely not desired. So, user needs to mention a regex for excluding all those interfaces. The regex in the given example will exclude the flannel interfaces. \"--blacklist=cali.|tunl.|vxlan[.]calico|veth.|cni[0-9a-z]\" regex must be used with calico CNI. Sample loxilb.yaml can be found here . Failover This diagram describes the failover scenario: kube-loxilb actively monitors loxilb's health. In case of failure, it detects change in state of loxilb and assigns new \u201cactive\u201d from available healthy loxilb pod pool. The new pod inherits svcIP assigned previously to other loxilb pod and Services are served by newly active loxilb pod. Scenario 2 - L3 network (active-backup mode using BGP) Setup For this deployment scenario, kubernetes and loxilb are setup as follows: Kubernetes uses a cluster with 2 Master Nodes and 2 Worker Nodes, all the nodes use the same 192.168.80.0/24 subnet. SVCs will have an external IP, not from the cluster/local subnet. In this scenario, loxilb will be deployed as a DaemonSet in all the master nodes. And, kube-loxilb will be deployed as Deployment. Ideal for use when Clients and Cluster are in different subnets. Clients and svc VIP need to be in different subnet (cluster end-points may also be in different networks). Ideal for cloud deployments. Roles and Responsiblities for kube-loxilb: Choose CIDR from a different subnet. Choose SetRoles option so it can choose active loxilb pod. Monitors loxilb's health and elect new master on failover. Automates provisioning of bgp-peering between loxilb pods. Sets up loxilb in one-arm svc mode towards end-points. Configuration options containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: .... .... - --externalCIDR=123.123.123.1/24 - --setRoles=0.0.0.0 - --setLBMode=1 - --setBGP=65100 - --extBGPPeers=50.50.50.1:65101 \"--externalCIDR=123.123.123.1/24\" - The external service IP for a svc is chosen from the externalCIDR range. In this scenario, the Client, svc and cluster are all in the different subnet. \"--setRoles=0.0.0.0\" - This option will enable kube-loxilb to choose active-backup amongst the loxilb instances and the svc IP to be configured on the active loxilb node. \"--setLBMode=1\" - This option will enable kube-loxilb to configure svc in one-arm mode towards the endpoints. \"--setBGP=65100\" - This option will let kube-loxilb to configure local AS number in the bgp instance. \"--extBGPPeers=50.50.50.1:65101\" - This option will configure the bgp instance's external neighbors. Sample kube-loxilb.yaml can be found here . Roles and Responsiblities for loxilb: Advertises SVC IP as per the state(active or backup). Tracks and directs the external traffic destined to svc to the endpoints. Monitors endpoint's health and chooses active endpoints, if configured. Configuration options containers: - name: loxilb-app image: \"ghcr.io/loxilb-io/loxilb:latest\" imagePullPolicy: Always command: [ \"/root/loxilb-io/loxilb/loxilb\", \"--bgp\", \"--egr-hooks\", \"--blacklist=cni[0-9a-z]|veth.|flannel.\" ] ports: - containerPort: 11111 - containerPort: 179 - containerPort: 50051 securityContext: privileged: true capabilities: add: - SYS_ADMIN \"--bgp\" - option enables loxilb to run with goBGP instance which will advertise the routes with appropriate preference as per active/backup state. \"--egr-hooks\" - required for those cases in which workloads can be scheduled in the master nodes. No need to mention this argument when you are managing the workload scheduling to worker nodes. \"--blacklist=cni[0-9a-z]|veth.|flannel.\" - mandatory for running in in-cluster mode. As loxilb attaches it's ebpf programs on all the interfaces but since we running it in the default namespace then all the interfaces including CNI interfaces will be exposed and loxilb will attach it's ebpf program in those interfaces which is definitely not desired. So, user needs to mention a regex for excluding all those interfaces. The regex in the given example will exclude the flannel interfaces. \"--blacklist=cali.|tunl.|vxlan[.]calico|veth.|cni[0-9a-z]\" regex must be used with calico CNI. Sample loxilb.yaml can be found here . Failover This diagram describes the failover scenario: kube-loxilb actively monitors loxilb's health. In case of failure, it detects change in state of loxilb and assigns new \u201cactive\u201d from available healthy loxilb pod pool. The new pod inherits svcIP assigned previously to other loxilb pod and advertises the SVC IP with the preference as per the new state. Client receives the new route to SVCIP and the Services are served by newly active loxilb pod. Scenario 3 - L3 network (active-active with BGP ECMP) Setup For this deployment scenario, kubernetes and loxilb are setup as follows: Kubernetes uses a cluster with 2 Master Nodes and 2 Worker Nodes, all the nodes use the same 192.168.80.0/24 subnet. SVCs will have an external IP, not from the cluster/local subnet. In this scenario, loxilb will be deployed as a DaemonSet in all the master nodes. And, kube-loxilb will be deployed as Deployment. Ideal for use when Clients and Cluster are in different subnets. Clients and svc VIP need to be in different subnet (cluster end-points may also be in different networks). Ideal for cloud deployments. Better performance is desired due to active-active clustering but network devices/hosts must be capable of supporting ecmp. Roles and Responsiblities for kube-loxilb: Choose CIDR from a different subnet. Do not choose SetRoles option in this case (svcIPs will be advertised with same attributes/prio/med). Automates provisioning of bgp-peering between loxilb pods. Sets up loxilb in one-arm svc mode towards end-points. Configuration options containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: .... .... - --externalCIDR=123.123.123.1/24 - --setLBMode=1 - --setBGP=65100 - --extBGPPeers=50.50.50.1:65101 \"--externalCIDR=123.123.123.1/24\" - The external service IP for a svc is chosen from the externalCIDR range. In this scenario, the Client, svc and cluster are all in the different subnet. \"--setLBMode=1\" - This option will enable kube-loxilb to configure svc in one-arm mode towards the endpoints. \"--setBGP=65100\" - This option will let kube-loxilb to configure local AS number in the bgp instance. \"--extBGPPeers=50.50.50.1:65101\" - This option will configure the bgp instance's external neighbors Sample kube-loxilb.yaml can be found here . Roles and Responsiblities for loxilb: Advertises SVC IP with same attributes. Tracks and directs the external traffic destined to svc to the endpoints. Monitors endpoint's health and chooses active endpoints, if configured. Configuration options containers: - name: loxilb-app image: \"ghcr.io/loxilb-io/loxilb:latest\" imagePullPolicy: Always command: [ \"/root/loxilb-io/loxilb/loxilb\", \"--bgp\", \"--egr-hooks\", \"--blacklist=cni[0-9a-z]|veth.|flannel.\" ] ports: - containerPort: 11111 - containerPort: 179 - containerPort: 50051 securityContext: privileged: true capabilities: add: - SYS_ADMIN \"--bgp\" - option enables loxilb to run with goBGP instance which will advertise the routes with same attributes. \"--egr-hooks\" - required for those cases in which workloads can be scheduled in the master nodes. No need to mention this argument when you are managing the workload scheduling to worker nodes. \"--blacklist=cni[0-9a-z]|veth.|flannel.\" - mandatory for running in in-cluster mode. As loxilb attaches it's ebpf programs on all the interfaces but since we running it in the default namespace then all the interfaces including CNI interfaces will be exposed and loxilb will attach it's ebpf program in those interfaces which is definitely not desired. So, user needs to mention a regex for excluding all those interfaces. The regex in the given example will exclude the flannel interfaces. \"--blacklist=cali.|tunl.|vxlan[.]calico|veth.|cni[0-9a-z]\" regex must be used with calico CNI. Sample loxilb.yaml can be found here . Failover This diagram describes the failover scenario: In case of failure, BGP running on the client will update the ECMP route and start sending the traffic to the active ECMP endpoints. Scenario 4 - ACTIVE-BACKUP with Connection Sync Setup For this deployment scenario, kubernetes and loxilb are setup as follows: This feature is only supported when loxilb runs externally outside the Kubernetes cluster in either default or fullnat mode. Kubernetes uses a cluster with 2 Master Nodes and 2 Worker Nodes, all the nodes use the same 192.168.80.0/24 subnet. SVCs will have an external IP. There are few possible scenarios which depends upon the connectivity of External Client, loxilb and the Kubernetes cluster. For this scenario, we are here considering L3 connectivity. Ideal for use when Need to preserve long running connections during lb pod failures Another LB mode known as DSR mode can be used to preserve connections but has the following limitations : Can't ensure stateful filtering and connection-tracking. Can't support multihoming features since different 5-tuples might belong to the same connection. Roles and Responsiblities for kube-loxilb: Choose CIDR as required. Choose SetRoles option so it can choose active loxilb (svcIPs will be advertised with different attributes/prio/med). Automates provisioning of bgp-peering between loxilb containers (if required). Sets up loxilb in fullnat svc mode towards end-points. Configuration options containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: .... .... - --setRoles=0.0.0.0 - --loxiURL=http://192.168.80.1:11111,http://192.168.80.2:11111 - --externalCIDR=123.123.123.1/24 - --setLBMode=2 - --setBGP=65100 - --extBGPPeers=50.50.50.1:65101 \"--setRoles=0.0.0.0\" - This option will enable kube-loxilb to choose active-backup amongst the loxilb instance. \"--loxiURL=http://192.168.80.1:11111,http://192.168.80.2:11111\" - loxilb URLs to connect with. \"--externalCIDR=123.123.123.1/24\" - The external service IP for a svc is chosen from the externalCIDR range. In this scenario, the Client, svc and cluster are all in the different subnet. \"--setLBMode=2\" - This option will enable kube-loxilb to configure svc in fullnat mode towards the endpoints. \"--setBGP=65100\" - This option will let kube-loxilb to configure local AS number in the bgp instance. \"--extBGPPeers=50.50.50.1:65101\" - This option will configure the bgp instance's external neighbors Sample kube-loxilb.yaml can be found here . Roles and Responsiblities for loxilb: Advertises SVC IP as per the state(active/backup). Tracks and directs the external traffic destined to svc to the endpoints. Monitors endpoint's health and chooses active endpoints, if configured. Syncs the long-lived connections to all other configured loxilb peers. Running options #llb1 docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest --cluster=$llb2IP --self=0 -b #llb2 docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest --cluster=$llb1IP --self=1 -b \"--cluster=\\<llb-peer-IP>\" - option configures the peer loxilb IP for syncing. \"--self=0/1\" - option to identify the instance. \"-b\" - option enables loxilb to run with goBGP instance which will advertise the routes with appropriate preference as per active/backup state. Failover This diagram describes the failover scenario: In case of failure, kube-loxilb will detect the failure. It will select a new loxilb from the pool of active loxilbs and update it's state to new master. New master loxilb will advertise the svcIPs with higher proference which will force the BGP running on the client to send the traffic towards new Master loxilb. Since, the connections are all synced up, new master loxilb will start sending the traffic to the designated endpoints. Please read this detailed blog about \"Hitless HA\" to know about this feature. Scenario 5 - ACTIVE-BACKUP with Fast Failover Detection Setup For this deployment scenario, kubernetes and loxilb are setup as follows: This feature is only supported when loxilb runs externally outside the Kubernetes cluster. Kubernetes uses a cluster with 2 Master Nodes and 2 Worker Nodes, all the nodes use the same 192.168.80.0/24 subnet. SVCs will have an external IP. There are few possible scenarios which depends upon the connectivity of External Client, loxilb and the Kubernetes cluster. For this scenario, we are here considering L2 connectivity with connection sync. Ideal for use when Need fast failover detection and service continuity. This feature works in both L2 or L3 network settings. Roles and Responsiblities for kube-loxilb: Choose CIDR as required. Disable SetRoles option so it should not choose active loxilb. Automates provisioning of bgp-peering between loxilb containers (if required). Sets up loxilb in configured svc mode towards end-points. Configuration options containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: .... .... # Disable setRoles option #- --setRoles=0.0.0.0 - --loxiURL=http://192.168.80.1:11111,http://192.168.80.2:11111 - --externalCIDR=192.168.80.5/32 - --setLBMode=2 \"--setRoles=0.0.0.0\" - We have to make sure to disable this option as it will enable kube-loxilb to choose active-backup amongst the loxilb instance. \"--loxiURL=http://192.168.80.1:11111,http://192.168.80.2:11111\" - loxilb URLs to connect with. \"--externalCIDR=192.168.80.5/32\" - The external service IP for a svc is chosen from the externalCIDR range. In this scenario, the Client, svc and cluster are all in the same subnet. \"--setLBMode=2\" - This option will enable kube-loxilb to configure svc in fullnat mode towards the endpoints. Sample kube-loxilb.yaml can be found here . Roles and Responsiblities for loxilb: Advertises SVC IP as per the state(active/backup). Tracks and directs the external traffic destined to svc to the endpoints. Monitors endpoint's health and chooses active endpoints, if configured. Syncs the long-lived connections to all other configured loxilb peers. Running options #llb1 docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest --cluster=192.168.80.2 --self=0 --ka=192.168.80.2:192.168.80.1 #llb2 docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest --cluster=192.168.80.1 --self=1 --ka=192.168.80.1:192.168.80.2 \"--ka=\\<llb-peer-IP>:\\<llb-self-IP>\" - option configures the peer loxilb IP and source IP for BFD. \"--cluster=\\<llb-peer-IP>\" - option configures the peer loxilb IP for syncing. \"--self=0/1\" - option to identify the instance. Failover This diagram describes the failover scenario: In case of failure, BFD will detect the failure. BACKUP loxilb will update it's state to new master. New master loxilb will advertise the svcIPs through gARP or with higher proference, if running with BGP, which will force the client to send the traffic towards new Master loxilb. Since, the connections are all synced up, new master loxilb will start sending the traffic to the designated endpoints. Please read this detailed blog about \"Fast Failover Detection with BFD\" to know about this feature. Note : There are ways to use loxilb in DSR mode, DNS etc which is still not covered in details in this doc. We will keep updating the scenarios.","title":"Understanding High-availability with loxilb"},{"location":"ha-deploy/#how-to-deploy-loxilb-with-high-availability","text":"This article describes different scenarios about how to deploy loxilb with High Availability. Before continuing to this page, all readers are advised to have a basic understanding about kube-loxilb and the different NAT modes supported by loxilb. loxilb can run in-cluster or external to kubernetes cluster depending on architectural choices. For this documentation, we have assumed an incluster deployment wherever applicable but similar configuration should suffice for an external deployment as well. Scenario 1 - Flat L2 Networking (active-backup) Scenario 2 - L3 network (active-backup mode using BGP) Scenario 3 - L3 network (active-active with BGP ECMP) Scenario 4 - ACTIVE-BACKUP with Connection Sync Scenario 5 - ACTIVE-BACKUP with Fast Failover Detection(BFD)","title":"How to deploy loxilb with High Availability"},{"location":"ha-deploy/#scenario-1-flat-l2-networking-active-backup","text":"","title":"Scenario 1 - Flat L2 Networking (active-backup)"},{"location":"ha-deploy/#setup","text":"For this deployment scenario, kubernetes and loxilb are setup as follows: Kubernetes uses a cluster with 2 Master Nodes and 2 Worker Nodes, all the nodes use the same 192.168.80.0/24 subnet. In this scenario, loxilb will be deployed as a DaemonSet in all the master nodes. And, kube-loxilb will be deployed as Deployment.","title":"Setup"},{"location":"ha-deploy/#ideal-for-use-when","text":"Clients and services need to be in same-subnet. End-points may or may not be in same subnet. Simpler deployment is desired.","title":"Ideal for use when"},{"location":"ha-deploy/#roles-and-responsiblities-for-kube-loxilb","text":"Choose CIDR from local subnet. Choose SetRoles option so it can choose active loxilb pod. Monitors loxilb's health and elect new master on failover. Sets up loxilb in one-arm svc mode towards end-points.","title":"Roles and Responsiblities for kube-loxilb:"},{"location":"ha-deploy/#configuration-options","text":"containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: .... .... - --externalCIDR=192.168.80.200/24 - --setRoles=0.0.0.0 - --setLBMode=1 \"--externalCIDR=192.168.80.200/24\" - The external service IP for a svc is chosen from the externalCIDR range. In this scenario, the Client, svc and cluster are in the same subnet. \"--setRoles=0.0.0.0\" - This option will enable kube-loxilb to choose active-backup amongst the loxilb instance and the svc IP to be configured on the active loxilb node. \"--setLBMode=1\" - This option will enable kube-loxilb to configure svc in one-arm mode towards the endpoints. Sample kube-loxilb.yaml can be found here .","title":"Configuration options"},{"location":"ha-deploy/#roles-and-responsiblities-for-loxilb","text":"Tracks and directs the external traffic destined to svc to the endpoints. Monitors endpoint's health and chooses active endpoints, if configured.","title":"Roles and Responsiblities for loxilb:"},{"location":"ha-deploy/#configuration-options_1","text":"containers: - name: loxilb-app image: \"ghcr.io/loxilb-io/loxilb:latest\" imagePullPolicy: Always command: [ \"/root/loxilb-io/loxilb/loxilb\", \"--egr-hooks\", \"--blacklist=cni[0-9a-z]|veth.|flannel.\" ] ports: - containerPort: 11111 - containerPort: 179 - containerPort: 50051 securityContext: privileged: true capabilities: add: - SYS_ADMIN \"--egr-hooks\" - required for those cases in which workloads can be scheduled in the master nodes. No need to mention this argument when you are managing the workload scheduling to worker nodes. \"--blacklist=cni[0-9a-z]|veth.|flannel.\" - mandatory for running in in-cluster mode. As loxilb attaches it's ebpf programs on all the interfaces but since we running it in the default namespace then all the interfaces including CNI interfaces will be exposed and loxilb will attach it's ebpf program in those interfaces which is definitely not desired. So, user needs to mention a regex for excluding all those interfaces. The regex in the given example will exclude the flannel interfaces. \"--blacklist=cali.|tunl.|vxlan[.]calico|veth.|cni[0-9a-z]\" regex must be used with calico CNI. Sample loxilb.yaml can be found here .","title":"Configuration options"},{"location":"ha-deploy/#failover","text":"This diagram describes the failover scenario: kube-loxilb actively monitors loxilb's health. In case of failure, it detects change in state of loxilb and assigns new \u201cactive\u201d from available healthy loxilb pod pool. The new pod inherits svcIP assigned previously to other loxilb pod and Services are served by newly active loxilb pod.","title":"Failover"},{"location":"ha-deploy/#scenario-2-l3-network-active-backup-mode-using-bgp","text":"","title":"Scenario 2 - L3 network (active-backup mode using BGP)"},{"location":"ha-deploy/#setup_1","text":"For this deployment scenario, kubernetes and loxilb are setup as follows: Kubernetes uses a cluster with 2 Master Nodes and 2 Worker Nodes, all the nodes use the same 192.168.80.0/24 subnet. SVCs will have an external IP, not from the cluster/local subnet. In this scenario, loxilb will be deployed as a DaemonSet in all the master nodes. And, kube-loxilb will be deployed as Deployment.","title":"Setup"},{"location":"ha-deploy/#ideal-for-use-when_1","text":"Clients and Cluster are in different subnets. Clients and svc VIP need to be in different subnet (cluster end-points may also be in different networks). Ideal for cloud deployments.","title":"Ideal for use when"},{"location":"ha-deploy/#roles-and-responsiblities-for-kube-loxilb_1","text":"Choose CIDR from a different subnet. Choose SetRoles option so it can choose active loxilb pod. Monitors loxilb's health and elect new master on failover. Automates provisioning of bgp-peering between loxilb pods. Sets up loxilb in one-arm svc mode towards end-points.","title":"Roles and Responsiblities for kube-loxilb:"},{"location":"ha-deploy/#configuration-options_2","text":"containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: .... .... - --externalCIDR=123.123.123.1/24 - --setRoles=0.0.0.0 - --setLBMode=1 - --setBGP=65100 - --extBGPPeers=50.50.50.1:65101 \"--externalCIDR=123.123.123.1/24\" - The external service IP for a svc is chosen from the externalCIDR range. In this scenario, the Client, svc and cluster are all in the different subnet. \"--setRoles=0.0.0.0\" - This option will enable kube-loxilb to choose active-backup amongst the loxilb instances and the svc IP to be configured on the active loxilb node. \"--setLBMode=1\" - This option will enable kube-loxilb to configure svc in one-arm mode towards the endpoints. \"--setBGP=65100\" - This option will let kube-loxilb to configure local AS number in the bgp instance. \"--extBGPPeers=50.50.50.1:65101\" - This option will configure the bgp instance's external neighbors. Sample kube-loxilb.yaml can be found here .","title":"Configuration options"},{"location":"ha-deploy/#roles-and-responsiblities-for-loxilb_1","text":"Advertises SVC IP as per the state(active or backup). Tracks and directs the external traffic destined to svc to the endpoints. Monitors endpoint's health and chooses active endpoints, if configured.","title":"Roles and Responsiblities for loxilb:"},{"location":"ha-deploy/#configuration-options_3","text":"containers: - name: loxilb-app image: \"ghcr.io/loxilb-io/loxilb:latest\" imagePullPolicy: Always command: [ \"/root/loxilb-io/loxilb/loxilb\", \"--bgp\", \"--egr-hooks\", \"--blacklist=cni[0-9a-z]|veth.|flannel.\" ] ports: - containerPort: 11111 - containerPort: 179 - containerPort: 50051 securityContext: privileged: true capabilities: add: - SYS_ADMIN \"--bgp\" - option enables loxilb to run with goBGP instance which will advertise the routes with appropriate preference as per active/backup state. \"--egr-hooks\" - required for those cases in which workloads can be scheduled in the master nodes. No need to mention this argument when you are managing the workload scheduling to worker nodes. \"--blacklist=cni[0-9a-z]|veth.|flannel.\" - mandatory for running in in-cluster mode. As loxilb attaches it's ebpf programs on all the interfaces but since we running it in the default namespace then all the interfaces including CNI interfaces will be exposed and loxilb will attach it's ebpf program in those interfaces which is definitely not desired. So, user needs to mention a regex for excluding all those interfaces. The regex in the given example will exclude the flannel interfaces. \"--blacklist=cali.|tunl.|vxlan[.]calico|veth.|cni[0-9a-z]\" regex must be used with calico CNI. Sample loxilb.yaml can be found here .","title":"Configuration options"},{"location":"ha-deploy/#failover_1","text":"This diagram describes the failover scenario: kube-loxilb actively monitors loxilb's health. In case of failure, it detects change in state of loxilb and assigns new \u201cactive\u201d from available healthy loxilb pod pool. The new pod inherits svcIP assigned previously to other loxilb pod and advertises the SVC IP with the preference as per the new state. Client receives the new route to SVCIP and the Services are served by newly active loxilb pod.","title":"Failover"},{"location":"ha-deploy/#scenario-3-l3-network-active-active-with-bgp-ecmp","text":"","title":"Scenario 3 - L3 network (active-active with BGP ECMP)"},{"location":"ha-deploy/#setup_2","text":"For this deployment scenario, kubernetes and loxilb are setup as follows: Kubernetes uses a cluster with 2 Master Nodes and 2 Worker Nodes, all the nodes use the same 192.168.80.0/24 subnet. SVCs will have an external IP, not from the cluster/local subnet. In this scenario, loxilb will be deployed as a DaemonSet in all the master nodes. And, kube-loxilb will be deployed as Deployment.","title":"Setup"},{"location":"ha-deploy/#ideal-for-use-when_2","text":"Clients and Cluster are in different subnets. Clients and svc VIP need to be in different subnet (cluster end-points may also be in different networks). Ideal for cloud deployments. Better performance is desired due to active-active clustering but network devices/hosts must be capable of supporting ecmp.","title":"Ideal for use when"},{"location":"ha-deploy/#roles-and-responsiblities-for-kube-loxilb_2","text":"Choose CIDR from a different subnet. Do not choose SetRoles option in this case (svcIPs will be advertised with same attributes/prio/med). Automates provisioning of bgp-peering between loxilb pods. Sets up loxilb in one-arm svc mode towards end-points.","title":"Roles and Responsiblities for kube-loxilb:"},{"location":"ha-deploy/#configuration-options_4","text":"containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: .... .... - --externalCIDR=123.123.123.1/24 - --setLBMode=1 - --setBGP=65100 - --extBGPPeers=50.50.50.1:65101 \"--externalCIDR=123.123.123.1/24\" - The external service IP for a svc is chosen from the externalCIDR range. In this scenario, the Client, svc and cluster are all in the different subnet. \"--setLBMode=1\" - This option will enable kube-loxilb to configure svc in one-arm mode towards the endpoints. \"--setBGP=65100\" - This option will let kube-loxilb to configure local AS number in the bgp instance. \"--extBGPPeers=50.50.50.1:65101\" - This option will configure the bgp instance's external neighbors Sample kube-loxilb.yaml can be found here .","title":"Configuration options"},{"location":"ha-deploy/#roles-and-responsiblities-for-loxilb_2","text":"Advertises SVC IP with same attributes. Tracks and directs the external traffic destined to svc to the endpoints. Monitors endpoint's health and chooses active endpoints, if configured.","title":"Roles and Responsiblities for loxilb:"},{"location":"ha-deploy/#configuration-options_5","text":"containers: - name: loxilb-app image: \"ghcr.io/loxilb-io/loxilb:latest\" imagePullPolicy: Always command: [ \"/root/loxilb-io/loxilb/loxilb\", \"--bgp\", \"--egr-hooks\", \"--blacklist=cni[0-9a-z]|veth.|flannel.\" ] ports: - containerPort: 11111 - containerPort: 179 - containerPort: 50051 securityContext: privileged: true capabilities: add: - SYS_ADMIN \"--bgp\" - option enables loxilb to run with goBGP instance which will advertise the routes with same attributes. \"--egr-hooks\" - required for those cases in which workloads can be scheduled in the master nodes. No need to mention this argument when you are managing the workload scheduling to worker nodes. \"--blacklist=cni[0-9a-z]|veth.|flannel.\" - mandatory for running in in-cluster mode. As loxilb attaches it's ebpf programs on all the interfaces but since we running it in the default namespace then all the interfaces including CNI interfaces will be exposed and loxilb will attach it's ebpf program in those interfaces which is definitely not desired. So, user needs to mention a regex for excluding all those interfaces. The regex in the given example will exclude the flannel interfaces. \"--blacklist=cali.|tunl.|vxlan[.]calico|veth.|cni[0-9a-z]\" regex must be used with calico CNI. Sample loxilb.yaml can be found here .","title":"Configuration options"},{"location":"ha-deploy/#failover_2","text":"This diagram describes the failover scenario: In case of failure, BGP running on the client will update the ECMP route and start sending the traffic to the active ECMP endpoints.","title":"Failover"},{"location":"ha-deploy/#scenario-4-active-backup-with-connection-sync","text":"","title":"Scenario 4 - ACTIVE-BACKUP with Connection Sync"},{"location":"ha-deploy/#setup_3","text":"For this deployment scenario, kubernetes and loxilb are setup as follows: This feature is only supported when loxilb runs externally outside the Kubernetes cluster in either default or fullnat mode. Kubernetes uses a cluster with 2 Master Nodes and 2 Worker Nodes, all the nodes use the same 192.168.80.0/24 subnet. SVCs will have an external IP. There are few possible scenarios which depends upon the connectivity of External Client, loxilb and the Kubernetes cluster. For this scenario, we are here considering L3 connectivity.","title":"Setup"},{"location":"ha-deploy/#ideal-for-use-when_3","text":"Need to preserve long running connections during lb pod failures Another LB mode known as DSR mode can be used to preserve connections but has the following limitations : Can't ensure stateful filtering and connection-tracking. Can't support multihoming features since different 5-tuples might belong to the same connection.","title":"Ideal for use when"},{"location":"ha-deploy/#roles-and-responsiblities-for-kube-loxilb_3","text":"Choose CIDR as required. Choose SetRoles option so it can choose active loxilb (svcIPs will be advertised with different attributes/prio/med). Automates provisioning of bgp-peering between loxilb containers (if required). Sets up loxilb in fullnat svc mode towards end-points.","title":"Roles and Responsiblities for kube-loxilb:"},{"location":"ha-deploy/#configuration-options_6","text":"containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: .... .... - --setRoles=0.0.0.0 - --loxiURL=http://192.168.80.1:11111,http://192.168.80.2:11111 - --externalCIDR=123.123.123.1/24 - --setLBMode=2 - --setBGP=65100 - --extBGPPeers=50.50.50.1:65101 \"--setRoles=0.0.0.0\" - This option will enable kube-loxilb to choose active-backup amongst the loxilb instance. \"--loxiURL=http://192.168.80.1:11111,http://192.168.80.2:11111\" - loxilb URLs to connect with. \"--externalCIDR=123.123.123.1/24\" - The external service IP for a svc is chosen from the externalCIDR range. In this scenario, the Client, svc and cluster are all in the different subnet. \"--setLBMode=2\" - This option will enable kube-loxilb to configure svc in fullnat mode towards the endpoints. \"--setBGP=65100\" - This option will let kube-loxilb to configure local AS number in the bgp instance. \"--extBGPPeers=50.50.50.1:65101\" - This option will configure the bgp instance's external neighbors Sample kube-loxilb.yaml can be found here .","title":"Configuration options"},{"location":"ha-deploy/#roles-and-responsiblities-for-loxilb_3","text":"Advertises SVC IP as per the state(active/backup). Tracks and directs the external traffic destined to svc to the endpoints. Monitors endpoint's health and chooses active endpoints, if configured. Syncs the long-lived connections to all other configured loxilb peers.","title":"Roles and Responsiblities for loxilb:"},{"location":"ha-deploy/#running-options","text":"#llb1 docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest --cluster=$llb2IP --self=0 -b #llb2 docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest --cluster=$llb1IP --self=1 -b \"--cluster=\\<llb-peer-IP>\" - option configures the peer loxilb IP for syncing. \"--self=0/1\" - option to identify the instance. \"-b\" - option enables loxilb to run with goBGP instance which will advertise the routes with appropriate preference as per active/backup state.","title":"Running options"},{"location":"ha-deploy/#failover_3","text":"This diagram describes the failover scenario: In case of failure, kube-loxilb will detect the failure. It will select a new loxilb from the pool of active loxilbs and update it's state to new master. New master loxilb will advertise the svcIPs with higher proference which will force the BGP running on the client to send the traffic towards new Master loxilb. Since, the connections are all synced up, new master loxilb will start sending the traffic to the designated endpoints. Please read this detailed blog about \"Hitless HA\" to know about this feature.","title":"Failover"},{"location":"ha-deploy/#scenario-5-active-backup-with-fast-failover-detection","text":"","title":"Scenario 5 - ACTIVE-BACKUP with Fast Failover Detection"},{"location":"ha-deploy/#setup_4","text":"For this deployment scenario, kubernetes and loxilb are setup as follows: This feature is only supported when loxilb runs externally outside the Kubernetes cluster. Kubernetes uses a cluster with 2 Master Nodes and 2 Worker Nodes, all the nodes use the same 192.168.80.0/24 subnet. SVCs will have an external IP. There are few possible scenarios which depends upon the connectivity of External Client, loxilb and the Kubernetes cluster. For this scenario, we are here considering L2 connectivity with connection sync.","title":"Setup"},{"location":"ha-deploy/#ideal-for-use-when_4","text":"Need fast failover detection and service continuity. This feature works in both L2 or L3 network settings.","title":"Ideal for use when"},{"location":"ha-deploy/#roles-and-responsiblities-for-kube-loxilb_4","text":"Choose CIDR as required. Disable SetRoles option so it should not choose active loxilb. Automates provisioning of bgp-peering between loxilb containers (if required). Sets up loxilb in configured svc mode towards end-points.","title":"Roles and Responsiblities for kube-loxilb:"},{"location":"ha-deploy/#configuration-options_7","text":"containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: .... .... # Disable setRoles option #- --setRoles=0.0.0.0 - --loxiURL=http://192.168.80.1:11111,http://192.168.80.2:11111 - --externalCIDR=192.168.80.5/32 - --setLBMode=2 \"--setRoles=0.0.0.0\" - We have to make sure to disable this option as it will enable kube-loxilb to choose active-backup amongst the loxilb instance. \"--loxiURL=http://192.168.80.1:11111,http://192.168.80.2:11111\" - loxilb URLs to connect with. \"--externalCIDR=192.168.80.5/32\" - The external service IP for a svc is chosen from the externalCIDR range. In this scenario, the Client, svc and cluster are all in the same subnet. \"--setLBMode=2\" - This option will enable kube-loxilb to configure svc in fullnat mode towards the endpoints. Sample kube-loxilb.yaml can be found here .","title":"Configuration options"},{"location":"ha-deploy/#roles-and-responsiblities-for-loxilb_4","text":"Advertises SVC IP as per the state(active/backup). Tracks and directs the external traffic destined to svc to the endpoints. Monitors endpoint's health and chooses active endpoints, if configured. Syncs the long-lived connections to all other configured loxilb peers.","title":"Roles and Responsiblities for loxilb:"},{"location":"ha-deploy/#running-options_1","text":"#llb1 docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest --cluster=192.168.80.2 --self=0 --ka=192.168.80.2:192.168.80.1 #llb2 docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest --cluster=192.168.80.1 --self=1 --ka=192.168.80.1:192.168.80.2 \"--ka=\\<llb-peer-IP>:\\<llb-self-IP>\" - option configures the peer loxilb IP and source IP for BFD. \"--cluster=\\<llb-peer-IP>\" - option configures the peer loxilb IP for syncing. \"--self=0/1\" - option to identify the instance.","title":"Running options"},{"location":"ha-deploy/#failover_4","text":"This diagram describes the failover scenario: In case of failure, BFD will detect the failure. BACKUP loxilb will update it's state to new master. New master loxilb will advertise the svcIPs through gARP or with higher proference, if running with BGP, which will force the client to send the traffic towards new Master loxilb. Since, the connections are all synced up, new master loxilb will start sending the traffic to the designated endpoints. Please read this detailed blog about \"Fast Failover Detection with BFD\" to know about this feature.","title":"Failover"},{"location":"ha-deploy/#note","text":"There are ways to use loxilb in DSR mode, DNS etc which is still not covered in details in this doc. We will keep updating the scenarios.","title":"Note :"},{"location":"https/","text":"HTTPS guide Key and Cert files are required for HTTPS, and they are not detailed, but explain how to generate them and where LoxiLB can read and use user-generated Key and Cert files. --tls enable TLS [$TLS] --tls-host= the IP to listen on for tls, when not specified it's the same as --host [$TLS_HOST] --tls-port= the port to listen on for secure connections (default: 8091) [$TLS_PORT] --tls-certificate= the certificate to use for secure connections (default: /opt/loxilb/cert/server.crt) [$TLS_CERTIFICATE] --tls-key= the private key to use for secure connections (default: /opt/loxilb/cert/server.key) [$TLS_PRIVATE_KEY] To enable https on LoxiLB, we changed it to enable it using the --tls option. Tls-host and tls-port are the contents of deciding which IP to listen to. The default IP address used as tls-host is 0.0.0.0, which is everywhere, but for future security, we recommend doing only certain values. The port is 8091 as the default. You can also find and change this from a value that does not overlap with the service you use. LoxiLB reads the key by default as /opt/loxilb/cert/path with server.key and the Cert file as server.crt in the same path. In this article, we will learn how to create the server.key and server.crt files. You can enable and run HTTLS (TLS) with the following commands. ./loxilb --tls Preparation First of all, the simplest way is to create it using openssl . To install openssl, you can install it using the command below. apt install openssl The LoxiLB team confirmed that it operates on 1.1.1f version of openssl. openssl version OpenSSL 1.1.1f 31 Mar 2020 1. Create server.key openssl genrsa -out server.key 2048 The way to generate server.key is simple. You can create a new key by typing the command above. In fact, if you type in the command, you can see that the process is output and the server.key is generated. openssl genrsa -out server.key 2048 Generating RSA private key, 2048 bit long modulus (2 primes) ..............................................+++++ ...........................................+++++ e is 65537 (0x010001) 2. Create server.csr openssl req -new -key server.key -out server.csr Create a csr file by putting the desired value in the corresponding item. This file is not used directly for https, but it is necessary to create a Cert file to be created later. When you type in the command above, a long sentence appears asking you to enter information, and you can fill in the corresponding value according to your situation. openssl req -new -key server.key -out server.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]: State or Province Name (full name) [Some-State]: Locality Name (eg, city) []: Organization Name (eg, company) [Internet Widgits Pty Ltd]: Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []: Email Address []: Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []: An optional company name []: 3. Create server.crt openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt This is the process of creating server.crt using server.key and server.csr generated above. You can issue a certificate with a limited deadline by setting the expiration date of the certificate well and putting a value after -day. The server.crt file is created with the following output. openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt Signature ok subject=C = AU, ST = Some-State, O = Internet Widgits Pty Ltd Getting Private key 4. Validation You can enable https with the server.key and server.cert files generated through the above process. If you move all of these files to the /opt/loxilb path and check them, you can see that they work well. sudo cp server.key /opt/loxilb/cert/. sudo cp server.crt /opt/loxilb/cert/. ./loxilb --tls curl http://0.0.0.0:11111/netlox/v1/config/loadbalancer/all {\"lbAttr\":[]} curl -k https://0.0.0.0:8091/netlox/v1/config/loadbalancer/all {\"lbAttr\":[]} It should appear in the log as follows. 2024/04/12 16:19:48 Serving loxilb rest API at http://[::]:11111 2024/04/12 16:19:48 Serving loxilb rest API at https://[::]:8091","title":"HTTPS guide"},{"location":"https/#https-guide","text":"Key and Cert files are required for HTTPS, and they are not detailed, but explain how to generate them and where LoxiLB can read and use user-generated Key and Cert files. --tls enable TLS [$TLS] --tls-host= the IP to listen on for tls, when not specified it's the same as --host [$TLS_HOST] --tls-port= the port to listen on for secure connections (default: 8091) [$TLS_PORT] --tls-certificate= the certificate to use for secure connections (default: /opt/loxilb/cert/server.crt) [$TLS_CERTIFICATE] --tls-key= the private key to use for secure connections (default: /opt/loxilb/cert/server.key) [$TLS_PRIVATE_KEY] To enable https on LoxiLB, we changed it to enable it using the --tls option. Tls-host and tls-port are the contents of deciding which IP to listen to. The default IP address used as tls-host is 0.0.0.0, which is everywhere, but for future security, we recommend doing only certain values. The port is 8091 as the default. You can also find and change this from a value that does not overlap with the service you use. LoxiLB reads the key by default as /opt/loxilb/cert/path with server.key and the Cert file as server.crt in the same path. In this article, we will learn how to create the server.key and server.crt files. You can enable and run HTTLS (TLS) with the following commands. ./loxilb --tls","title":"HTTPS guide"},{"location":"https/#preparation","text":"First of all, the simplest way is to create it using openssl . To install openssl, you can install it using the command below. apt install openssl The LoxiLB team confirmed that it operates on 1.1.1f version of openssl. openssl version OpenSSL 1.1.1f 31 Mar 2020","title":"Preparation"},{"location":"https/#1-create-serverkey","text":"openssl genrsa -out server.key 2048 The way to generate server.key is simple. You can create a new key by typing the command above. In fact, if you type in the command, you can see that the process is output and the server.key is generated. openssl genrsa -out server.key 2048 Generating RSA private key, 2048 bit long modulus (2 primes) ..............................................+++++ ...........................................+++++ e is 65537 (0x010001)","title":"1. Create server.key"},{"location":"https/#2-create-servercsr","text":"openssl req -new -key server.key -out server.csr Create a csr file by putting the desired value in the corresponding item. This file is not used directly for https, but it is necessary to create a Cert file to be created later. When you type in the command above, a long sentence appears asking you to enter information, and you can fill in the corresponding value according to your situation. openssl req -new -key server.key -out server.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]: State or Province Name (full name) [Some-State]: Locality Name (eg, city) []: Organization Name (eg, company) [Internet Widgits Pty Ltd]: Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []: Email Address []: Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []: An optional company name []:","title":"2. Create server.csr"},{"location":"https/#3-create-servercrt","text":"openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt This is the process of creating server.crt using server.key and server.csr generated above. You can issue a certificate with a limited deadline by setting the expiration date of the certificate well and putting a value after -day. The server.crt file is created with the following output. openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt Signature ok subject=C = AU, ST = Some-State, O = Internet Widgits Pty Ltd Getting Private key","title":"3. Create server.crt"},{"location":"https/#4-validation","text":"You can enable https with the server.key and server.cert files generated through the above process. If you move all of these files to the /opt/loxilb path and check them, you can see that they work well. sudo cp server.key /opt/loxilb/cert/. sudo cp server.crt /opt/loxilb/cert/. ./loxilb --tls curl http://0.0.0.0:11111/netlox/v1/config/loadbalancer/all {\"lbAttr\":[]} curl -k https://0.0.0.0:8091/netlox/v1/config/loadbalancer/all {\"lbAttr\":[]} It should appear in the log as follows. 2024/04/12 16:19:48 Serving loxilb rest API at http://[::]:11111 2024/04/12 16:19:48 Serving loxilb rest API at https://[::]:8091","title":"4. Validation"},{"location":"integrate_bgp/","text":"loxilb & calico BGP \uc5f0\ub3d9 \uc774 \ubb38\uc11c\uc5d0\uc11c\ub294 calico CNI\ub97c \uc0ac\uc6a9\ud558\ub294 kubernetes\uc640 loxilb\ub97c \uc5f0\ub3d9\ud558\ub294 \ubc29\ubc95\uc744 \uc124\uba85\ud569\ub2c8\ub2e4. \ud658\uacbd \uc774 \uc608\uc81c\uc5d0\uc11c\ub294 kubernetes\uc640 loxilb\uac00 \ub2e4\uc74c\uacfc \uac19\uc774 \uc5f0\uacb0\ub418\uc5b4 \uc788\ub2e4\uace0 \uac00\uc815\ud569\ub2c8\ub2e4. kubernetes\ub294 \ub2e8\uc21c\ud568\uc744 \uc704\ud574\uc11c \ub2e8\uc77c \ub9c8\uc2a4\ud130 \ud074\ub7ec\uc2a4\ud130\ub97c \uc0ac\uc6a9\ud558\uba70 \ubaa8\ub4e0 \ud074\ub7ec\uc2a4\ud130\ub294 192.168.57.0/24 \ub3d9\uc77c\ud55c \uc11c\ube0c\ub137\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. loxilb \ucee8\ud14c\uc774\ub108\uac00 \uc2e4\ud589\uc911\uc778 \ub85c\ub4dc\ubc38\ub7f0\uc11c \ub178\ub4dc \uc5ed\uc2dc kubernetes\uc640 \ub3d9\uc77c\ud55c \uc11c\ube0c\ub137\uc5d0 \uc5f0\uacb0\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc678\ubd80\uc5d0\uc11c kubernetes\uc811\uc18d\uc740 \ubaa8\ub450 \ub85c\ub4dc\ubc38\ub7f0\uc11c \ub178\ub4dc\uc640 loxilb\ub97c \uac70\uce58\ub3c4\ub85d \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4. \ud574\ub2f9 \uc608\uc81c\uc5d0\uc11c\ub294 docker\ub97c \uc0ac\uc6a9\ud574 loxilb \ucee8\ud14c\uc774\ub108\ub97c \uc2e4\ud589\ud569\ub2c8\ub2e4. \ud574\ub2f9 \uc608\uc81c\uc5d0\uc11c\ub294 kubernetes & calico\ub294 \uc774\ubbf8 \uc124\uce58\ub418\uc5b4 \uc788\ub2e4\uace0 \uac00\uc815\ud558\uace0 \uc124\uba85\ud569\ub2c8\ub2e4. 1. loxilb container \uc0dd\uc131 1.1 docker network \uc0dd\uc131 \uc6b0\uc120 loxilb\uc640 kubernetes \uc5f0\ub3d9\uc744 \uc704\ud574\uc11c\ub294 \uc11c\ub85c \ud1b5\uc2e0\ud560 \uc218 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4. kubernetes & \ub85c\ub4dc\ubc38\ub7f0\uc11c \ub178\ub4dc\uac00 \uc5f0\uacb0\ub418\uc5b4 \uc788\ub294 \ub124\ud2b8\uc6cc\ud06c\uc5d0 loxilb \ucee8\ud14c\uc774\ub108\ub3c4 \uc5f0\uacb0\ub418\ub3c4\ub85d docker network\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \ud604\uc7ac \ub85c\ub4dc\ubc38\ub7f0\uc11c \ub178\ub4dc\ub294 eno6 \uc778\ud130\ud398\uc774\uc2a4\ub97c \ud1b5\ud574 kubernetes\uc640 \uc5f0\uacb0\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c eno6 \uc778\ud130\ud398\uc774\uc2a4\ub97c parent\ub85c \uc0ac\uc6a9\ud558\ub294 macvlan \ud0c0\uc785 docker network \ub9cc\ub4e4\uc5b4\uc11c loxilb \ucee8\ud14c\uc774\ub108\uc5d0 \uc81c\uacf5\ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \uba85\ub839\uc5b4\ub85c docker network\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. sudo docker network create -d macvlan -o parent=eno6 \\ --subnet 192.168.57.0/24 \\ --gateway 192.168.57.1 \\ --aux-address 'cp1=192.168.57.101' \\ --aux-address 'cp2=192.168.57.102' \\ --aux-address 'cp3=192.168.57.103' k8snet |\uc635\uc158|\uc124\uba85| |----|----| |-d macvlan|\ub124\ud2b8\uc6cc\ud06c \ud0c0\uc785\uc744 macvlan\uc73c\ub85c \uc9c0\uc815| |-o parent=eno6|eno6 \uc778\ud130\ud398\uc774\uc2a4\ub97c parent\ub85c \uc0ac\uc6a9\ud574\uc11c macvlan type \ub124\ud2b8\uc6cc\ud06c \uc0dd\uc131| |--subnet 192.168.57.0/24|\ub124\ud2b8\uc6cc\ud06c \uc11c\ube0c\ub137 \uc9c0\uc815| |--gateway 192.168.57.1|\uac8c\uc774\ud2b8\uc6e8\uc774 \uc124\uc815(\uc0dd\ub7b5 \uac00\ub2a5)| |--aux-address 'serverName=serverIP'|\ud574\ub2f9 \ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c \uc774\ubbf8 \uc0ac\uc6a9\uc911\uc778 IP \uc8fc\uc18c\ub4e4\uc774 \uc911\ubcf5\uc73c\ub85c \ucee8\ud14c\uc774\ub108\uc5d0 \ud560\ub2f9\ub418\uc9c0 \uc54a\ub3c4\ub85d \ubbf8\ub9ac \ub4f1\ub85d\ud558\ub294 \uc635\uc158| |k8snet|\ub124\ud2b8\uc6cc\ud06c \uc774\ub984\uc744 k8snet\uc73c\ub85c \uc9c0\uc815| \uc678\ubd80\uc5d0\uc11c kubernetes \uc11c\ube44\uc2a4\ub85c \uc811\uadfc\ud558\ub294 \ud2b8\ub798\ud53d \uc5ed\uc2dc loxilb\ub97c \uac70\uce58\ub3c4\ub85d, \uc678\ubd80\uc640 \ud1b5\uc2e0\uc774 \uac00\ub2a5\ud55c docker network \uc5ed\uc2dc \uc0dd\uc131\ud569\ub2c8\ub2e4. \ub85c\ub4dc\ubc38\ub7f0\uc11c \ub178\ub4dc\ub294 eno8\uc744 \ud1b5\ud574 \uc678\ubd80\uc640 \uc5f0\uacb0\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. sudo docker network create -d macvlan -o parent=eno8 \\ --subnet 192.168.20.0/24 \\ --gateway 192.168.20.1 llbnet docker network list \uba85\ub839\uc5b4\ub85c \uc0dd\uc131\ud55c \ub124\ud2b8\uc6cc\ud06c\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. netlox@nd8:~$ sudo docker network list NETWORK ID NAME DRIVER SCOPE 5c97ae74fc32 bridge bridge local 6142f53e8be6 host host local 24ee7dbd7707 k8snet macvlan local 81c96ceda375 llbnet macvlan local 7bcd1738501b none null local 1.2 loxilb container \uc0dd\uc131 loxilb container \uc774\ubbf8\uc9c0\ub294 github \uc5d0\uc11c \uc81c\uacf5\ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ucee8\ud14c\uc774\ub108 \uc774\ubbf8\uc9c0\ub9cc \uba3c\uc800 \ub2e4\uc6b4\ub85c\ub4dc\ud558\uace0 \uc2f6\uc744 \uacbd\uc6b0 \ub2e4\uc74c \uba85\ub839\uc5b4\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. docker pull ghcr.io/loxilb-io/loxilb:latest \ub2e4\uc74c \uba85\ub839\uc5b4\ub85c loxilb \ucee8\ud14c\uc774\ub108\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped \\ --privileged -dit -v /dev/log:/dev/log \\ --net=k8snet --ip=192.168.57.4 --name loxilb ghcr.io/loxilb-io/loxilb:latest \\ --host=0.0.0.0 \uc0ac\uc6a9\uc790\uac00 \uc9c0\uc815\ud574\uc57c \ud558\ub294 \uc635\uc158\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. |\uc635\uc158|\uc124\uba85| |----|----| |--net=k8snet|\ucee8\ud14c\uc774\ub108\ub97c \uc5f0\uacb0\ud560 \ub124\ud2b8\uc6cc\ud06c| |--ip=192.168.57.4|\ucee8\ud14c\uc774\ub108\uac00 \uc0ac\uc6a9\ud560 IP address \uc9c0\uc815. \uc9c0\uc815\ud558\uc9c0 \uc54a\uc744 \uacbd\uc6b0 network \uc11c\ube0c\ub0c7 \ubc94\uc704 \ub0b4\uc5d0\uc11c \uc784\uc758\uc758 IP \uc0ac\uc6a9| |--name loxilb|\ucee8\ud14c\uc774\ub108 \uc774\ub984 \uc124\uc815| docker ps \uba85\ub839\uc5b4\ub85c \uc0dd\uc131\ud55c \ucee8\ud14c\uc774\ub108\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. netlox@nd8:~$ sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES eae349a283ae loxilbio/loxilb:beta \"/root/loxilb-io/lox\u2026\" 11 days ago Up 11 days loxilb \uc704\uc5d0\uc11c \ucee8\ud14c\uc774\ub108 \uc0dd\uc131\ud560 \ub54c kubernetes \ub124\ud2b8\uc6cc\ud06c\ub9cc \uc5f0\uacb0\ud588\uae30 \ub54c\ubb38\uc5d0, \uc678\ubd80 \ud1b5\uc2e0\uc6a9 docker network\uc640\ub3c4 \uc5f0\uacb0\ud574\uc57c \ud569\ub2c8\ub2e4. \ub2e4\uc74c \uba85\ub839\uc5b4\ub85c \ucee8\ud14c\uc774\ub108\uc5d0 \ub124\ud2b8\uc6cc\ud06c\ub97c \uc8fc\uac00\ub85c \uc5f0\uacb0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. sudo docker network connect llbnet loxilb \uc5f0\uacb0\uc774 \uc644\ub8cc\ub418\uba74 \ub2e4\uc74c\uacfc \uac19\uc774 \ucee8\ud14c\uc774\ub108\uc758 \uc778\ud130\ud398\uc774\uc2a4 2\uac1c\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4 netlox@netlox:~$ sudo docker exec -ti loxilb ip route default via 192.168.20.1 dev eth0 192.168.20.0/24 dev eth0 proto kernel scope link src 192.168.20.4 192.168.30.0/24 dev eth1 proto kernel scope link src 192.168.30.2 2. kubernetes\uc5d0 loxi-ccm \uc124\uce58 loxi-ccm\uc740 loxilb \ub85c\ub4dc\ubc38\ub7f0\uc11c\ub97c kubernetes\uc5d0\uac8c \uc81c\uacf5\ud558\uae30 \uc704\ud55c cloud-controller-manager \ub85c\uc11c, kubernetes\uc640 loxilb \uc5f0\ub3d9\uc5d0 \ubc18\ub4dc\uc2dc \ud544\uc694\ud569\ub2c8\ub2e4. \ud574\ub2f9 \ubb38\uc11c \ub97c \ucc38\uace0\ud574\uc11c, configMap\uc758 apiServerURL\uc744 \uc704\uc5d0\uc11c \uc0dd\uc131\ud55c loxilb\uc758 IP \uc8fc\uc18c\ub85c \ubcc0\uacbd\ud55c \ud6c4 kubernetes\uc5d0 \uc124\uce58\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4. loxi-ccm\uae4c\uc9c0 \uc815\uc0c1\uc801\uc73c\ub85c \uc124\uce58\ub418\uc5c8\ub2e4\uba74 \uc5f0\ub3d9 \uc791\uc5c5\uc774 \uc644\ub8cc\ub429\ub2c8\ub2e4. 3. \uae30\ubcf8 \uc5f0\ub3d9 \ud655\uc778 2\ubc88 \ud56d\ubaa9\uae4c\uc9c0 \uc644\ub8cc\ub418\uc5c8\ub2e4\uba74, \uc774\uc81c kubernetes\uc5d0\uc11c LoadBalancer \ud0c0\uc785 \uc11c\ube44\uc2a4\ub97c \uc0dd\uc131\ud558\uba74 External IP\uac00 \ubd80\uc5ec\ub429\ub2c8\ub2e4. \ub2e4\uc74c\uacfc \uac19\uc774 \ud14c\uc2a4\ud2b8\uc6a9\uc73c\ub85c test-nginx-svc.yaml \ud30c\uc77c\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. apiVersion: v1 kind: Pod metadata: name: nginx labels: app.kubernetes.io/name: proxy spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 name: http-web-svc --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: LoadBalancer selector: app.kubernetes.io/name: proxy ports: - name: name-of-service-port protocol: TCP port: 8888 targetPort: http-web-svc \ud30c\uc77c\uc744 \uc0dd\uc131\ud55c \ub2e4\uc74c, \uc544\ub798 \uba85\ub839\uc73c\ub85c nginx pod\uc640 LoadBalancer \uc11c\ube44\uc2a4\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. kubectl apply -f test-nginx-svc.yaml \uc11c\ube44\uc2a4 nginx-service\uac00 LoadBalancer \ud0c0\uc785\uc73c\ub85c \uc0dd\uc131\ub418\uc5c8\uace0 External IP\ub97c \ud560\ub2f9\ubc1b\uc558\uc74c\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uc81c IP 123.123.123.15\uc640 port 8888\uc744 \uc0ac\uc6a9\ud574 \uc678\ubd80\uc5d0\uc11c kubernetes \uc11c\ube44\uc2a4\ub85c \uc811\uadfc\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. vagrant@node1:~$ sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.233.0.1 <none> 443/TCP 28d nginx-service LoadBalancer 10.233.21.235 123.123.123.15 8888:31655/TCP 3s LoadBalancer \ub8f0\uc740 loxilb \ucee8\ud14c\uc774\ub108\uc5d0\ub3c4 \uc0dd\uc131\ub429\ub2c8\ub2e4. \ub85c\ub4dc\ubc38\ub7f0\uc11c \ub178\ub4dc\uc5d0\uc11c \ub2e4\uc74c\uacfc \uac19\uc774 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. netlox@nd8:~$ sudo docker exec -ti loxilb loxicmd get lb | EXTERNAL IP | PORT | PROTOCOL | SELECT | # OF ENDPOINTS | |----------------|------|----------|--------|----------------| | 123.123.123.15 | 8888 | tcp | 0 | 2 | 4. calico BGP & loxilb \uc5f0\ub3d9 calico\uc5d0\uc11c BGP \ubaa8\ub4dc\ub85c \ub124\ud2b8\uc6cc\ud06c\ub97c \uad6c\uc131\ud560 \uacbd\uc6b0, loxilb \uc5ed\uc2dc BGP \ubaa8\ub4dc\ub85c \ub3d9\uc791\ud574\uc57c \ud569\ub2c8\ub2e4. loxilb\ub294 goBGP \uae30\ubc18\uc73c\ub85c BGP \ub124\ud2b8\uc6cc\ud06c \uae30\ub2a5\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \uc774\ud558 \ub0b4\uc6a9\uc740 calico\uac00 BGP mode\ub85c \uc124\uc815\ub418\uc5b4 \uc788\ub2e4\uace0 \uac00\uc815\ud558\uace0 \uc124\uba85\ud569\ub2c8\ub2e4. 4.1 loxilb BGP \ubaa8\ub4dc\ub85c \uc2e4\ud589 \ub2e4\uc74c \uba85\ub839\uc5b4\ub85c loxilb \ucee8\ud14c\uc774\ub108\ub97c \uc0dd\uc131\ud558\uba74 BGP \ubaa8\ub4dc\ub85c \uc2e4\ud589\ub429\ub2c8\ub2e4. \uba85\ub839\uc5b4 \ub9c8\uc9c0\ub9c9\uc758 -b \uc635\uc158\uc774 BGP \ubaa8\ub4dc \uc635\uc158\uc785\ub2c8\ub2e4. sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped \\ --privileged -dit -v /dev/log:/dev/log \\ --net=k8snet --ip=192.168.57.4 --name loxilb ghcr.io/loxilb-io/loxilb:latest \\ --host=0.0.0.0 -b 4.2 gobgp_loxilb.yaml \ud30c\uc77c \uc0dd\uc131 loxilb \ucee8\ud14c\uc774\ub108\uc758 /opt/loxilb/ \ub514\ub809\ud1a0\ub9ac\uc5d0 gobgp_loxilb.yaml \ud30c\uc77c\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. global: config: as: 65002 router-id: 172.1.0.2 neighbors: - config: neighbor-address: 192.168.57.101 peer-as: 64512 - config: neighbor-address: 192.168.20.55 peer-as: 64001 global \ud56d\ubaa9\uc5d0\ub294 loxilb \ucee8\ud14c\uc774\ub108\uc758 as-id\uc640 router-id \ub4f1 BGP \uc815\ubcf4\ub97c \ub4f1\ub85d\ud574\uc57c \ud569\ub2c8\ub2e4. neighbors\ub294 loxilb\uc640 Peering\ub418\ub294 BGP \ub77c\uc6b0\ud130\uc758 IP \uc8fc\uc18c \ubc0f as-id \uc815\ubcf4\ub97c \ub4f1\ub85d\ud569\ub2c8\ub2e4. \ud574\ub2f9 \uc608\uc81c\uc5d0\uc11c\ub294 calico\uc758 BGP \uc815\ubcf4(192.168.57.101)\uc640 \uc678\ubd80 BGP \uc815\ubcf4(129.168.20.55) \ub97c \ub4f1\ub85d\ud588\uc2b5\ub2c8\ub2e4. 4.3 loxilb \ucee8\ud14c\uc774\ub108\uc758 lo \uc778\ud130\ud398\uc774\uc2a4\uc5d0 router-id \ucd94\uac00 gobgp_loxilb.yaml \ud30c\uc77c\uc5d0\uc11c router-id\ub85c \ub4f1\ub85d\ud55c IP\ub97c lo \uc778\ud130\ud398\uc774\uc2a4\uc5d0 \ucd94\uac00\ud574\uc57c \ud569\ub2c8\ub2e4. sudo docker exec -ti loxilb ip addr add 172.1.0.2/32 dev lo 4.4 loxilb \ucee8\ud14c\uc774\ub108 \uc7ac\uc2dc\uc791 gobgp_loxilb.yaml\uc5d0 \uc791\uc131\ud55c \uc124\uc815\uc774 \uc801\uc6a9\ub418\ub3c4\ub85d \ucee8\ud14c\uc774\ub108\ub97c \uc7ac\uc2dc\uc791\ud569\ub2c8\ub2e4. sudo docker stop loxilb sudo docker start loxilb 4.5 calico\uc5d0 BGP Peer \uc815\ubcf4 \ucd94\uac00 calico\uc5d0\ub3c4 loxilb\uc758 BGP Peer \uc815\ubcf4\ub97c \ucd94\uac00\ud574\uc57c \ud569\ub2c8\ub2e4. \ub2e4\uc74c\uacfc \uac19\uc774 calico-bgp-config.yaml \ud30c\uc77c\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peers2 spec: peerIP: 192.168.57.4 asNumber: 65002 peerIP\uc5d0 loxilb\uc758 IP \uc8fc\uc18c\ub97c \uc785\ub825\ud569\ub2c8\ub2e4. asNumber\uc5d0\ub294 \uc704\uc5d0\uc11c \uc124\uc815\ud55c loxilb BGP\uc758 as-ID\ub97c \uc785\ub825\ud569\ub2c8\ub2e4. \ud30c\uc77c\uc744 \uc0dd\uc131\ud55c \ub2e4\uc74c, \uc544\ub798 \uba85\ub839\uc5b4\ub85c calico\uc5d0 BGP Peer \uc815\ubcf4\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4. sudo calicoctl apply -f calico-bgp-config.yaml 4.6 BGP \uc124\uc815 \ud655\uc778 \uc774\uc81c \ub2e4\uc74c\uacfc \uac19\uc774 loxilb \ucee8\ud14c\uc774\ub108\uc5d0\uc11c BGP \uc5f0\uacb0\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. netlox@nd8:~$ sudo docker exec -ti loxilb3 gobgp neigh Peer AS Up/Down State |#Received Accepted 192.168.57.101 64512 00:00:59 Establ | 4 4 \uc815\uc0c1\uc801\uc73c\ub85c \uc5f0\uacb0\ub418\uc5c8\ub2e4\uba74 State\uac00 Establish\ub85c \ud45c\uc2dc\ub429\ub2c8\ub2e4. gobgp global rib \uba85\ub839\uc73c\ub85c calico\uc758 route \uc815\ubcf4\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. netlox@nd8:~$ sudo docker exec -ti loxilb3 gobgp global rib Network Next Hop AS_PATH Age Attrs *> 10.233.71.0/26 192.168.57.101 64512 01:02:03 [{Origin: i}] *> 10.233.74.64/26 192.168.57.101 64512 01:02:03 [{Origin: i}] *> 10.233.75.0/26 192.168.57.101 64512 01:02:03 [{Origin: i}] *> 10.233.102.128/26 192.168.57.101 64512 01:02:03 [{Origin: i}]","title":"loxilb &amp; calico BGP \uc5f0\ub3d9"},{"location":"integrate_bgp/#loxilb-calico-bgp","text":"\uc774 \ubb38\uc11c\uc5d0\uc11c\ub294 calico CNI\ub97c \uc0ac\uc6a9\ud558\ub294 kubernetes\uc640 loxilb\ub97c \uc5f0\ub3d9\ud558\ub294 \ubc29\ubc95\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.","title":"loxilb &amp; calico BGP \uc5f0\ub3d9"},{"location":"integrate_bgp/#_1","text":"\uc774 \uc608\uc81c\uc5d0\uc11c\ub294 kubernetes\uc640 loxilb\uac00 \ub2e4\uc74c\uacfc \uac19\uc774 \uc5f0\uacb0\ub418\uc5b4 \uc788\ub2e4\uace0 \uac00\uc815\ud569\ub2c8\ub2e4. kubernetes\ub294 \ub2e8\uc21c\ud568\uc744 \uc704\ud574\uc11c \ub2e8\uc77c \ub9c8\uc2a4\ud130 \ud074\ub7ec\uc2a4\ud130\ub97c \uc0ac\uc6a9\ud558\uba70 \ubaa8\ub4e0 \ud074\ub7ec\uc2a4\ud130\ub294 192.168.57.0/24 \ub3d9\uc77c\ud55c \uc11c\ube0c\ub137\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. loxilb \ucee8\ud14c\uc774\ub108\uac00 \uc2e4\ud589\uc911\uc778 \ub85c\ub4dc\ubc38\ub7f0\uc11c \ub178\ub4dc \uc5ed\uc2dc kubernetes\uc640 \ub3d9\uc77c\ud55c \uc11c\ube0c\ub137\uc5d0 \uc5f0\uacb0\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc678\ubd80\uc5d0\uc11c kubernetes\uc811\uc18d\uc740 \ubaa8\ub450 \ub85c\ub4dc\ubc38\ub7f0\uc11c \ub178\ub4dc\uc640 loxilb\ub97c \uac70\uce58\ub3c4\ub85d \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4. \ud574\ub2f9 \uc608\uc81c\uc5d0\uc11c\ub294 docker\ub97c \uc0ac\uc6a9\ud574 loxilb \ucee8\ud14c\uc774\ub108\ub97c \uc2e4\ud589\ud569\ub2c8\ub2e4. \ud574\ub2f9 \uc608\uc81c\uc5d0\uc11c\ub294 kubernetes & calico\ub294 \uc774\ubbf8 \uc124\uce58\ub418\uc5b4 \uc788\ub2e4\uace0 \uac00\uc815\ud558\uace0 \uc124\uba85\ud569\ub2c8\ub2e4.","title":"\ud658\uacbd"},{"location":"integrate_bgp/#1-loxilb-container","text":"","title":"1. loxilb container \uc0dd\uc131"},{"location":"integrate_bgp/#11-docker-network","text":"\uc6b0\uc120 loxilb\uc640 kubernetes \uc5f0\ub3d9\uc744 \uc704\ud574\uc11c\ub294 \uc11c\ub85c \ud1b5\uc2e0\ud560 \uc218 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4. kubernetes & \ub85c\ub4dc\ubc38\ub7f0\uc11c \ub178\ub4dc\uac00 \uc5f0\uacb0\ub418\uc5b4 \uc788\ub294 \ub124\ud2b8\uc6cc\ud06c\uc5d0 loxilb \ucee8\ud14c\uc774\ub108\ub3c4 \uc5f0\uacb0\ub418\ub3c4\ub85d docker network\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \ud604\uc7ac \ub85c\ub4dc\ubc38\ub7f0\uc11c \ub178\ub4dc\ub294 eno6 \uc778\ud130\ud398\uc774\uc2a4\ub97c \ud1b5\ud574 kubernetes\uc640 \uc5f0\uacb0\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c eno6 \uc778\ud130\ud398\uc774\uc2a4\ub97c parent\ub85c \uc0ac\uc6a9\ud558\ub294 macvlan \ud0c0\uc785 docker network \ub9cc\ub4e4\uc5b4\uc11c loxilb \ucee8\ud14c\uc774\ub108\uc5d0 \uc81c\uacf5\ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \uba85\ub839\uc5b4\ub85c docker network\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. sudo docker network create -d macvlan -o parent=eno6 \\ --subnet 192.168.57.0/24 \\ --gateway 192.168.57.1 \\ --aux-address 'cp1=192.168.57.101' \\ --aux-address 'cp2=192.168.57.102' \\ --aux-address 'cp3=192.168.57.103' k8snet |\uc635\uc158|\uc124\uba85| |----|----| |-d macvlan|\ub124\ud2b8\uc6cc\ud06c \ud0c0\uc785\uc744 macvlan\uc73c\ub85c \uc9c0\uc815| |-o parent=eno6|eno6 \uc778\ud130\ud398\uc774\uc2a4\ub97c parent\ub85c \uc0ac\uc6a9\ud574\uc11c macvlan type \ub124\ud2b8\uc6cc\ud06c \uc0dd\uc131| |--subnet 192.168.57.0/24|\ub124\ud2b8\uc6cc\ud06c \uc11c\ube0c\ub137 \uc9c0\uc815| |--gateway 192.168.57.1|\uac8c\uc774\ud2b8\uc6e8\uc774 \uc124\uc815(\uc0dd\ub7b5 \uac00\ub2a5)| |--aux-address 'serverName=serverIP'|\ud574\ub2f9 \ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c \uc774\ubbf8 \uc0ac\uc6a9\uc911\uc778 IP \uc8fc\uc18c\ub4e4\uc774 \uc911\ubcf5\uc73c\ub85c \ucee8\ud14c\uc774\ub108\uc5d0 \ud560\ub2f9\ub418\uc9c0 \uc54a\ub3c4\ub85d \ubbf8\ub9ac \ub4f1\ub85d\ud558\ub294 \uc635\uc158| |k8snet|\ub124\ud2b8\uc6cc\ud06c \uc774\ub984\uc744 k8snet\uc73c\ub85c \uc9c0\uc815| \uc678\ubd80\uc5d0\uc11c kubernetes \uc11c\ube44\uc2a4\ub85c \uc811\uadfc\ud558\ub294 \ud2b8\ub798\ud53d \uc5ed\uc2dc loxilb\ub97c \uac70\uce58\ub3c4\ub85d, \uc678\ubd80\uc640 \ud1b5\uc2e0\uc774 \uac00\ub2a5\ud55c docker network \uc5ed\uc2dc \uc0dd\uc131\ud569\ub2c8\ub2e4. \ub85c\ub4dc\ubc38\ub7f0\uc11c \ub178\ub4dc\ub294 eno8\uc744 \ud1b5\ud574 \uc678\ubd80\uc640 \uc5f0\uacb0\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. sudo docker network create -d macvlan -o parent=eno8 \\ --subnet 192.168.20.0/24 \\ --gateway 192.168.20.1 llbnet docker network list \uba85\ub839\uc5b4\ub85c \uc0dd\uc131\ud55c \ub124\ud2b8\uc6cc\ud06c\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. netlox@nd8:~$ sudo docker network list NETWORK ID NAME DRIVER SCOPE 5c97ae74fc32 bridge bridge local 6142f53e8be6 host host local 24ee7dbd7707 k8snet macvlan local 81c96ceda375 llbnet macvlan local 7bcd1738501b none null local","title":"1.1 docker network \uc0dd\uc131"},{"location":"integrate_bgp/#12-loxilb-container","text":"loxilb container \uc774\ubbf8\uc9c0\ub294 github \uc5d0\uc11c \uc81c\uacf5\ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ucee8\ud14c\uc774\ub108 \uc774\ubbf8\uc9c0\ub9cc \uba3c\uc800 \ub2e4\uc6b4\ub85c\ub4dc\ud558\uace0 \uc2f6\uc744 \uacbd\uc6b0 \ub2e4\uc74c \uba85\ub839\uc5b4\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. docker pull ghcr.io/loxilb-io/loxilb:latest \ub2e4\uc74c \uba85\ub839\uc5b4\ub85c loxilb \ucee8\ud14c\uc774\ub108\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped \\ --privileged -dit -v /dev/log:/dev/log \\ --net=k8snet --ip=192.168.57.4 --name loxilb ghcr.io/loxilb-io/loxilb:latest \\ --host=0.0.0.0 \uc0ac\uc6a9\uc790\uac00 \uc9c0\uc815\ud574\uc57c \ud558\ub294 \uc635\uc158\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. |\uc635\uc158|\uc124\uba85| |----|----| |--net=k8snet|\ucee8\ud14c\uc774\ub108\ub97c \uc5f0\uacb0\ud560 \ub124\ud2b8\uc6cc\ud06c| |--ip=192.168.57.4|\ucee8\ud14c\uc774\ub108\uac00 \uc0ac\uc6a9\ud560 IP address \uc9c0\uc815. \uc9c0\uc815\ud558\uc9c0 \uc54a\uc744 \uacbd\uc6b0 network \uc11c\ube0c\ub0c7 \ubc94\uc704 \ub0b4\uc5d0\uc11c \uc784\uc758\uc758 IP \uc0ac\uc6a9| |--name loxilb|\ucee8\ud14c\uc774\ub108 \uc774\ub984 \uc124\uc815| docker ps \uba85\ub839\uc5b4\ub85c \uc0dd\uc131\ud55c \ucee8\ud14c\uc774\ub108\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. netlox@nd8:~$ sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES eae349a283ae loxilbio/loxilb:beta \"/root/loxilb-io/lox\u2026\" 11 days ago Up 11 days loxilb \uc704\uc5d0\uc11c \ucee8\ud14c\uc774\ub108 \uc0dd\uc131\ud560 \ub54c kubernetes \ub124\ud2b8\uc6cc\ud06c\ub9cc \uc5f0\uacb0\ud588\uae30 \ub54c\ubb38\uc5d0, \uc678\ubd80 \ud1b5\uc2e0\uc6a9 docker network\uc640\ub3c4 \uc5f0\uacb0\ud574\uc57c \ud569\ub2c8\ub2e4. \ub2e4\uc74c \uba85\ub839\uc5b4\ub85c \ucee8\ud14c\uc774\ub108\uc5d0 \ub124\ud2b8\uc6cc\ud06c\ub97c \uc8fc\uac00\ub85c \uc5f0\uacb0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. sudo docker network connect llbnet loxilb \uc5f0\uacb0\uc774 \uc644\ub8cc\ub418\uba74 \ub2e4\uc74c\uacfc \uac19\uc774 \ucee8\ud14c\uc774\ub108\uc758 \uc778\ud130\ud398\uc774\uc2a4 2\uac1c\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4 netlox@netlox:~$ sudo docker exec -ti loxilb ip route default via 192.168.20.1 dev eth0 192.168.20.0/24 dev eth0 proto kernel scope link src 192.168.20.4 192.168.30.0/24 dev eth1 proto kernel scope link src 192.168.30.2","title":"1.2 loxilb container \uc0dd\uc131"},{"location":"integrate_bgp/#2-kubernetes-loxi-ccm","text":"loxi-ccm\uc740 loxilb \ub85c\ub4dc\ubc38\ub7f0\uc11c\ub97c kubernetes\uc5d0\uac8c \uc81c\uacf5\ud558\uae30 \uc704\ud55c cloud-controller-manager \ub85c\uc11c, kubernetes\uc640 loxilb \uc5f0\ub3d9\uc5d0 \ubc18\ub4dc\uc2dc \ud544\uc694\ud569\ub2c8\ub2e4. \ud574\ub2f9 \ubb38\uc11c \ub97c \ucc38\uace0\ud574\uc11c, configMap\uc758 apiServerURL\uc744 \uc704\uc5d0\uc11c \uc0dd\uc131\ud55c loxilb\uc758 IP \uc8fc\uc18c\ub85c \ubcc0\uacbd\ud55c \ud6c4 kubernetes\uc5d0 \uc124\uce58\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4. loxi-ccm\uae4c\uc9c0 \uc815\uc0c1\uc801\uc73c\ub85c \uc124\uce58\ub418\uc5c8\ub2e4\uba74 \uc5f0\ub3d9 \uc791\uc5c5\uc774 \uc644\ub8cc\ub429\ub2c8\ub2e4.","title":"2. kubernetes\uc5d0 loxi-ccm \uc124\uce58"},{"location":"integrate_bgp/#3","text":"2\ubc88 \ud56d\ubaa9\uae4c\uc9c0 \uc644\ub8cc\ub418\uc5c8\ub2e4\uba74, \uc774\uc81c kubernetes\uc5d0\uc11c LoadBalancer \ud0c0\uc785 \uc11c\ube44\uc2a4\ub97c \uc0dd\uc131\ud558\uba74 External IP\uac00 \ubd80\uc5ec\ub429\ub2c8\ub2e4. \ub2e4\uc74c\uacfc \uac19\uc774 \ud14c\uc2a4\ud2b8\uc6a9\uc73c\ub85c test-nginx-svc.yaml \ud30c\uc77c\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. apiVersion: v1 kind: Pod metadata: name: nginx labels: app.kubernetes.io/name: proxy spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 name: http-web-svc --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: LoadBalancer selector: app.kubernetes.io/name: proxy ports: - name: name-of-service-port protocol: TCP port: 8888 targetPort: http-web-svc \ud30c\uc77c\uc744 \uc0dd\uc131\ud55c \ub2e4\uc74c, \uc544\ub798 \uba85\ub839\uc73c\ub85c nginx pod\uc640 LoadBalancer \uc11c\ube44\uc2a4\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. kubectl apply -f test-nginx-svc.yaml \uc11c\ube44\uc2a4 nginx-service\uac00 LoadBalancer \ud0c0\uc785\uc73c\ub85c \uc0dd\uc131\ub418\uc5c8\uace0 External IP\ub97c \ud560\ub2f9\ubc1b\uc558\uc74c\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uc81c IP 123.123.123.15\uc640 port 8888\uc744 \uc0ac\uc6a9\ud574 \uc678\ubd80\uc5d0\uc11c kubernetes \uc11c\ube44\uc2a4\ub85c \uc811\uadfc\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. vagrant@node1:~$ sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.233.0.1 <none> 443/TCP 28d nginx-service LoadBalancer 10.233.21.235 123.123.123.15 8888:31655/TCP 3s LoadBalancer \ub8f0\uc740 loxilb \ucee8\ud14c\uc774\ub108\uc5d0\ub3c4 \uc0dd\uc131\ub429\ub2c8\ub2e4. \ub85c\ub4dc\ubc38\ub7f0\uc11c \ub178\ub4dc\uc5d0\uc11c \ub2e4\uc74c\uacfc \uac19\uc774 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. netlox@nd8:~$ sudo docker exec -ti loxilb loxicmd get lb | EXTERNAL IP | PORT | PROTOCOL | SELECT | # OF ENDPOINTS | |----------------|------|----------|--------|----------------| | 123.123.123.15 | 8888 | tcp | 0 | 2 |","title":"3. \uae30\ubcf8 \uc5f0\ub3d9 \ud655\uc778"},{"location":"integrate_bgp/#4-calico-bgp-loxilb","text":"calico\uc5d0\uc11c BGP \ubaa8\ub4dc\ub85c \ub124\ud2b8\uc6cc\ud06c\ub97c \uad6c\uc131\ud560 \uacbd\uc6b0, loxilb \uc5ed\uc2dc BGP \ubaa8\ub4dc\ub85c \ub3d9\uc791\ud574\uc57c \ud569\ub2c8\ub2e4. loxilb\ub294 goBGP \uae30\ubc18\uc73c\ub85c BGP \ub124\ud2b8\uc6cc\ud06c \uae30\ub2a5\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \uc774\ud558 \ub0b4\uc6a9\uc740 calico\uac00 BGP mode\ub85c \uc124\uc815\ub418\uc5b4 \uc788\ub2e4\uace0 \uac00\uc815\ud558\uace0 \uc124\uba85\ud569\ub2c8\ub2e4.","title":"4. calico BGP &amp; loxilb \uc5f0\ub3d9"},{"location":"integrate_bgp/#41-loxilb-bgp","text":"\ub2e4\uc74c \uba85\ub839\uc5b4\ub85c loxilb \ucee8\ud14c\uc774\ub108\ub97c \uc0dd\uc131\ud558\uba74 BGP \ubaa8\ub4dc\ub85c \uc2e4\ud589\ub429\ub2c8\ub2e4. \uba85\ub839\uc5b4 \ub9c8\uc9c0\ub9c9\uc758 -b \uc635\uc158\uc774 BGP \ubaa8\ub4dc \uc635\uc158\uc785\ub2c8\ub2e4. sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped \\ --privileged -dit -v /dev/log:/dev/log \\ --net=k8snet --ip=192.168.57.4 --name loxilb ghcr.io/loxilb-io/loxilb:latest \\ --host=0.0.0.0 -b","title":"4.1 loxilb BGP \ubaa8\ub4dc\ub85c \uc2e4\ud589"},{"location":"integrate_bgp/#42-gobgp_loxilbyaml","text":"loxilb \ucee8\ud14c\uc774\ub108\uc758 /opt/loxilb/ \ub514\ub809\ud1a0\ub9ac\uc5d0 gobgp_loxilb.yaml \ud30c\uc77c\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. global: config: as: 65002 router-id: 172.1.0.2 neighbors: - config: neighbor-address: 192.168.57.101 peer-as: 64512 - config: neighbor-address: 192.168.20.55 peer-as: 64001 global \ud56d\ubaa9\uc5d0\ub294 loxilb \ucee8\ud14c\uc774\ub108\uc758 as-id\uc640 router-id \ub4f1 BGP \uc815\ubcf4\ub97c \ub4f1\ub85d\ud574\uc57c \ud569\ub2c8\ub2e4. neighbors\ub294 loxilb\uc640 Peering\ub418\ub294 BGP \ub77c\uc6b0\ud130\uc758 IP \uc8fc\uc18c \ubc0f as-id \uc815\ubcf4\ub97c \ub4f1\ub85d\ud569\ub2c8\ub2e4. \ud574\ub2f9 \uc608\uc81c\uc5d0\uc11c\ub294 calico\uc758 BGP \uc815\ubcf4(192.168.57.101)\uc640 \uc678\ubd80 BGP \uc815\ubcf4(129.168.20.55) \ub97c \ub4f1\ub85d\ud588\uc2b5\ub2c8\ub2e4.","title":"4.2 gobgp_loxilb.yaml \ud30c\uc77c \uc0dd\uc131"},{"location":"integrate_bgp/#43-loxilb-lo-router-id","text":"gobgp_loxilb.yaml \ud30c\uc77c\uc5d0\uc11c router-id\ub85c \ub4f1\ub85d\ud55c IP\ub97c lo \uc778\ud130\ud398\uc774\uc2a4\uc5d0 \ucd94\uac00\ud574\uc57c \ud569\ub2c8\ub2e4. sudo docker exec -ti loxilb ip addr add 172.1.0.2/32 dev lo","title":"4.3 loxilb \ucee8\ud14c\uc774\ub108\uc758 lo \uc778\ud130\ud398\uc774\uc2a4\uc5d0 router-id \ucd94\uac00"},{"location":"integrate_bgp/#44-loxilb","text":"gobgp_loxilb.yaml\uc5d0 \uc791\uc131\ud55c \uc124\uc815\uc774 \uc801\uc6a9\ub418\ub3c4\ub85d \ucee8\ud14c\uc774\ub108\ub97c \uc7ac\uc2dc\uc791\ud569\ub2c8\ub2e4. sudo docker stop loxilb sudo docker start loxilb","title":"4.4 loxilb \ucee8\ud14c\uc774\ub108 \uc7ac\uc2dc\uc791"},{"location":"integrate_bgp/#45-calico-bgp-peer","text":"calico\uc5d0\ub3c4 loxilb\uc758 BGP Peer \uc815\ubcf4\ub97c \ucd94\uac00\ud574\uc57c \ud569\ub2c8\ub2e4. \ub2e4\uc74c\uacfc \uac19\uc774 calico-bgp-config.yaml \ud30c\uc77c\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peers2 spec: peerIP: 192.168.57.4 asNumber: 65002 peerIP\uc5d0 loxilb\uc758 IP \uc8fc\uc18c\ub97c \uc785\ub825\ud569\ub2c8\ub2e4. asNumber\uc5d0\ub294 \uc704\uc5d0\uc11c \uc124\uc815\ud55c loxilb BGP\uc758 as-ID\ub97c \uc785\ub825\ud569\ub2c8\ub2e4. \ud30c\uc77c\uc744 \uc0dd\uc131\ud55c \ub2e4\uc74c, \uc544\ub798 \uba85\ub839\uc5b4\ub85c calico\uc5d0 BGP Peer \uc815\ubcf4\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4. sudo calicoctl apply -f calico-bgp-config.yaml","title":"4.5 calico\uc5d0 BGP Peer \uc815\ubcf4 \ucd94\uac00"},{"location":"integrate_bgp/#46-bgp","text":"\uc774\uc81c \ub2e4\uc74c\uacfc \uac19\uc774 loxilb \ucee8\ud14c\uc774\ub108\uc5d0\uc11c BGP \uc5f0\uacb0\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. netlox@nd8:~$ sudo docker exec -ti loxilb3 gobgp neigh Peer AS Up/Down State |#Received Accepted 192.168.57.101 64512 00:00:59 Establ | 4 4 \uc815\uc0c1\uc801\uc73c\ub85c \uc5f0\uacb0\ub418\uc5c8\ub2e4\uba74 State\uac00 Establish\ub85c \ud45c\uc2dc\ub429\ub2c8\ub2e4. gobgp global rib \uba85\ub839\uc73c\ub85c calico\uc758 route \uc815\ubcf4\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. netlox@nd8:~$ sudo docker exec -ti loxilb3 gobgp global rib Network Next Hop AS_PATH Age Attrs *> 10.233.71.0/26 192.168.57.101 64512 01:02:03 [{Origin: i}] *> 10.233.74.64/26 192.168.57.101 64512 01:02:03 [{Origin: i}] *> 10.233.75.0/26 192.168.57.101 64512 01:02:03 [{Origin: i}] *> 10.233.102.128/26 192.168.57.101 64512 01:02:03 [{Origin: i}]","title":"4.6 BGP \uc124\uc815 \ud655\uc778"},{"location":"integrate_bgp_eng/","text":"How to run loxilb with calico CNI in BGP mode This article describes how to integrate loxilb using calico CNI in Kubernetes. Setup For this example, kubernetes and loxilb are setup as follows: Kubernetes uses a single master cluster for simplicity, all clusters use the same 192.168.57.0/24 subnet. The load balancer node where the loxilb container is running is also connected to the same subnet as kubernetes. Externally, all kubernetes connections are configured to go through the \"loxilb\" load balancer node. This example uses docker to run the loxilb container. These examples assume that kubernetes & calico are already installed. 1. loxilb setup 1.1 docker network setup In order to integrate loxilb and kubernetes, loxilb needs to be able to communicate with Kubernetes. First, we create a docker network so that the loxilb container is also connected to the same network which is used by kubernetes nodes. Currently, the load balancer node is connected to kubernetes through the eno6 interface. Therefore, we will create a macvlan-type docker network that uses the eno6 interface as a parent and provide it to the loxilb docker. Create a docker network with the following command: sudo docker network create -d macvlan -o parent=eno6 \\ --subnet 192.168.57.0/24 \\ --gateway 192.168.57.1 \\ --aux-address 'cp1=192.168.57.101' \\ --aux-address 'cp2=192.168.57.102' \\ --aux-address 'cp3=192.168.57.103' k8snet |Options|Description| |----|----| |-d macvlan|Specify network type as macvlan| |-o parent=eno6|Create a macvlan type network using the eno6 interface as parent| |--subnet 192.168.57.0/24|Specify network subnet| |--gateway 192.168.57.1|Set gateway (optional)| |--aux-address 'serverName=serverIP'|Option to register in advance so that IP addresses already in use on the network are not duplicated| |k8snet|Name the network k8snet| A docker network that can communicate with the outside is also created so that traffic accessing the kubernetes service from outside also goes through loxilb. The load balancer node is connected to the outside through eno8. sudo docker network create -d macvlan -o parent=eno8 \\ --subnet 192.168.20.0/24 \\ --gateway 192.168.20.1 llbnet We can check the network created with the docker network list command. netlox@nd8:~$ sudo docker network list NETWORK ID NAME DRIVER SCOPE 5c97ae74fc32 bridge bridge local 6142f53e8be6 host host local 24ee7dbd7707 k8snet macvlan local 81c96ceda375 llbnet macvlan local 7bcd1738501b none null local 1.2 loxilb docker setup The loxilb container image is provided at github . To download the docker image, use the following command. docker pull ghcr.io/loxilb-io/loxilb:latest To run loxilb docker, we can use the following command. sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped \\ --privileged -dit -v /dev/log:/dev/log \\ --net=k8snet --ip=192.168.57.4 --name loxilb ghcr.io/loxilb-io/loxilb:latest \\ --host=0.0.0.0 The options that need to specified are: Options Description --net=k8snet Network to connect to container --ip=192.168.57.4 Specifies the IP address the container will use. If not specified, use any IP within the network subnet range --name loxilb Set container name We can check the docker created with the docker ps command. netlox@nd8:~$ sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES eae349a283ae loxilbio/loxilb:beta \"/root/loxilb-io/lox\u2026\" 11 days ago Up 11 days loxilb Since we only connected the kubernetes network (k8snet) when running the docker above, we also need to connect to the docker network for external communication. Currently, docker only supports one network to be connected when running \"docker run\" command. But, its easy to connect other network to the docker with the following command: sudo docker network connect llbnet loxilb Once the connection is complete, we can see the docker container's interfaces as follows: netlox@netlox:~$ sudo docker exec -ti loxilb ip route default via 192.168.20.1 dev eth0 192.168.20.0/24 dev eth0 proto kernel scope link src 192.168.20.4 192.168.30.0/24 dev eth1 proto kernel scope link src 192.168.30.2 2. loxi-ccm setup in kubernetes loxi-ccm is a ccm provider to provide a loxilb load balancer to kubernetes, and it is essential for interworking with kubernetes and loxilb. Refer to the relevant document , change the apiServerURL of configMap to the IP address of loxilb created above and install it in kubernetes. If loxi-ccm is installed properly, the setup is complete. 3.Basic load-balancer test We can now give an External IP when you create a LoadBalancer type service in kubernetes. Create a test-nginx-svc.yaml file for testing as follows: apiVersion: v1 kind: Pod metadata: name: nginx labels: app.kubernetes.io/name: proxy spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 name: http-web-svc --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: LoadBalancer selector: app.kubernetes.io/name: proxy ports: - name: name-of-service-port protocol: TCP port: 8888 targetPort: http-web-svc The above steps create the nginx pod and then associates a LoadBalancer service to it. The command to apply is as follows kubectl apply -f test-nginx-svc.yaml We can verify that the service nginx-service has been created as a LoadBalancer type and has been assigned an External IP. Now you can access the kubernetes service from outside using IP 123.123.123.15 and port 8888. vagrant@node1:~$ sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.233.0.1 <none> 443/TCP 28d nginx-service LoadBalancer 10.233.21.235 123.123.123.15 8888:31655/TCP 3s The LoadBalancer rule is also created in the loxilb container. We can check in the loxilb load-balancer node as follows netlox@nd8:~$ sudo docker exec -ti loxilb loxicmd get lb | EXTERNAL IP | PORT | PROTOCOL | SELECT | # OF ENDPOINTS | |----------------|------|----------|--------|----------------| | 123.123.123.15 | 8888 | tcp | 0 | 2 | 4. calico BGP & loxilb setup If calico configures the network in BGP mode, loxilb must also operate in BGP mode. loxilb supports BGP functions based on goBGP. The following description assumes that calico is already set to use BGP mode. 4.1 loxilb BGP mode setup If we create a loxilb container with the following command, it will run in BGP mode. The -b option at the end of the command is to enable BGP mode in loxilb. sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped \\ --privileged -dit -v /dev/log:/dev/log \\ --net=k8snet --ip=192.168.57.4 --name loxilb ghcr.io/loxilb-io/loxilb:latest \\ --host=0.0.0.0 -b 4.2 gobgp_loxilb.yaml file setup Create a gobgp_loxilb.yaml file in the /etc/gobgp/ directory of the loxilb container. global: config: as: 65002 router-id: 172.1.0.2 neighbors: - config: neighbor-address: 192.168.57.101 peer-as: 64512 - config: neighbor-address: 192.168.20.55 peer-as: 64001 BGP information such as as-id and router-id of the loxilb container must be registered as global items. The neighbors item need info about the IP address and as-id information of the BGP router peering with loxilb. In this example, calico's BGP information (192.168.57.101) and external BGP information (129.168.20.55) were registered. 4.3 Add router-id to the lo interface of the loxilb container We need to add the IP registered as loxilb router-id in the gobgp_loxilb.yaml file to the lo interface of loxilb docker. sudo docker exec -ti loxilb ip addr add 172.1.0.2/32 dev lo 4.4 loxilb docker restart Restart the loxilb docker for the settings in gobgp_loxilb.yaml to take effect. sudo docker stop loxilb sudo docker start loxilb 4.5 Setup BGP Peer information in Calico We also need to add loxilb's BGP peer information to calico. Create the calico-bgp-config.yaml file as follows: apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peers2 spec: peerIP: 192.168.57.4 asNumber: 65002 In peerIP, enter the IP address of loxilb. In asNumber, enter the as-ID of the loxilb BGP set above. After creating the file, add BGP peer information to calico with the command below. sudo calicoctl apply -f calico-bgp-config.yaml 4.6 Check BGP status We now check the BGP connectivity in the loxilb docker like this: netlox@nd8:~$ sudo docker exec -ti loxilb3 gobgp neigh Peer AS Up/Down State |#Received Accepted 192.168.57.101 64512 00:00:59 Establ | 4 4 If the connection is successful, the State will be shown as \"Established\". We can check the route information of calico with the gobgp global rib command. netlox@nd8:~$ sudo docker exec -ti loxilb3 gobgp global rib Network Next Hop AS_PATH Age Attrs *> 10.233.71.0/26 192.168.57.101 64512 01:02:03 [{Origin: i}] *> 10.233.74.64/26 192.168.57.101 64512 01:02:03 [{Origin: i}] *> 10.233.75.0/26 192.168.57.101 64512 01:02:03 [{Origin: i}] *> 10.233.102.128/26 192.168.57.101 64512 01:02:03 [{Origin: i}]","title":"How to run loxilb with calico CNI in BGP mode"},{"location":"integrate_bgp_eng/#how-to-run-loxilb-with-calico-cni-in-bgp-mode","text":"This article describes how to integrate loxilb using calico CNI in Kubernetes.","title":"How to run loxilb with calico CNI in BGP mode"},{"location":"integrate_bgp_eng/#setup","text":"For this example, kubernetes and loxilb are setup as follows: Kubernetes uses a single master cluster for simplicity, all clusters use the same 192.168.57.0/24 subnet. The load balancer node where the loxilb container is running is also connected to the same subnet as kubernetes. Externally, all kubernetes connections are configured to go through the \"loxilb\" load balancer node. This example uses docker to run the loxilb container. These examples assume that kubernetes & calico are already installed.","title":"Setup"},{"location":"integrate_bgp_eng/#1-loxilb-setup","text":"","title":"1. loxilb setup"},{"location":"integrate_bgp_eng/#11-docker-network-setup","text":"In order to integrate loxilb and kubernetes, loxilb needs to be able to communicate with Kubernetes. First, we create a docker network so that the loxilb container is also connected to the same network which is used by kubernetes nodes. Currently, the load balancer node is connected to kubernetes through the eno6 interface. Therefore, we will create a macvlan-type docker network that uses the eno6 interface as a parent and provide it to the loxilb docker. Create a docker network with the following command: sudo docker network create -d macvlan -o parent=eno6 \\ --subnet 192.168.57.0/24 \\ --gateway 192.168.57.1 \\ --aux-address 'cp1=192.168.57.101' \\ --aux-address 'cp2=192.168.57.102' \\ --aux-address 'cp3=192.168.57.103' k8snet |Options|Description| |----|----| |-d macvlan|Specify network type as macvlan| |-o parent=eno6|Create a macvlan type network using the eno6 interface as parent| |--subnet 192.168.57.0/24|Specify network subnet| |--gateway 192.168.57.1|Set gateway (optional)| |--aux-address 'serverName=serverIP'|Option to register in advance so that IP addresses already in use on the network are not duplicated| |k8snet|Name the network k8snet| A docker network that can communicate with the outside is also created so that traffic accessing the kubernetes service from outside also goes through loxilb. The load balancer node is connected to the outside through eno8. sudo docker network create -d macvlan -o parent=eno8 \\ --subnet 192.168.20.0/24 \\ --gateway 192.168.20.1 llbnet We can check the network created with the docker network list command. netlox@nd8:~$ sudo docker network list NETWORK ID NAME DRIVER SCOPE 5c97ae74fc32 bridge bridge local 6142f53e8be6 host host local 24ee7dbd7707 k8snet macvlan local 81c96ceda375 llbnet macvlan local 7bcd1738501b none null local","title":"1.1 docker network setup"},{"location":"integrate_bgp_eng/#12-loxilb-docker-setup","text":"The loxilb container image is provided at github . To download the docker image, use the following command. docker pull ghcr.io/loxilb-io/loxilb:latest To run loxilb docker, we can use the following command. sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped \\ --privileged -dit -v /dev/log:/dev/log \\ --net=k8snet --ip=192.168.57.4 --name loxilb ghcr.io/loxilb-io/loxilb:latest \\ --host=0.0.0.0 The options that need to specified are: Options Description --net=k8snet Network to connect to container --ip=192.168.57.4 Specifies the IP address the container will use. If not specified, use any IP within the network subnet range --name loxilb Set container name We can check the docker created with the docker ps command. netlox@nd8:~$ sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES eae349a283ae loxilbio/loxilb:beta \"/root/loxilb-io/lox\u2026\" 11 days ago Up 11 days loxilb Since we only connected the kubernetes network (k8snet) when running the docker above, we also need to connect to the docker network for external communication. Currently, docker only supports one network to be connected when running \"docker run\" command. But, its easy to connect other network to the docker with the following command: sudo docker network connect llbnet loxilb Once the connection is complete, we can see the docker container's interfaces as follows: netlox@netlox:~$ sudo docker exec -ti loxilb ip route default via 192.168.20.1 dev eth0 192.168.20.0/24 dev eth0 proto kernel scope link src 192.168.20.4 192.168.30.0/24 dev eth1 proto kernel scope link src 192.168.30.2","title":"1.2 loxilb docker setup"},{"location":"integrate_bgp_eng/#2-loxi-ccm-setup-in-kubernetes","text":"loxi-ccm is a ccm provider to provide a loxilb load balancer to kubernetes, and it is essential for interworking with kubernetes and loxilb. Refer to the relevant document , change the apiServerURL of configMap to the IP address of loxilb created above and install it in kubernetes. If loxi-ccm is installed properly, the setup is complete.","title":"2. loxi-ccm setup in kubernetes"},{"location":"integrate_bgp_eng/#3basic-load-balancer-test","text":"We can now give an External IP when you create a LoadBalancer type service in kubernetes. Create a test-nginx-svc.yaml file for testing as follows: apiVersion: v1 kind: Pod metadata: name: nginx labels: app.kubernetes.io/name: proxy spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 name: http-web-svc --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: LoadBalancer selector: app.kubernetes.io/name: proxy ports: - name: name-of-service-port protocol: TCP port: 8888 targetPort: http-web-svc The above steps create the nginx pod and then associates a LoadBalancer service to it. The command to apply is as follows kubectl apply -f test-nginx-svc.yaml We can verify that the service nginx-service has been created as a LoadBalancer type and has been assigned an External IP. Now you can access the kubernetes service from outside using IP 123.123.123.15 and port 8888. vagrant@node1:~$ sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.233.0.1 <none> 443/TCP 28d nginx-service LoadBalancer 10.233.21.235 123.123.123.15 8888:31655/TCP 3s The LoadBalancer rule is also created in the loxilb container. We can check in the loxilb load-balancer node as follows netlox@nd8:~$ sudo docker exec -ti loxilb loxicmd get lb | EXTERNAL IP | PORT | PROTOCOL | SELECT | # OF ENDPOINTS | |----------------|------|----------|--------|----------------| | 123.123.123.15 | 8888 | tcp | 0 | 2 |","title":"3.Basic load-balancer test"},{"location":"integrate_bgp_eng/#4-calico-bgp-loxilb-setup","text":"If calico configures the network in BGP mode, loxilb must also operate in BGP mode. loxilb supports BGP functions based on goBGP. The following description assumes that calico is already set to use BGP mode.","title":"4. calico BGP &amp; loxilb setup"},{"location":"integrate_bgp_eng/#41-loxilb-bgp-mode-setup","text":"If we create a loxilb container with the following command, it will run in BGP mode. The -b option at the end of the command is to enable BGP mode in loxilb. sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped \\ --privileged -dit -v /dev/log:/dev/log \\ --net=k8snet --ip=192.168.57.4 --name loxilb ghcr.io/loxilb-io/loxilb:latest \\ --host=0.0.0.0 -b","title":"4.1 loxilb BGP mode setup"},{"location":"integrate_bgp_eng/#42-gobgp_loxilbyaml-file-setup","text":"Create a gobgp_loxilb.yaml file in the /etc/gobgp/ directory of the loxilb container. global: config: as: 65002 router-id: 172.1.0.2 neighbors: - config: neighbor-address: 192.168.57.101 peer-as: 64512 - config: neighbor-address: 192.168.20.55 peer-as: 64001 BGP information such as as-id and router-id of the loxilb container must be registered as global items. The neighbors item need info about the IP address and as-id information of the BGP router peering with loxilb. In this example, calico's BGP information (192.168.57.101) and external BGP information (129.168.20.55) were registered.","title":"4.2 gobgp_loxilb.yaml file setup"},{"location":"integrate_bgp_eng/#43-add-router-id-to-the-lo-interface-of-the-loxilb-container","text":"We need to add the IP registered as loxilb router-id in the gobgp_loxilb.yaml file to the lo interface of loxilb docker. sudo docker exec -ti loxilb ip addr add 172.1.0.2/32 dev lo","title":"4.3 Add router-id to the lo interface of the loxilb container"},{"location":"integrate_bgp_eng/#44-loxilb-docker-restart","text":"Restart the loxilb docker for the settings in gobgp_loxilb.yaml to take effect. sudo docker stop loxilb sudo docker start loxilb","title":"4.4 loxilb docker restart"},{"location":"integrate_bgp_eng/#45-setup-bgp-peer-information-in-calico","text":"We also need to add loxilb's BGP peer information to calico. Create the calico-bgp-config.yaml file as follows: apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peers2 spec: peerIP: 192.168.57.4 asNumber: 65002 In peerIP, enter the IP address of loxilb. In asNumber, enter the as-ID of the loxilb BGP set above. After creating the file, add BGP peer information to calico with the command below. sudo calicoctl apply -f calico-bgp-config.yaml","title":"4.5 Setup BGP Peer information in Calico"},{"location":"integrate_bgp_eng/#46-check-bgp-status","text":"We now check the BGP connectivity in the loxilb docker like this: netlox@nd8:~$ sudo docker exec -ti loxilb3 gobgp neigh Peer AS Up/Down State |#Received Accepted 192.168.57.101 64512 00:00:59 Establ | 4 4 If the connection is successful, the State will be shown as \"Established\". We can check the route information of calico with the gobgp global rib command. netlox@nd8:~$ sudo docker exec -ti loxilb3 gobgp global rib Network Next Hop AS_PATH Age Attrs *> 10.233.71.0/26 192.168.57.101 64512 01:02:03 [{Origin: i}] *> 10.233.74.64/26 192.168.57.101 64512 01:02:03 [{Origin: i}] *> 10.233.75.0/26 192.168.57.101 64512 01:02:03 [{Origin: i}] *> 10.233.102.128/26 192.168.57.101 64512 01:02:03 [{Origin: i}]","title":"4.6 Check BGP status"},{"location":"k0s_quick_start/","text":"LoxiLB Quick Start Guide with k0s/kube-router This guide will explain how to: Deploy a single-node K0s cluster with kube-router networking Expose services with loxilb as an external load balancer Prerequisite(s) Single node with Linux Install docker runtime to manage loxilb Topology For quickly bringing up loxilb with K0s/kube-router, we will be deploying all components in a single node : loxilb is run as a docker and will use macvlan for the incoming traffic. This is to mimic a topology close to cloud-hosted k8s where LB nodes run outside a cluster. loxilb can be used in more complex in-cluster mode as well, but not used here for simplicity. Install loxilb docker ## Set underlying interface of the VM/cluster-node to promisc mode for mac-vlan to work sudo ifconfig eth1 promisc ## Run loxilb sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged --entrypoint /root/loxilb-io/loxilb/loxilb -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest # Create mac-vlan on top of underlying eth1 interface sudo docker network create -d macvlan -o parent=eth1 --subnet 192.168.82.0/24 --gateway 192.168.82.1 --aux-address 'host=192.168.82.252' llbnet # Assign mac-vlan to loxilb docker with specified IP (which will be used as LB VIP) # Please note that this node should already have an IP assigned belonging to the same subnet on underlying interface sudo docker network connect llbnet loxilb --ip=192.168.82.100 # Add iptables rule to allow traffic from source/host IP(192.168.82.1) to loxilb sudo iptables -A DOCKER -s 192.168.82.1 -j ACCEPT All the above steps related to docker setup can be further automated using docker-compose. Setup k0s/kube-router in single-node #K0s installation steps curl -sSLf https://get.k0s.sh | sudo sh sudo k0s install controller --single sudo k0s start Check k0s status sudo k0s status How to deploy kube-loxilb ? kube-loxilb is used to deploy loxilb with Kubernetes. wget https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/ext-cluster/kube-loxilb.yaml Change args in kube-loxilb.yaml args: - --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setMode=1 In the above snippet, loxiURL uses docker interface IP of loxilb, which can be different for each setup. Apply in k0s: $ sudo k0s kubectl apply -f kube-loxilb.yaml Create the service $ sudo k0s kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/docker-k0s-lb/tcp-svc-lb.yml Check the status In k0s: $ sudo k0s kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 80m tcp-lb-onearm LoadBalancer 10.43.183.123 llb-192.168.82.100 56002:30001/TCP 6m50s In loxilb docker: $ sudo docker exec -it loxilb loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-----------------------|------|-----|--------|-----------|-------|--------|--------|----------| | 192.168.82.100 | | 56002 | tcp | default_tcp-lb-onearm | 0 | rr | onearm | 10.0.2.15 | 30001 | 1 | active | 12:880 | Connect from host/client $ curl http://192.168.82.100:56002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> All of the above steps are also available as part of loxilb CICD workflow. Follow the steps below to replicate the above (please note that you will need vagrant tool installed to run: $ git clone https://github.com/loxilb-io/loxilb.git $ cd cicd/docker-k0s-lb/ # To setup the single node k0s setup with kube-router networking and loxilb as external load balancer $ ./config.sh # To validate the results $ ./validation.sh # Cleanup $ ./rmconfig.sh","title":"ext-cluster pod - K0s/loxilb with kube-router"},{"location":"k0s_quick_start/#loxilb-quick-start-guide-with-k0skube-router","text":"This guide will explain how to: Deploy a single-node K0s cluster with kube-router networking Expose services with loxilb as an external load balancer","title":"LoxiLB Quick Start Guide with k0s/kube-router"},{"location":"k0s_quick_start/#prerequisites","text":"Single node with Linux Install docker runtime to manage loxilb","title":"Prerequisite(s)"},{"location":"k0s_quick_start/#topology","text":"For quickly bringing up loxilb with K0s/kube-router, we will be deploying all components in a single node : loxilb is run as a docker and will use macvlan for the incoming traffic. This is to mimic a topology close to cloud-hosted k8s where LB nodes run outside a cluster. loxilb can be used in more complex in-cluster mode as well, but not used here for simplicity.","title":"Topology"},{"location":"k0s_quick_start/#install-loxilb-docker","text":"## Set underlying interface of the VM/cluster-node to promisc mode for mac-vlan to work sudo ifconfig eth1 promisc ## Run loxilb sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged --entrypoint /root/loxilb-io/loxilb/loxilb -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest # Create mac-vlan on top of underlying eth1 interface sudo docker network create -d macvlan -o parent=eth1 --subnet 192.168.82.0/24 --gateway 192.168.82.1 --aux-address 'host=192.168.82.252' llbnet # Assign mac-vlan to loxilb docker with specified IP (which will be used as LB VIP) # Please note that this node should already have an IP assigned belonging to the same subnet on underlying interface sudo docker network connect llbnet loxilb --ip=192.168.82.100 # Add iptables rule to allow traffic from source/host IP(192.168.82.1) to loxilb sudo iptables -A DOCKER -s 192.168.82.1 -j ACCEPT All the above steps related to docker setup can be further automated using docker-compose.","title":"Install loxilb docker"},{"location":"k0s_quick_start/#setup-k0skube-router-in-single-node","text":"#K0s installation steps curl -sSLf https://get.k0s.sh | sudo sh sudo k0s install controller --single sudo k0s start","title":"Setup k0s/kube-router in single-node"},{"location":"k0s_quick_start/#check-k0s-status","text":"sudo k0s status","title":"Check k0s status"},{"location":"k0s_quick_start/#how-to-deploy-kube-loxilb","text":"kube-loxilb is used to deploy loxilb with Kubernetes. wget https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/ext-cluster/kube-loxilb.yaml Change args in kube-loxilb.yaml args: - --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setMode=1 In the above snippet, loxiURL uses docker interface IP of loxilb, which can be different for each setup. Apply in k0s: $ sudo k0s kubectl apply -f kube-loxilb.yaml","title":"How to deploy kube-loxilb ?"},{"location":"k0s_quick_start/#create-the-service","text":"$ sudo k0s kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/docker-k0s-lb/tcp-svc-lb.yml","title":"Create the service"},{"location":"k0s_quick_start/#check-the-status","text":"In k0s: $ sudo k0s kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 80m tcp-lb-onearm LoadBalancer 10.43.183.123 llb-192.168.82.100 56002:30001/TCP 6m50s In loxilb docker: $ sudo docker exec -it loxilb loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-----------------------|------|-----|--------|-----------|-------|--------|--------|----------| | 192.168.82.100 | | 56002 | tcp | default_tcp-lb-onearm | 0 | rr | onearm | 10.0.2.15 | 30001 | 1 | active | 12:880 |","title":"Check the status"},{"location":"k0s_quick_start/#connect-from-hostclient","text":"$ curl http://192.168.82.100:56002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> All of the above steps are also available as part of loxilb CICD workflow. Follow the steps below to replicate the above (please note that you will need vagrant tool installed to run: $ git clone https://github.com/loxilb-io/loxilb.git $ cd cicd/docker-k0s-lb/ # To setup the single node k0s setup with kube-router networking and loxilb as external load balancer $ ./config.sh # To validate the results $ ./validation.sh # Cleanup $ ./rmconfig.sh","title":"Connect from host/client"},{"location":"k0s_quick_start_incluster/","text":"Quick Start Guide with K0s and LoxiLB in-cluster mode This document will explain how to install a K0s cluster with loxilb as a serviceLB provider running in-cluster mode. Prerequisite(s) Single node with Linux Topology For quickly bringing up loxilb in-cluster and K0s, we will be deploying all components in a single node : loxilb and kube-loxilb components run as pods managed by kubernetes in this scenario. Setup k0s in a single-node # k0s installation steps curl -sSLf https://get.k0s.sh | sudo sh sudo k0s install controller --single sudo k0s start Check k0s status $ sudo k0s status Version: v1.29.2+k0s.0 Process ID: 2631 Workloads: true SingleNode: true Kube-api probing successful: true Kube-api probing last error: How to deploy loxilb ? loxilb can be deloyed by using the following command in the K3s node sudo k0s kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/k0s-incluster/loxilb.yml How to deploy kube-loxilb ? kube-loxilb is used as an operator to manage loxilb. wget https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/k0s-incluster/kube-loxilb.yml kube-loxilb.yaml args: #- --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setRoles=0.0.0.0 #- --monitor #- --setBGP In the above snippet, loxiURL is commented out which denotes to utilize in-cluster mode to discover loxilb pods automatically. External CIDR represents the IP pool from where serviceLB VIP will be allocated. Apply after making changes (if any) : sudo k0s kubectl apply -f kube-loxilb.yaml Create the service sudo k0s kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/k0s-incluster/tcp-svc-lb.yml Check status of various components in k0s node In k0s node: ## Check the pods created $ sudo k0s kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kube-proxy-vczxm 1/1 Running 0 4m48s kube-system kube-router-gjp7g 1/1 Running 0 4m48s kube-system metrics-server-7556957bb7-25hsk 1/1 Running 0 4m50s kube-system coredns-6cd46fb86c-xllg2 1/1 Running 0 4m50s kube-system loxilb-lb-4fmdp 1/1 Running 0 3m43s kube-system kube-loxilb-6f44cdcdf5-ffdcv 1/1 Running 0 2m22s default tcp-onearm-test 1/1 Running 0 92s ## Check the services created $ sudo k0s kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 5m28s tcp-lb-onearm LoadBalancer 10.96.108.109 llb-192.168.82.100 56002:32033/TCP 111s In loxilb pod, we can check internal LB rules: $ sudo k0s kubectl exec -it -n kube-system loxilb-lb-4fmdp -- loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-----------------------|------|-----|--------|-----------|-------|--------|--------|----------| | 192.168.82.100 | | 56002 | tcp | default_tcp-lb-onearm | 0 | rr | onearm | 10.0.2.15 | 32033 | 1 | active | 25:1842 | Connect from host/client $ curl http://192.168.82.100:56002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> For more detailed information on incluster deployment of loxilb with bgp in a full-blown cluster, kindly follow this blog . All of the above steps are also available as part of loxilb CICD workflow. Follow the steps below to replicate the above (please note that you will need vagrant tool installed to run: $ git clone https://github.com/loxilb-io/loxilb.git $ cd cicd/k0s-incluster/ # To setup the single node k0s setup with kube-router networking and loxilb as external load balancer $ ./config.sh # To validate the results $ ./validation.sh # To login to the node and check the installation $ vagrant ssh k0s-node1 # Cleanup $ ./rmconfig.sh","title":"in-cluster pod - K0s/loxilb in-cluster mode"},{"location":"k0s_quick_start_incluster/#quick-start-guide-with-k0s-and-loxilb-in-cluster-mode","text":"This document will explain how to install a K0s cluster with loxilb as a serviceLB provider running in-cluster mode.","title":"Quick Start Guide with K0s and LoxiLB in-cluster mode"},{"location":"k0s_quick_start_incluster/#prerequisites","text":"Single node with Linux","title":"Prerequisite(s)"},{"location":"k0s_quick_start_incluster/#topology","text":"For quickly bringing up loxilb in-cluster and K0s, we will be deploying all components in a single node : loxilb and kube-loxilb components run as pods managed by kubernetes in this scenario.","title":"Topology"},{"location":"k0s_quick_start_incluster/#setup-k0s-in-a-single-node","text":"# k0s installation steps curl -sSLf https://get.k0s.sh | sudo sh sudo k0s install controller --single sudo k0s start","title":"Setup k0s in a single-node"},{"location":"k0s_quick_start_incluster/#check-k0s-status","text":"$ sudo k0s status Version: v1.29.2+k0s.0 Process ID: 2631 Workloads: true SingleNode: true Kube-api probing successful: true Kube-api probing last error:","title":"Check k0s status"},{"location":"k0s_quick_start_incluster/#how-to-deploy-loxilb","text":"loxilb can be deloyed by using the following command in the K3s node sudo k0s kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/k0s-incluster/loxilb.yml","title":"How to deploy loxilb ?"},{"location":"k0s_quick_start_incluster/#how-to-deploy-kube-loxilb","text":"kube-loxilb is used as an operator to manage loxilb. wget https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/k0s-incluster/kube-loxilb.yml kube-loxilb.yaml args: #- --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setRoles=0.0.0.0 #- --monitor #- --setBGP In the above snippet, loxiURL is commented out which denotes to utilize in-cluster mode to discover loxilb pods automatically. External CIDR represents the IP pool from where serviceLB VIP will be allocated. Apply after making changes (if any) : sudo k0s kubectl apply -f kube-loxilb.yaml","title":"How to deploy kube-loxilb ?"},{"location":"k0s_quick_start_incluster/#create-the-service","text":"sudo k0s kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/k0s-incluster/tcp-svc-lb.yml","title":"Create the service"},{"location":"k0s_quick_start_incluster/#check-status-of-various-components-in-k0s-node","text":"In k0s node: ## Check the pods created $ sudo k0s kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kube-proxy-vczxm 1/1 Running 0 4m48s kube-system kube-router-gjp7g 1/1 Running 0 4m48s kube-system metrics-server-7556957bb7-25hsk 1/1 Running 0 4m50s kube-system coredns-6cd46fb86c-xllg2 1/1 Running 0 4m50s kube-system loxilb-lb-4fmdp 1/1 Running 0 3m43s kube-system kube-loxilb-6f44cdcdf5-ffdcv 1/1 Running 0 2m22s default tcp-onearm-test 1/1 Running 0 92s ## Check the services created $ sudo k0s kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 5m28s tcp-lb-onearm LoadBalancer 10.96.108.109 llb-192.168.82.100 56002:32033/TCP 111s In loxilb pod, we can check internal LB rules: $ sudo k0s kubectl exec -it -n kube-system loxilb-lb-4fmdp -- loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-----------------------|------|-----|--------|-----------|-------|--------|--------|----------| | 192.168.82.100 | | 56002 | tcp | default_tcp-lb-onearm | 0 | rr | onearm | 10.0.2.15 | 32033 | 1 | active | 25:1842 |","title":"Check status of various components in k0s node"},{"location":"k0s_quick_start_incluster/#connect-from-hostclient","text":"$ curl http://192.168.82.100:56002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> For more detailed information on incluster deployment of loxilb with bgp in a full-blown cluster, kindly follow this blog . All of the above steps are also available as part of loxilb CICD workflow. Follow the steps below to replicate the above (please note that you will need vagrant tool installed to run: $ git clone https://github.com/loxilb-io/loxilb.git $ cd cicd/k0s-incluster/ # To setup the single node k0s setup with kube-router networking and loxilb as external load balancer $ ./config.sh # To validate the results $ ./validation.sh # To login to the node and check the installation $ vagrant ssh k0s-node1 # Cleanup $ ./rmconfig.sh","title":"Connect from host/client"},{"location":"k3s-multi-master/","text":"Guide to deploy multi-master HA K3s with loxilb This document will explain how to install a multi-master HA K3s cluster with loxilb as a serviceLB provider running in-cluster mode. K3s is a lightweight Kubernetes distribution and is increasingly used for prototyping as well as for production workloads. K3s nodes are deployed as: 1) k3s-server nodes for k3s control plane components like apiserver and etcd. 2) k3s-agent nodes hosting user workloads/apps. When we deploy multi-master nodes, it is necessary that they be accessed from the k3s-agents in HA configuration and behind a load-balancer. Usually deploying such a load-balancer is outside the scope of kubernetes. In this guide, we will see how to deploy loxilb not only as cluster's serviceLB provider but also as a VIP-LB for accessing server/master node(s) services. Topology For multi-master setup we need an odd number of server nodes to maintain quorum. So, we will have 3 k3s-server nodes for this setup. Overall, we will be deploying the components as per the following topology : K3s installation and Setup In k3s-server1 node - $ curl -fL https://get.k3s.io | sh -s - server --node-ip=192.168.80.10 \\ --disable servicelb --disable traefik --cluster-init external-hostname=192.168.80.10 \\ --node-external-ip=192.168.80.80 --disable-cloud-controller It is to be noted that --node-external-ip=192.168.80.80 is used since we will utilize 192.168.80.80 as the VIP to access the multi-master setup from k3s-agents and other clients. Setup the node for loxilb : sudo mkdir -p /etc/loxilb Create the following files in /etc/loxilb lbconfig.txt with following contents (change as per your requirement) { \"lbAttr\":[ { \"serviceArguments\":{ \"externalIP\":\"192.168.80.80\", \"port\":6443, \"protocol\":\"tcp\", \"sel\":0, \"mode\":2, \"BGP\":false, \"Monitor\":true, \"inactiveTimeOut\":240, \"block\":0 }, \"secondaryIPs\":null, \"endpoints\":[ { \"endpointIP\":\"192.168.80.10\", \"targetPort\":6443, \"weight\":1, \"state\":\"active\", \"counter\":\"\" }, { \"endpointIP\":\"192.168.80.11\", \"targetPort\":6443, \"weight\":1, \"state\":\"active\", \"counter\":\"\" }, { \"endpointIP\":\"192.168.80.12\", \"targetPort\":6443, \"weight\":1, \"state\":\"active\", \"counter\":\"\" } ] } ] } 2. EPconfig.txt with the following contents (change as per your requirement) { \"Attr\":[ { \"hostName\":\"192.168.80.10\", \"name\":\"192.168.80.10_tcp_6443\", \"inactiveReTries\":2, \"probeType\":\"tcp\", \"probeReq\":\"\", \"probeResp\":\"\", \"probeDuration\":10, \"probePort\":6443 }, { \"hostName\":\"192.168.80.11\", \"name\":\"192.168.80.11_tcp_6443\", \"inactiveReTries\":2, \"probeType\":\"tcp\", \"probeReq\":\"\", \"probeResp\":\"\", \"probeDuration\":10, \"probePort\":6443 }, { \"hostName\":\"192.168.80.12\", \"name\":\"192.168.80.12_tcp_6443\", \"inactiveReTries\":2, \"probeType\":\"tcp\", \"probeReq\":\"\", \"probeResp\":\"\", \"probeDuration\":10, \"probePort\":6443 } ] } The above serve as bootstrap LB rules for load-balancing into the k3s-server nodes as we will see later. In k3s-server2 node - $ curl -fL https://get.k3s.io | K3S_TOKEN=${NODE_TOKEN} sh -s - server --server https://192.168.80.10:6443 \\ --disable traefik --disable servicelb --node-ip=192.168.80.11 \\ external-hostname=192.168.80.11 --node-external-ip=192.168.80.80 -t ${NODE_TOKEN} where NODE_TOKEN contain simply contents of /var/lib/rancher/k3s/server/node-token from server1. For example, it can be set using a command equivalent to the following : export NODE_TOKEN=$(cat node-token) Setup the node for loxilb: Simply follow the steps as outlined for server1. In k3s-server3 node - $ curl -fL https://get.k3s.io | K3S_TOKEN=${NODE_TOKEN} sh -s - server --server https://192.168.80.10:6443 \\ --disable traefik --disable servicelb --node-ip=192.168.80.12 \\ external-hostname=192.168.80.12 --node-external-ip=192.168.80.80 -t ${NODE_TOKEN} where NODE_TOKEN contain simply contents of /var/lib/rancher/k3s/server/node-token from server1. For example, it can be set using a command equivalent to the following : export NODE_TOKEN=$(cat node-token) Setup the node for loxilb: First, follow the steps as outlined for server1. Additionally, we will have to start loxilb pod instances as follows : $ sudo kubectl apply -f - <<EOF apiVersion: apps/v1 kind: DaemonSet metadata: name: loxilb-lb namespace: kube-system spec: selector: matchLabels: app: loxilb-app template: metadata: name: loxilb-lb labels: app: loxilb-app spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet tolerations: - key: \"node-role.kubernetes.io/master\" operator: Exists - key: \"node-role.kubernetes.io/control-plane\" operator: Exists affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: \"node-role.kubernetes.io/master\" operator: Exists - key: \"node-role.kubernetes.io/control-plane\" operator: Exists volumes: - name: hllb hostPath: path: /etc/loxilb type: DirectoryOrCreate containers: - name: loxilb-app image: \"ghcr.io/loxilb-io/loxilb:latest\" imagePullPolicy: Always command: - /root/loxilb-io/loxilb/loxilb args: - --egr-hooks - --blacklist=cni[0-9a-z]|veth.|flannel. volumeMounts: - name: hllb mountPath: /etc/loxilb ports: - containerPort: 11111 - containerPort: 179 securityContext: privileged: true capabilities: add: - SYS_ADMIN --- apiVersion: v1 kind: Service metadata: name: loxilb-lb-service namespace: kube-system spec: clusterIP: None selector: app: loxilb-app ports: - name: loxilb-app port: 11111 targetPort: 11111 protocol: TCP EOF Kindly note that the args for loxilb might change depending on the scenario. This scenario considers loxilb running in-cluster mode. For service-proxy mode, please follow this yaml for exact args. Next, we will install loxilb's operator kube-loxilb as follows : $ sudo kubectl apply -f - <<EOF --- apiVersion: v1 kind: ServiceAccount metadata: name: kube-loxilb namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb rules: - apiGroups: - \"\" resources: - nodes verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - pods verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - endpoints - services - services/status verbs: - get - watch - list - patch - update - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - get - watch - list - apiGroups: - authentication.k8s.io resources: - tokenreviews verbs: - create - apiGroups: - authorization.k8s.io resources: - subjectaccessreviews verbs: - create --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-loxilb subjects: - kind: ServiceAccount name: kube-loxilb namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: kube-loxilb namespace: kube-system labels: app: loxilb spec: replicas: 1 selector: matchLabels: app: loxilb template: metadata: labels: app: loxilb spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet tolerations: - effect: NoSchedule operator: Exists # Mark the pod as a critical add-on for rescheduling. - key: CriticalAddonsOnly operator: Exists - effect: NoExecute operator: Exists - key: \"node-role.kubernetes.io/master\" operator: Exists - key: \"node-role.kubernetes.io/control-plane\" operator: Exists affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: \"node-role.kubernetes.io/master\" operator: Exists - key: \"node-role.kubernetes.io/control-plane\" operator: Exists priorityClassName: system-node-critical serviceAccountName: kube-loxilb terminationGracePeriodSeconds: 0 containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: - --externalCIDR=192.168.80.200/32 - --setRoles=0.0.0.0 resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"] EOF At this point we can check the pods running in our kubernetes cluster (in server1, server2 & server3 at this point): $ sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-6799fbcd5-7jhcx 1/1 Running 0 3h15m kube-system kube-loxilb-5d99c445f7-j4x6k 1/1 Running 0 3h6m kube-system local-path-provisioner-6c86858495-pjn9j 1/1 Running 0 3h15m kube-system loxilb-lb-8bddf 1/1 Running 0 3h6m kube-system loxilb-lb-nsrr9 1/1 Running 0 3h6m kube-system loxilb-lb-fp2z6 1/1 Running 0 3h6m kube-system metrics-server-54fd9b65b-g5lfn 1/1 Running 0 3h15m In k3s-agent1 node - The following steps need to be followed to install k3s in the agent nodes: $ curl -sfL https://get.k3s.io | K3S_TOKEN=${NODE_TOKEN} sh -s - agent --server https://192.168.80.80:6443 --node-ip=${WORKER_ADDR} --node-external-ip=${WORKER_ADDR} -t ${NODE_TOKEN} where WORKER_ADDR is the IP address of the agent node itself (in this case 192.168.80.101) and NODE_TOKEN has contents of /var/lib/rancher/k3s/server/node-token from server1. It is also to be noted that we use VIP - 192.168.80.80 provided by loxilb to access the server(master) K3s nodes and not the actual private node addresses. For rest of the agent nodes, we can follow the same set of steps as outlined above for k3s-agent1. Validation After setting up all the k3s-server and k3s-agents, we should be able to see all nodes up and running $ sudo kubectl get nodes -A NAME STATUS ROLES AGE VERSION master1 Ready control-plane,etcd,master 4h v1.29.3+k3s1 master2 Ready control-plane,etcd,master 4h v1.29.3+k3s1 master3 Ready control-plane,etcd,master 4h v1.29.3+k3s1 worker1 Ready <none> 4h v1.29.3+k3s1 worker2 Ready <none> 4h v1.29.3+k3s1 worker3 Ready <none> 4h v1.29.3+k3s1 To verify, let's shutdown master1 k3s-server. ## Run shutdown the master1 node $ sudo shutdown -t now And try to access cluster information from other master nodes or worker nodes : $ sudo kubectl get nodes -A NAME STATUS ROLES AGE VERSION master1 NotReady control-plane,etcd,master 4h10m v1.29.3+k3s1 master2 Ready control-plane,etcd,master 4h10m v1.29.3+k3s1 master3 Ready control-plane,etcd,master 4h10m v1.29.3+k3s1 worker1 Ready <none> 4h10m v1.29.3+k3s1 worker2 Ready <none> 4h10m v1.29.3+k3s1 Also, we can confirm pods getting rescheduled to other \"ready\" nodes : $ sudo kubectl get pods -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system coredns-6799fbcd5-6dvm7 1/1 Running 0 27m 10.42.2.2 master3 <none> <none> kube-system coredns-6799fbcd5-mrjgt 1/1 Terminating 0 3h58m 10.42.0.4 master1 <none> <none> kube-system kube-loxilb-5d99c445f7-x7qd6 1/1 Running 0 3h58m 192.168.80.11 master2 <none> <none> kube-system local-path-provisioner-6c86858495-6f8rz 1/1 Terminating 0 3h58m 10.42.0.2 master1 <none> <none> kube-system local-path-provisioner-6c86858495-z2p6m 1/1 Running 0 27m 10.42.3.2 worker1 <none> <none> kube-system loxilb-lb-65jnz 1/1 Running 0 3h58m 192.168.80.10 master1 <none> <none> kube-system loxilb-lb-pfkf8 1/1 Running 0 3h58m 192.168.80.12 master3 <none> <none> kube-system loxilb-lb-xhr95 1/1 Running 0 3h58m 192.168.80.11 master2 <none> <none> kube-system metrics-server-54fd9b65b-l5pqz 1/1 Running 0 27m 10.42.4.2 worker2 <none> <none> kube-system metrics-server-54fd9b65b-x9bd7 1/1 Terminating 0 3h58m 10.42.0.3 master1 <none> <none> If the above set of command works fine in any of the \"ready\" nodes, it indicates that the api server is available even when one of k3s server (master) goes down. The same can be followed if need be for any services apart from K8s/K3s apiserver as well.","title":"How-To - Deploy multi-server K3s HA with loxilb"},{"location":"k3s-multi-master/#guide-to-deploy-multi-master-ha-k3s-with-loxilb","text":"This document will explain how to install a multi-master HA K3s cluster with loxilb as a serviceLB provider running in-cluster mode. K3s is a lightweight Kubernetes distribution and is increasingly used for prototyping as well as for production workloads. K3s nodes are deployed as: 1) k3s-server nodes for k3s control plane components like apiserver and etcd. 2) k3s-agent nodes hosting user workloads/apps. When we deploy multi-master nodes, it is necessary that they be accessed from the k3s-agents in HA configuration and behind a load-balancer. Usually deploying such a load-balancer is outside the scope of kubernetes. In this guide, we will see how to deploy loxilb not only as cluster's serviceLB provider but also as a VIP-LB for accessing server/master node(s) services.","title":"Guide to deploy multi-master HA K3s with loxilb"},{"location":"k3s-multi-master/#topology","text":"For multi-master setup we need an odd number of server nodes to maintain quorum. So, we will have 3 k3s-server nodes for this setup. Overall, we will be deploying the components as per the following topology :","title":"Topology"},{"location":"k3s-multi-master/#k3s-installation-and-setup","text":"","title":"K3s installation and Setup"},{"location":"k3s-multi-master/#in-k3s-server1-node-","text":"$ curl -fL https://get.k3s.io | sh -s - server --node-ip=192.168.80.10 \\ --disable servicelb --disable traefik --cluster-init external-hostname=192.168.80.10 \\ --node-external-ip=192.168.80.80 --disable-cloud-controller It is to be noted that --node-external-ip=192.168.80.80 is used since we will utilize 192.168.80.80 as the VIP to access the multi-master setup from k3s-agents and other clients.","title":"In k3s-server1 node -"},{"location":"k3s-multi-master/#setup-the-node-for-loxilb","text":"sudo mkdir -p /etc/loxilb Create the following files in /etc/loxilb lbconfig.txt with following contents (change as per your requirement) { \"lbAttr\":[ { \"serviceArguments\":{ \"externalIP\":\"192.168.80.80\", \"port\":6443, \"protocol\":\"tcp\", \"sel\":0, \"mode\":2, \"BGP\":false, \"Monitor\":true, \"inactiveTimeOut\":240, \"block\":0 }, \"secondaryIPs\":null, \"endpoints\":[ { \"endpointIP\":\"192.168.80.10\", \"targetPort\":6443, \"weight\":1, \"state\":\"active\", \"counter\":\"\" }, { \"endpointIP\":\"192.168.80.11\", \"targetPort\":6443, \"weight\":1, \"state\":\"active\", \"counter\":\"\" }, { \"endpointIP\":\"192.168.80.12\", \"targetPort\":6443, \"weight\":1, \"state\":\"active\", \"counter\":\"\" } ] } ] } 2. EPconfig.txt with the following contents (change as per your requirement) { \"Attr\":[ { \"hostName\":\"192.168.80.10\", \"name\":\"192.168.80.10_tcp_6443\", \"inactiveReTries\":2, \"probeType\":\"tcp\", \"probeReq\":\"\", \"probeResp\":\"\", \"probeDuration\":10, \"probePort\":6443 }, { \"hostName\":\"192.168.80.11\", \"name\":\"192.168.80.11_tcp_6443\", \"inactiveReTries\":2, \"probeType\":\"tcp\", \"probeReq\":\"\", \"probeResp\":\"\", \"probeDuration\":10, \"probePort\":6443 }, { \"hostName\":\"192.168.80.12\", \"name\":\"192.168.80.12_tcp_6443\", \"inactiveReTries\":2, \"probeType\":\"tcp\", \"probeReq\":\"\", \"probeResp\":\"\", \"probeDuration\":10, \"probePort\":6443 } ] } The above serve as bootstrap LB rules for load-balancing into the k3s-server nodes as we will see later.","title":"Setup the node for loxilb :"},{"location":"k3s-multi-master/#in-k3s-server2-node-","text":"$ curl -fL https://get.k3s.io | K3S_TOKEN=${NODE_TOKEN} sh -s - server --server https://192.168.80.10:6443 \\ --disable traefik --disable servicelb --node-ip=192.168.80.11 \\ external-hostname=192.168.80.11 --node-external-ip=192.168.80.80 -t ${NODE_TOKEN} where NODE_TOKEN contain simply contents of /var/lib/rancher/k3s/server/node-token from server1. For example, it can be set using a command equivalent to the following : export NODE_TOKEN=$(cat node-token)","title":"In k3s-server2 node -"},{"location":"k3s-multi-master/#setup-the-node-for-loxilb_1","text":"Simply follow the steps as outlined for server1.","title":"Setup the node for loxilb:"},{"location":"k3s-multi-master/#in-k3s-server3-node-","text":"$ curl -fL https://get.k3s.io | K3S_TOKEN=${NODE_TOKEN} sh -s - server --server https://192.168.80.10:6443 \\ --disable traefik --disable servicelb --node-ip=192.168.80.12 \\ external-hostname=192.168.80.12 --node-external-ip=192.168.80.80 -t ${NODE_TOKEN} where NODE_TOKEN contain simply contents of /var/lib/rancher/k3s/server/node-token from server1. For example, it can be set using a command equivalent to the following : export NODE_TOKEN=$(cat node-token)","title":"In k3s-server3 node -"},{"location":"k3s-multi-master/#setup-the-node-for-loxilb_2","text":"First, follow the steps as outlined for server1. Additionally, we will have to start loxilb pod instances as follows : $ sudo kubectl apply -f - <<EOF apiVersion: apps/v1 kind: DaemonSet metadata: name: loxilb-lb namespace: kube-system spec: selector: matchLabels: app: loxilb-app template: metadata: name: loxilb-lb labels: app: loxilb-app spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet tolerations: - key: \"node-role.kubernetes.io/master\" operator: Exists - key: \"node-role.kubernetes.io/control-plane\" operator: Exists affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: \"node-role.kubernetes.io/master\" operator: Exists - key: \"node-role.kubernetes.io/control-plane\" operator: Exists volumes: - name: hllb hostPath: path: /etc/loxilb type: DirectoryOrCreate containers: - name: loxilb-app image: \"ghcr.io/loxilb-io/loxilb:latest\" imagePullPolicy: Always command: - /root/loxilb-io/loxilb/loxilb args: - --egr-hooks - --blacklist=cni[0-9a-z]|veth.|flannel. volumeMounts: - name: hllb mountPath: /etc/loxilb ports: - containerPort: 11111 - containerPort: 179 securityContext: privileged: true capabilities: add: - SYS_ADMIN --- apiVersion: v1 kind: Service metadata: name: loxilb-lb-service namespace: kube-system spec: clusterIP: None selector: app: loxilb-app ports: - name: loxilb-app port: 11111 targetPort: 11111 protocol: TCP EOF Kindly note that the args for loxilb might change depending on the scenario. This scenario considers loxilb running in-cluster mode. For service-proxy mode, please follow this yaml for exact args. Next, we will install loxilb's operator kube-loxilb as follows : $ sudo kubectl apply -f - <<EOF --- apiVersion: v1 kind: ServiceAccount metadata: name: kube-loxilb namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb rules: - apiGroups: - \"\" resources: - nodes verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - pods verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - endpoints - services - services/status verbs: - get - watch - list - patch - update - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - get - watch - list - apiGroups: - authentication.k8s.io resources: - tokenreviews verbs: - create - apiGroups: - authorization.k8s.io resources: - subjectaccessreviews verbs: - create --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-loxilb subjects: - kind: ServiceAccount name: kube-loxilb namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: kube-loxilb namespace: kube-system labels: app: loxilb spec: replicas: 1 selector: matchLabels: app: loxilb template: metadata: labels: app: loxilb spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet tolerations: - effect: NoSchedule operator: Exists # Mark the pod as a critical add-on for rescheduling. - key: CriticalAddonsOnly operator: Exists - effect: NoExecute operator: Exists - key: \"node-role.kubernetes.io/master\" operator: Exists - key: \"node-role.kubernetes.io/control-plane\" operator: Exists affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: \"node-role.kubernetes.io/master\" operator: Exists - key: \"node-role.kubernetes.io/control-plane\" operator: Exists priorityClassName: system-node-critical serviceAccountName: kube-loxilb terminationGracePeriodSeconds: 0 containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:latest imagePullPolicy: Always command: - /bin/kube-loxilb args: - --externalCIDR=192.168.80.200/32 - --setRoles=0.0.0.0 resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"] EOF At this point we can check the pods running in our kubernetes cluster (in server1, server2 & server3 at this point): $ sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-6799fbcd5-7jhcx 1/1 Running 0 3h15m kube-system kube-loxilb-5d99c445f7-j4x6k 1/1 Running 0 3h6m kube-system local-path-provisioner-6c86858495-pjn9j 1/1 Running 0 3h15m kube-system loxilb-lb-8bddf 1/1 Running 0 3h6m kube-system loxilb-lb-nsrr9 1/1 Running 0 3h6m kube-system loxilb-lb-fp2z6 1/1 Running 0 3h6m kube-system metrics-server-54fd9b65b-g5lfn 1/1 Running 0 3h15m","title":"Setup the node for loxilb:"},{"location":"k3s-multi-master/#in-k3s-agent1-node-","text":"The following steps need to be followed to install k3s in the agent nodes: $ curl -sfL https://get.k3s.io | K3S_TOKEN=${NODE_TOKEN} sh -s - agent --server https://192.168.80.80:6443 --node-ip=${WORKER_ADDR} --node-external-ip=${WORKER_ADDR} -t ${NODE_TOKEN} where WORKER_ADDR is the IP address of the agent node itself (in this case 192.168.80.101) and NODE_TOKEN has contents of /var/lib/rancher/k3s/server/node-token from server1. It is also to be noted that we use VIP - 192.168.80.80 provided by loxilb to access the server(master) K3s nodes and not the actual private node addresses. For rest of the agent nodes, we can follow the same set of steps as outlined above for k3s-agent1.","title":"In k3s-agent1 node -"},{"location":"k3s-multi-master/#validation","text":"After setting up all the k3s-server and k3s-agents, we should be able to see all nodes up and running $ sudo kubectl get nodes -A NAME STATUS ROLES AGE VERSION master1 Ready control-plane,etcd,master 4h v1.29.3+k3s1 master2 Ready control-plane,etcd,master 4h v1.29.3+k3s1 master3 Ready control-plane,etcd,master 4h v1.29.3+k3s1 worker1 Ready <none> 4h v1.29.3+k3s1 worker2 Ready <none> 4h v1.29.3+k3s1 worker3 Ready <none> 4h v1.29.3+k3s1 To verify, let's shutdown master1 k3s-server. ## Run shutdown the master1 node $ sudo shutdown -t now And try to access cluster information from other master nodes or worker nodes : $ sudo kubectl get nodes -A NAME STATUS ROLES AGE VERSION master1 NotReady control-plane,etcd,master 4h10m v1.29.3+k3s1 master2 Ready control-plane,etcd,master 4h10m v1.29.3+k3s1 master3 Ready control-plane,etcd,master 4h10m v1.29.3+k3s1 worker1 Ready <none> 4h10m v1.29.3+k3s1 worker2 Ready <none> 4h10m v1.29.3+k3s1 Also, we can confirm pods getting rescheduled to other \"ready\" nodes : $ sudo kubectl get pods -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system coredns-6799fbcd5-6dvm7 1/1 Running 0 27m 10.42.2.2 master3 <none> <none> kube-system coredns-6799fbcd5-mrjgt 1/1 Terminating 0 3h58m 10.42.0.4 master1 <none> <none> kube-system kube-loxilb-5d99c445f7-x7qd6 1/1 Running 0 3h58m 192.168.80.11 master2 <none> <none> kube-system local-path-provisioner-6c86858495-6f8rz 1/1 Terminating 0 3h58m 10.42.0.2 master1 <none> <none> kube-system local-path-provisioner-6c86858495-z2p6m 1/1 Running 0 27m 10.42.3.2 worker1 <none> <none> kube-system loxilb-lb-65jnz 1/1 Running 0 3h58m 192.168.80.10 master1 <none> <none> kube-system loxilb-lb-pfkf8 1/1 Running 0 3h58m 192.168.80.12 master3 <none> <none> kube-system loxilb-lb-xhr95 1/1 Running 0 3h58m 192.168.80.11 master2 <none> <none> kube-system metrics-server-54fd9b65b-l5pqz 1/1 Running 0 27m 10.42.4.2 worker2 <none> <none> kube-system metrics-server-54fd9b65b-x9bd7 1/1 Terminating 0 3h58m 10.42.0.3 master1 <none> <none> If the above set of command works fine in any of the \"ready\" nodes, it indicates that the api server is available even when one of k3s server (master) goes down. The same can be followed if need be for any services apart from K8s/K3s apiserver as well.","title":"Validation"},{"location":"k3s-rmq/","text":"Quick Start Guide with K3s and LoxiLB in-cluster \"service-proxy\" mode This document will explain how to install a K3s cluster with loxilb as a serviceLB provider running in-cluster \"service-proxy\" mode. What is service-proxy mode? service-proxy mode is where kubernetes cluster networking is entirely streamlined by loxilb for better performance. Looking at the left side of the image, you will notice the traffic flow of the packet as it enters the Kubernetes cluster. Kube-proxy, the de-facto networking agent in the Kubernetes which runs on each node of the cluster which monitors the services and translates them to either iptables or IPVS tangible rules. If we talk about the functionality or a cluster with low volume traffic then kube-proxy is fine but when it comes to scalability or a high volume traffic then it acts as a bottle-neck. loxilb \"service-proxy\" mode works with Flannel and kube-proxy in IPVS mode. It simplifies all the IPVS rules and injects them in it's in-kernel eBPF data-path. Traffic will reach at the interface, will be processed by eBPF and sent directly to the pod or to the other node, bypassing all the layers of Linux networking. This way, all the services, be it External, NodePort or ClusterIP, can be managed through LoxiLB. Topology For quickly bringing up loxilb in-cluster and K3s, we will be deploying a 4 nodes k3s cluster : loxilb and kube-loxilb components run as pods managed by kubernetes in this scenario. Setup K3s Configure Master node $ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik \\ --disable servicelb \\ --disable-cloud-controller \\ --kube-proxy-arg proxy-mode=ipvs \\ --flannel-iface=eth1 \\ --disable-network-policy \\ --node-ip=${MASTER_IP} \\ --node-external-ip=${MASTER_IP} \\ --bind-address=${MASTER_IP}\" sh - Configure Worker nodes $ curl -sfL https://get.k3s.io | K3S_URL=\"https://${MASTER_IP}:6443\"\\ K3S_TOKEN=\"${NODE_TOKEN}\" \\ INSTALL_K3S_EXEC=\"--node-ip=${WORKER_ADDR} \\ --node-external-ip=${WORKER_IP} \\ --kube-proxy-arg proxy-mode=ipvs \\ --flannel-iface=eth1\" sh - How to deploy kube-loxilb ? kube-loxilb is used as an operator to manage loxilb. wget https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/mesh/kube-loxilb.yml kube-loxilb.yaml args: #- --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setRoles=0.0.0.0 #- --monitor #- --setBGP In the above snippet, loxiURL is commented out which denotes to utilize in-cluster mode to discover loxilb pods automatically. External CIDR represents the IP pool from where serviceLB VIP will be allocated. Apply after making changes (if any) : sudo kubectl apply -f kube-loxilb.yaml How to deploy loxilb ? loxilb can be deloyed by using the following command in the K3s node sudo kubectl apply -f https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/mesh/loxilb-mesh.yml Seup RabbitMQ Operator sudo kubectl apply -f \"https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml\" Setup RabbitMQ application with loxilb wget https://raw.githubusercontent.com/rabbitmq/cluster-operator/main/docs/examples/hello-world/rabbitmq.yaml Change the following: apiVersion: rabbitmq.com/v1beta1 kind: RabbitmqCluster metadata: name: hello-world spec: replicas: 3 service: type: LoadBalancer override: service: spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local ports: - port: 5672 Create the service sudo kubectl apply -f rabbitmq.yaml Check the status In k3s node: ## Check the pods created $ sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-6c86858495-65tbc 1/1 Running 0 137m kube-system coredns-6799fbcd5-5h2dw 1/1 Running 0 137m kube-system metrics-server-67c658944b-mtv9q 1/1 Running 0 137m rabbitmq-system rabbitmq-cluster-operator-ccf488f4c-sphfm 1/1 Running 0 8m12s kube-system kube-loxilb-5fb5566999-4dj2v 1/1 Running 0 4m18s kube-system loxilb-lb-txtfm 1/1 Running 0 3m57s kube-system loxilb-lb-fnv97 1/1 Running 0 3m57s kube-system loxilb-lb-r7mks 1/1 Running 0 3m57s kube-system loxilb-lb-xxn29 1/1 Running 0 3m57s default hello-world-server-0 1/1 Running 0 72s default hello-world-server-1 1/1 Running 0 72s default hello-world-server-2 1/1 Running 0 72s ## Check the services created $ sudo kubectl get svc sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 136m hello-world-nodes ClusterIP None <none> 4369/TCP,25672/TCP 7s hello-world LoadBalancer 10.43.190.199 llb-192.168.82.100 15692:31224/TCP,5672:30817/TCP,15672:30698/TCP 7s In loxilb pod, we can check internal LB rules: $ sudo kubectl exec -it -n kube-system loxilb-lb-8l85d -- loxicmd get lb -o wide sudo kubectl exec -it loxilb-lb-txtfm -n kube-system -- loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |---------------|---------|-------|-------|------------------------------|------|-----|---------|----------------|-------|--------|--------|----------| | 10.0.2.15 | | 30698 | tcp | ipvs_10.0.2.15:30698-tcp | 0 | rr | fullnat | 10.42.1.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15672 | 1 | active | 0:0 | | 10.0.2.15 | | 30817 | tcp | ipvs_10.0.2.15:30817-tcp | 0 | rr | fullnat | 10.42.1.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 5672 | 1 | active | 0:0 | | 10.0.2.15 | | 31224 | tcp | ipvs_10.0.2.15:31224-tcp | 0 | rr | fullnat | 10.42.1.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15692 | 1 | active | 0:0 | | 10.42.0.0 | | 30698 | tcp | ipvs_10.42.0.0:30698-tcp | 0 | rr | fullnat | 10.42.1.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15672 | 1 | active | 0:0 | | 10.42.0.0 | | 30817 | tcp | ipvs_10.42.0.0:30817-tcp | 0 | rr | fullnat | 10.42.1.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 5672 | 1 | active | 0:0 | | 10.42.0.0 | | 31224 | tcp | ipvs_10.42.0.0:31224-tcp | 0 | rr | fullnat | 10.42.1.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15692 | 1 | active | 0:0 | | 10.42.0.1 | | 30698 | tcp | ipvs_10.42.0.1:30698-tcp | 0 | rr | fullnat | 10.42.1.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15672 | 1 | active | 0:0 | | 10.42.0.1 | | 30817 | tcp | ipvs_10.42.0.1:30817-tcp | 0 | rr | fullnat | 10.42.1.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 5672 | 1 | active | 0:0 | | 10.42.0.1 | | 31224 | tcp | ipvs_10.42.0.1:31224-tcp | 0 | rr | fullnat | 10.42.1.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15692 | 1 | active | 0:0 | | 10.43.0.10 | | 53 | tcp | ipvs_10.43.0.10:53-tcp | 0 | rr | default | 10.42.0.3 | 53 | 1 | - | 0:0 | | 10.43.0.10 | | 53 | udp | ipvs_10.43.0.10:53-udp | 0 | rr | default | 10.42.0.3 | 53 | 1 | - | 0:0 | | 10.43.0.10 | | 9153 | tcp | ipvs_10.43.0.10:9153-tcp | 0 | rr | default | 10.42.0.3 | 9153 | 1 | - | 0:0 | | 10.43.0.1 | | 443 | tcp | ipvs_10.43.0.1:443-tcp | 0 | rr | default | 192.168.80.10 | 6443 | 1 | - | 0:0 | | 10.43.190.199 | | 5672 | tcp | ipvs_10.43.190.199:5672-tcp | 0 | rr | default | 10.42.1.6 | 5672 | 1 | - | 0:0 | | | | | | | | | | 10.42.2.6 | 5672 | 1 | - | 0:0 | | | | | | | | | | 10.42.3.7 | 5672 | 1 | - | 0:0 | | 10.43.190.199 | | 15672 | tcp | ipvs_10.43.190.199:15672-tcp | 0 | rr | default | 10.42.1.6 | 15672 | 1 | - | 0:0 | | | | | | | | | | 10.42.2.6 | 15672 | 1 | - | 0:0 | | | | | | | | | | 10.42.3.7 | 15672 | 1 | - | 0:0 | | 10.43.190.199 | | 15692 | tcp | ipvs_10.43.190.199:15692-tcp | 0 | rr | default | 10.42.1.6 | 15692 | 1 | - | 0:0 | | | | | | | | | | 10.42.2.6 | 15692 | 1 | - | 0:0 | | | | | | | | | | 10.42.3.7 | 15692 | 1 | - | 0:0 | | 10.43.5.58 | | 443 | tcp | ipvs_10.43.5.58:443-tcp | 0 | rr | default | 10.42.0.4 | 10250 | 1 | - | 0:0 | | 192.168.80.10 | | 30698 | tcp | ipvs_192.168.80.10:30698-tcp | 0 | rr | fullnat | 10.42.1.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15672 | 1 | active | 0:0 | | 192.168.80.10 | | 30817 | tcp | ipvs_192.168.80.10:30817-tcp | 0 | rr | fullnat | 10.42.1.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 5672 | 1 | active | 0:0 | | 192.168.80.10 | | 31224 | tcp | ipvs_192.168.80.10:31224-tcp | 0 | rr | fullnat | 10.42.1.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15692 | 1 | active | 0:0 | | 192.168.80.20 | | 5672 | tcp | default_hello-world | 0 | rr | onearm | 192.168.80.101 | 30817 | 1 | - | 0:0 | | | | | | | | | | 192.168.80.102 | 30817 | 1 | - | 0:0 | | | | | | | | | | 192.168.80.103 | 30817 | 1 | - | 0:0 | | 192.168.80.20 | | 15672 | tcp | default_hello-world | 0 | rr | onearm | 192.168.80.101 | 30698 | 1 | - | 0:0 | | | | | | | | | | 192.168.80.102 | 30698 | 1 | - | 0:0 | | | | | | | | | | 192.168.80.103 | 30698 | 1 | - | 0:0 | | 192.168.80.20 | | 15692 | tcp | default_hello-world | 0 | rr | onearm | 192.168.80.101 | 31224 | 1 | - | 0:0 | | | | | | | | | | 192.168.80.102 | 31224 | 1 | - | 0:0 | | | | | | | | | | 192.168.80.103 | 31224 | 1 | - | 0:0 | | 192.168.80.20 | | 30698 | tcp | ipvs_192.168.80.20:30698-tcp | 0 | rr | fullnat | 10.42.1.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15672 | 1 | active | 0:0 | | 192.168.80.20 | | 30817 | tcp | ipvs_192.168.80.20:30817-tcp | 0 | rr | fullnat | 10.42.1.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 5672 | 1 | active | 0:0 | | 192.168.80.20 | | 31224 | tcp | ipvs_192.168.80.20:31224-tcp | 0 | rr | fullnat | 10.42.1.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15692 | 1 | active | 0:0 | Get RabbitMQ Credentials username=\"$(sudo kubectl get secret hello-world-default-user -o jsonpath='{.data.username}' | base64 --decode)\" password=\"$(sudo kubectl get secret hello-world-default-user -o jsonpath='{.data.password}' | base64 --decode)\" Test RabbitMQ from host/client sudo docker run -it --rm --net=host pivotalrabbitmq/perf-test:latest --uri amqp://$username:$password@192.168.80.20:5672 -x 10 -y 10 -u \"throughput-test-4\" -a --id \"test 4\" -z100 For more detailed performance comparison with other solutions, kindly follow this blog and for more detailed information on incluster deployment of loxilb with bgp in a full-blown cluster, kindly follow this blog .","title":"K3s rmq"},{"location":"k3s-rmq/#quick-start-guide-with-k3s-and-loxilb-in-cluster-service-proxy-mode","text":"This document will explain how to install a K3s cluster with loxilb as a serviceLB provider running in-cluster \"service-proxy\" mode.","title":"Quick Start Guide with K3s and LoxiLB in-cluster \"service-proxy\" mode"},{"location":"k3s-rmq/#what-is-service-proxy-mode","text":"service-proxy mode is where kubernetes cluster networking is entirely streamlined by loxilb for better performance. Looking at the left side of the image, you will notice the traffic flow of the packet as it enters the Kubernetes cluster. Kube-proxy, the de-facto networking agent in the Kubernetes which runs on each node of the cluster which monitors the services and translates them to either iptables or IPVS tangible rules. If we talk about the functionality or a cluster with low volume traffic then kube-proxy is fine but when it comes to scalability or a high volume traffic then it acts as a bottle-neck. loxilb \"service-proxy\" mode works with Flannel and kube-proxy in IPVS mode. It simplifies all the IPVS rules and injects them in it's in-kernel eBPF data-path. Traffic will reach at the interface, will be processed by eBPF and sent directly to the pod or to the other node, bypassing all the layers of Linux networking. This way, all the services, be it External, NodePort or ClusterIP, can be managed through LoxiLB.","title":"What is service-proxy mode?"},{"location":"k3s-rmq/#topology","text":"For quickly bringing up loxilb in-cluster and K3s, we will be deploying a 4 nodes k3s cluster : loxilb and kube-loxilb components run as pods managed by kubernetes in this scenario.","title":"Topology"},{"location":"k3s-rmq/#setup-k3s","text":"","title":"Setup K3s"},{"location":"k3s-rmq/#configure-master-node","text":"$ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik \\ --disable servicelb \\ --disable-cloud-controller \\ --kube-proxy-arg proxy-mode=ipvs \\ --flannel-iface=eth1 \\ --disable-network-policy \\ --node-ip=${MASTER_IP} \\ --node-external-ip=${MASTER_IP} \\ --bind-address=${MASTER_IP}\" sh -","title":"Configure Master node"},{"location":"k3s-rmq/#configure-worker-nodes","text":"$ curl -sfL https://get.k3s.io | K3S_URL=\"https://${MASTER_IP}:6443\"\\ K3S_TOKEN=\"${NODE_TOKEN}\" \\ INSTALL_K3S_EXEC=\"--node-ip=${WORKER_ADDR} \\ --node-external-ip=${WORKER_IP} \\ --kube-proxy-arg proxy-mode=ipvs \\ --flannel-iface=eth1\" sh -","title":"Configure Worker nodes"},{"location":"k3s-rmq/#how-to-deploy-kube-loxilb","text":"kube-loxilb is used as an operator to manage loxilb. wget https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/mesh/kube-loxilb.yml kube-loxilb.yaml args: #- --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setRoles=0.0.0.0 #- --monitor #- --setBGP In the above snippet, loxiURL is commented out which denotes to utilize in-cluster mode to discover loxilb pods automatically. External CIDR represents the IP pool from where serviceLB VIP will be allocated. Apply after making changes (if any) : sudo kubectl apply -f kube-loxilb.yaml","title":"How to deploy kube-loxilb ?"},{"location":"k3s-rmq/#how-to-deploy-loxilb","text":"loxilb can be deloyed by using the following command in the K3s node sudo kubectl apply -f https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/mesh/loxilb-mesh.yml","title":"How to deploy loxilb ?"},{"location":"k3s-rmq/#seup-rabbitmq-operator","text":"sudo kubectl apply -f \"https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml\"","title":"Seup RabbitMQ Operator"},{"location":"k3s-rmq/#setup-rabbitmq-application-with-loxilb","text":"wget https://raw.githubusercontent.com/rabbitmq/cluster-operator/main/docs/examples/hello-world/rabbitmq.yaml Change the following: apiVersion: rabbitmq.com/v1beta1 kind: RabbitmqCluster metadata: name: hello-world spec: replicas: 3 service: type: LoadBalancer override: service: spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local ports: - port: 5672","title":"Setup RabbitMQ application with loxilb"},{"location":"k3s-rmq/#create-the-service","text":"sudo kubectl apply -f rabbitmq.yaml","title":"Create the service"},{"location":"k3s-rmq/#check-the-status","text":"In k3s node: ## Check the pods created $ sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-6c86858495-65tbc 1/1 Running 0 137m kube-system coredns-6799fbcd5-5h2dw 1/1 Running 0 137m kube-system metrics-server-67c658944b-mtv9q 1/1 Running 0 137m rabbitmq-system rabbitmq-cluster-operator-ccf488f4c-sphfm 1/1 Running 0 8m12s kube-system kube-loxilb-5fb5566999-4dj2v 1/1 Running 0 4m18s kube-system loxilb-lb-txtfm 1/1 Running 0 3m57s kube-system loxilb-lb-fnv97 1/1 Running 0 3m57s kube-system loxilb-lb-r7mks 1/1 Running 0 3m57s kube-system loxilb-lb-xxn29 1/1 Running 0 3m57s default hello-world-server-0 1/1 Running 0 72s default hello-world-server-1 1/1 Running 0 72s default hello-world-server-2 1/1 Running 0 72s ## Check the services created $ sudo kubectl get svc sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 136m hello-world-nodes ClusterIP None <none> 4369/TCP,25672/TCP 7s hello-world LoadBalancer 10.43.190.199 llb-192.168.82.100 15692:31224/TCP,5672:30817/TCP,15672:30698/TCP 7s In loxilb pod, we can check internal LB rules: $ sudo kubectl exec -it -n kube-system loxilb-lb-8l85d -- loxicmd get lb -o wide sudo kubectl exec -it loxilb-lb-txtfm -n kube-system -- loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |---------------|---------|-------|-------|------------------------------|------|-----|---------|----------------|-------|--------|--------|----------| | 10.0.2.15 | | 30698 | tcp | ipvs_10.0.2.15:30698-tcp | 0 | rr | fullnat | 10.42.1.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15672 | 1 | active | 0:0 | | 10.0.2.15 | | 30817 | tcp | ipvs_10.0.2.15:30817-tcp | 0 | rr | fullnat | 10.42.1.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 5672 | 1 | active | 0:0 | | 10.0.2.15 | | 31224 | tcp | ipvs_10.0.2.15:31224-tcp | 0 | rr | fullnat | 10.42.1.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15692 | 1 | active | 0:0 | | 10.42.0.0 | | 30698 | tcp | ipvs_10.42.0.0:30698-tcp | 0 | rr | fullnat | 10.42.1.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15672 | 1 | active | 0:0 | | 10.42.0.0 | | 30817 | tcp | ipvs_10.42.0.0:30817-tcp | 0 | rr | fullnat | 10.42.1.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 5672 | 1 | active | 0:0 | | 10.42.0.0 | | 31224 | tcp | ipvs_10.42.0.0:31224-tcp | 0 | rr | fullnat | 10.42.1.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15692 | 1 | active | 0:0 | | 10.42.0.1 | | 30698 | tcp | ipvs_10.42.0.1:30698-tcp | 0 | rr | fullnat | 10.42.1.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15672 | 1 | active | 0:0 | | 10.42.0.1 | | 30817 | tcp | ipvs_10.42.0.1:30817-tcp | 0 | rr | fullnat | 10.42.1.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 5672 | 1 | active | 0:0 | | 10.42.0.1 | | 31224 | tcp | ipvs_10.42.0.1:31224-tcp | 0 | rr | fullnat | 10.42.1.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15692 | 1 | active | 0:0 | | 10.43.0.10 | | 53 | tcp | ipvs_10.43.0.10:53-tcp | 0 | rr | default | 10.42.0.3 | 53 | 1 | - | 0:0 | | 10.43.0.10 | | 53 | udp | ipvs_10.43.0.10:53-udp | 0 | rr | default | 10.42.0.3 | 53 | 1 | - | 0:0 | | 10.43.0.10 | | 9153 | tcp | ipvs_10.43.0.10:9153-tcp | 0 | rr | default | 10.42.0.3 | 9153 | 1 | - | 0:0 | | 10.43.0.1 | | 443 | tcp | ipvs_10.43.0.1:443-tcp | 0 | rr | default | 192.168.80.10 | 6443 | 1 | - | 0:0 | | 10.43.190.199 | | 5672 | tcp | ipvs_10.43.190.199:5672-tcp | 0 | rr | default | 10.42.1.6 | 5672 | 1 | - | 0:0 | | | | | | | | | | 10.42.2.6 | 5672 | 1 | - | 0:0 | | | | | | | | | | 10.42.3.7 | 5672 | 1 | - | 0:0 | | 10.43.190.199 | | 15672 | tcp | ipvs_10.43.190.199:15672-tcp | 0 | rr | default | 10.42.1.6 | 15672 | 1 | - | 0:0 | | | | | | | | | | 10.42.2.6 | 15672 | 1 | - | 0:0 | | | | | | | | | | 10.42.3.7 | 15672 | 1 | - | 0:0 | | 10.43.190.199 | | 15692 | tcp | ipvs_10.43.190.199:15692-tcp | 0 | rr | default | 10.42.1.6 | 15692 | 1 | - | 0:0 | | | | | | | | | | 10.42.2.6 | 15692 | 1 | - | 0:0 | | | | | | | | | | 10.42.3.7 | 15692 | 1 | - | 0:0 | | 10.43.5.58 | | 443 | tcp | ipvs_10.43.5.58:443-tcp | 0 | rr | default | 10.42.0.4 | 10250 | 1 | - | 0:0 | | 192.168.80.10 | | 30698 | tcp | ipvs_192.168.80.10:30698-tcp | 0 | rr | fullnat | 10.42.1.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15672 | 1 | active | 0:0 | | 192.168.80.10 | | 30817 | tcp | ipvs_192.168.80.10:30817-tcp | 0 | rr | fullnat | 10.42.1.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 5672 | 1 | active | 0:0 | | 192.168.80.10 | | 31224 | tcp | ipvs_192.168.80.10:31224-tcp | 0 | rr | fullnat | 10.42.1.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15692 | 1 | active | 0:0 | | 192.168.80.20 | | 5672 | tcp | default_hello-world | 0 | rr | onearm | 192.168.80.101 | 30817 | 1 | - | 0:0 | | | | | | | | | | 192.168.80.102 | 30817 | 1 | - | 0:0 | | | | | | | | | | 192.168.80.103 | 30817 | 1 | - | 0:0 | | 192.168.80.20 | | 15672 | tcp | default_hello-world | 0 | rr | onearm | 192.168.80.101 | 30698 | 1 | - | 0:0 | | | | | | | | | | 192.168.80.102 | 30698 | 1 | - | 0:0 | | | | | | | | | | 192.168.80.103 | 30698 | 1 | - | 0:0 | | 192.168.80.20 | | 15692 | tcp | default_hello-world | 0 | rr | onearm | 192.168.80.101 | 31224 | 1 | - | 0:0 | | | | | | | | | | 192.168.80.102 | 31224 | 1 | - | 0:0 | | | | | | | | | | 192.168.80.103 | 31224 | 1 | - | 0:0 | | 192.168.80.20 | | 30698 | tcp | ipvs_192.168.80.20:30698-tcp | 0 | rr | fullnat | 10.42.1.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15672 | 1 | active | 0:0 | | 192.168.80.20 | | 30817 | tcp | ipvs_192.168.80.20:30817-tcp | 0 | rr | fullnat | 10.42.1.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 5672 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 5672 | 1 | active | 0:0 | | 192.168.80.20 | | 31224 | tcp | ipvs_192.168.80.20:31224-tcp | 0 | rr | fullnat | 10.42.1.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.2.6 | 15692 | 1 | active | 0:0 | | | | | | | | | | 10.42.3.7 | 15692 | 1 | active | 0:0 |","title":"Check the status"},{"location":"k3s-rmq/#get-rabbitmq-credentials","text":"username=\"$(sudo kubectl get secret hello-world-default-user -o jsonpath='{.data.username}' | base64 --decode)\" password=\"$(sudo kubectl get secret hello-world-default-user -o jsonpath='{.data.password}' | base64 --decode)\"","title":"Get RabbitMQ Credentials"},{"location":"k3s-rmq/#test-rabbitmq-from-hostclient","text":"sudo docker run -it --rm --net=host pivotalrabbitmq/perf-test:latest --uri amqp://$username:$password@192.168.80.20:5672 -x 10 -y 10 -u \"throughput-test-4\" -a --id \"test 4\" -z100 For more detailed performance comparison with other solutions, kindly follow this blog and for more detailed information on incluster deployment of loxilb with bgp in a full-blown cluster, kindly follow this blog .","title":"Test RabbitMQ from host/client"},{"location":"k3s_quick_start_calico/","text":"LoxiLB Quick Start Guide with Calico This guide will explain how to: Deploy a single-node K3s cluster with calico networking Expose services with loxilb as an external load balancer Pre-requisite Single node with Linux Install docker runtime to manage loxilb Topology For quickly bringing up loxilb with K3s and calico, we will be deploying all components in a single node : loxilb is run as a docker and will use macvlan for the incoming traffic. This is to mimic a topology close to cloud-hosted k8s where LB nodes run outside a cluster. loxilb can be used in more complex in-cluster mode as well, but not used here for simplicity. Install loxilb docker ## Set underlying interface of the VM/cluster-node to promisc mode for mac-vlan to work sudo ifconfig eth1 promisc ## Run loxilb sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged --entrypoint /root/loxilb-io/loxilb/loxilb -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest # Create mac-vlan on top of underlying eth1 interface sudo docker network create -d macvlan -o parent=eth1 --subnet 192.168.82.0/24 --gateway 192.168.82.1 --aux-address 'host=192.168.82.252' llbnet # Assign mac-vlan to loxilb docker with specified IP (which will be used as LB VIP) # Please note that this node should already have an IP assigned belonging to the same subnet on underlying interface sudo docker network connect llbnet loxilb --ip=192.168.82.100 # Add iptables rule to allow traffic from source/host IP(192.168.82.1) to loxilb sudo iptables -A DOCKER -s 192.168.82.1 -j ACCEPT All the above steps related to docker setup can be further automated using docker-compose. Setup K3s with Calico # Install IPVS sudo apt-get -y install ipset ipvsadm # Install K3s with Calico and kube-proxy in IPVS mode curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik,metrics-server,servicelb --disable-cloud-controller --kubelet-arg cloud-provider=external --flannel-backend=none --disable-network-policy\" K3S_KUBECONFIG_MODE=\"644\" sh -s - server --kube-proxy-arg proxy-mode=ipvs # Install Calico kubectl $KUBECONFIG create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/tigera-operator.yaml kubectl $KUBECONFIG create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/custom-resources.yaml # Remove taints in k3s if any (usually happens if started without cloud-manager) sudo kubectl taint nodes --all node.cloudprovider.kubernetes.io/uninitialized=false:NoSchedule- How to deploy kube-loxilb ? kube-loxilb is used to deploy loxilb with Kubernetes. wget https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/ext-cluster/kube-loxilb.yaml kube-loxilb.yaml args: - --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setMode=1 In the above snippet, loxiURL uses docker interface IP of loxilb, which can be different for each setup. Apply in k8s: kubectl apply -f kube-loxilb.yaml Create the service kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/docker-k3s-cilium/tcp-svc-lb.yml Check the status In k3s: kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 2m48s tcp-lb-onearm LoadBalancer 10.43.183.123 llb-192.168.82.100 56002:30001/TCP 30s In loxilb docker: $ sudo docker exec -it loxilb loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-----------------------|------|-----|--------|-----------|-------|--------|--------|----------| | 192.168.82.100 | | 56002 | tcp | default_tcp-lb-onearm | 0 | rr | onearm | 10.0.2.15 | 30001 | 1 | active | 0:0 | Connect from host/client $ curl http://192.168.82.100:56002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> All of the above steps are also available as part of loxilb CICD workflow. Follow the steps below to replicate the above: $ cd cicd/docker-k3s-calico/ # To setup the single node k3s setup with calico as CNI and loxilb as external load balancer $ ./config.sh # To validate the results $ ./validation.sh # Cleanup $ ./rmconfig.sh","title":"ext-cluster pod - K3s/loxilb with calico"},{"location":"k3s_quick_start_calico/#loxilb-quick-start-guide-with-calico","text":"This guide will explain how to: Deploy a single-node K3s cluster with calico networking Expose services with loxilb as an external load balancer","title":"LoxiLB Quick Start Guide with Calico"},{"location":"k3s_quick_start_calico/#pre-requisite","text":"Single node with Linux Install docker runtime to manage loxilb","title":"Pre-requisite"},{"location":"k3s_quick_start_calico/#topology","text":"For quickly bringing up loxilb with K3s and calico, we will be deploying all components in a single node : loxilb is run as a docker and will use macvlan for the incoming traffic. This is to mimic a topology close to cloud-hosted k8s where LB nodes run outside a cluster. loxilb can be used in more complex in-cluster mode as well, but not used here for simplicity.","title":"Topology"},{"location":"k3s_quick_start_calico/#install-loxilb-docker","text":"## Set underlying interface of the VM/cluster-node to promisc mode for mac-vlan to work sudo ifconfig eth1 promisc ## Run loxilb sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged --entrypoint /root/loxilb-io/loxilb/loxilb -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest # Create mac-vlan on top of underlying eth1 interface sudo docker network create -d macvlan -o parent=eth1 --subnet 192.168.82.0/24 --gateway 192.168.82.1 --aux-address 'host=192.168.82.252' llbnet # Assign mac-vlan to loxilb docker with specified IP (which will be used as LB VIP) # Please note that this node should already have an IP assigned belonging to the same subnet on underlying interface sudo docker network connect llbnet loxilb --ip=192.168.82.100 # Add iptables rule to allow traffic from source/host IP(192.168.82.1) to loxilb sudo iptables -A DOCKER -s 192.168.82.1 -j ACCEPT All the above steps related to docker setup can be further automated using docker-compose.","title":"Install loxilb docker"},{"location":"k3s_quick_start_calico/#setup-k3s-with-calico","text":"# Install IPVS sudo apt-get -y install ipset ipvsadm # Install K3s with Calico and kube-proxy in IPVS mode curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik,metrics-server,servicelb --disable-cloud-controller --kubelet-arg cloud-provider=external --flannel-backend=none --disable-network-policy\" K3S_KUBECONFIG_MODE=\"644\" sh -s - server --kube-proxy-arg proxy-mode=ipvs # Install Calico kubectl $KUBECONFIG create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/tigera-operator.yaml kubectl $KUBECONFIG create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/custom-resources.yaml # Remove taints in k3s if any (usually happens if started without cloud-manager) sudo kubectl taint nodes --all node.cloudprovider.kubernetes.io/uninitialized=false:NoSchedule-","title":"Setup K3s with Calico"},{"location":"k3s_quick_start_calico/#how-to-deploy-kube-loxilb","text":"kube-loxilb is used to deploy loxilb with Kubernetes. wget https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/ext-cluster/kube-loxilb.yaml kube-loxilb.yaml args: - --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setMode=1 In the above snippet, loxiURL uses docker interface IP of loxilb, which can be different for each setup. Apply in k8s: kubectl apply -f kube-loxilb.yaml","title":"How to deploy kube-loxilb ?"},{"location":"k3s_quick_start_calico/#create-the-service","text":"kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/docker-k3s-cilium/tcp-svc-lb.yml","title":"Create the service"},{"location":"k3s_quick_start_calico/#check-the-status","text":"In k3s: kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 2m48s tcp-lb-onearm LoadBalancer 10.43.183.123 llb-192.168.82.100 56002:30001/TCP 30s In loxilb docker: $ sudo docker exec -it loxilb loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-----------------------|------|-----|--------|-----------|-------|--------|--------|----------| | 192.168.82.100 | | 56002 | tcp | default_tcp-lb-onearm | 0 | rr | onearm | 10.0.2.15 | 30001 | 1 | active | 0:0 |","title":"Check the status"},{"location":"k3s_quick_start_calico/#connect-from-hostclient","text":"$ curl http://192.168.82.100:56002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> All of the above steps are also available as part of loxilb CICD workflow. Follow the steps below to replicate the above: $ cd cicd/docker-k3s-calico/ # To setup the single node k3s setup with calico as CNI and loxilb as external load balancer $ ./config.sh # To validate the results $ ./validation.sh # Cleanup $ ./rmconfig.sh","title":"Connect from host/client"},{"location":"k3s_quick_start_flannel/","text":"LoxiLB Quick Start Guide with K3s/Flannel This guide will explain how to: Deploy a single-node K3s cluster with flannel networking Expose services with loxilb as an external load balancer Pre-requisite Single node with Linux Install docker runtime to manage loxilb Topology For quickly bringing up loxilb with K3s/Flannel, we will be deploying all components in a single node : loxilb is run as a docker and will use macvlan for the incoming traffic. This is to mimic a topology close to cloud-hosted k8s where LB nodes run outside a cluster. loxilb can be used in more complex in-cluster mode as well, but not used here for simplicity. Install loxilb docker ## Set underlying interface of the VM/cluster-node to promisc mode for mac-vlan to work sudo ifconfig eth1 promisc ## Run loxilb sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged --entrypoint /root/loxilb-io/loxilb/loxilb -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest # Create mac-vlan on top of underlying eth1 interface sudo docker network create -d macvlan -o parent=eth1 --subnet 192.168.82.0/24 --gateway 192.168.82.1 --aux-address 'host=192.168.82.252' llbnet # Assign mac-vlan to loxilb docker with specified IP (which will be used as LB VIP) # Please note that this node should already have an IP assigned belonging to the same subnet on underlying interface sudo docker network connect llbnet loxilb --ip=192.168.82.100 # Add iptables rule to allow traffic from source/host IP(192.168.82.1) to loxilb sudo iptables -A DOCKER -s 192.168.82.1 -j ACCEPT All the above steps related to docker setup can be further automated using docker-compose. Setup K3s/Flannel #K3s installation curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"server --disable traefik --disable servicelb --disable-cloud-controller --kube-proxy-arg metrics-bind-address=0.0.0.0 --kubelet-arg cloud-provider=external\" K3S_KUBECONFIG_MODE=\"644\" sh - # Remove taints in k3s if any (usually happens if started without cloud-manager) sudo kubectl taint nodes --all node.cloudprovider.kubernetes.io/uninitialized=false:NoSchedule- How to deploy kube-loxilb ? kube-loxilb is used to deploy loxilb with Kubernetes. wget https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/ext-cluster/kube-loxilb.yaml kube-loxilb.yaml args: - --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setMode=1 In the above snippet, loxiURL uses docker interface IP of loxilb, which can be different for each setup. Apply in k8s: kubectl apply -f kube-loxilb.yaml Create the service kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/docker-k3s-cilium/tcp-svc-lb.yml Check the status In k3s: kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 80m tcp-lb-onearm LoadBalancer 10.43.183.123 llb-192.168.82.100 56002:30001/TCP 6m50s In loxilb docker: $ sudo docker exec -it loxilb loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-----------------------|------|-----|--------|-----------|-------|--------|--------|----------| | 192.168.82.100 | | 56002 | tcp | default_tcp-lb-onearm | 0 | rr | onearm | 10.0.2.15 | 30001 | 1 | active | 12:880 | Connect from host/client $ curl http://192.168.82.100:56002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html>","title":"ext-cluster pod - K3s/loxilb with default flannel"},{"location":"k3s_quick_start_flannel/#loxilb-quick-start-guide-with-k3sflannel","text":"This guide will explain how to: Deploy a single-node K3s cluster with flannel networking Expose services with loxilb as an external load balancer","title":"LoxiLB Quick Start Guide with K3s/Flannel"},{"location":"k3s_quick_start_flannel/#pre-requisite","text":"Single node with Linux Install docker runtime to manage loxilb","title":"Pre-requisite"},{"location":"k3s_quick_start_flannel/#topology","text":"For quickly bringing up loxilb with K3s/Flannel, we will be deploying all components in a single node : loxilb is run as a docker and will use macvlan for the incoming traffic. This is to mimic a topology close to cloud-hosted k8s where LB nodes run outside a cluster. loxilb can be used in more complex in-cluster mode as well, but not used here for simplicity.","title":"Topology"},{"location":"k3s_quick_start_flannel/#install-loxilb-docker","text":"## Set underlying interface of the VM/cluster-node to promisc mode for mac-vlan to work sudo ifconfig eth1 promisc ## Run loxilb sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged --entrypoint /root/loxilb-io/loxilb/loxilb -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest # Create mac-vlan on top of underlying eth1 interface sudo docker network create -d macvlan -o parent=eth1 --subnet 192.168.82.0/24 --gateway 192.168.82.1 --aux-address 'host=192.168.82.252' llbnet # Assign mac-vlan to loxilb docker with specified IP (which will be used as LB VIP) # Please note that this node should already have an IP assigned belonging to the same subnet on underlying interface sudo docker network connect llbnet loxilb --ip=192.168.82.100 # Add iptables rule to allow traffic from source/host IP(192.168.82.1) to loxilb sudo iptables -A DOCKER -s 192.168.82.1 -j ACCEPT All the above steps related to docker setup can be further automated using docker-compose.","title":"Install loxilb docker"},{"location":"k3s_quick_start_flannel/#setup-k3sflannel","text":"#K3s installation curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"server --disable traefik --disable servicelb --disable-cloud-controller --kube-proxy-arg metrics-bind-address=0.0.0.0 --kubelet-arg cloud-provider=external\" K3S_KUBECONFIG_MODE=\"644\" sh - # Remove taints in k3s if any (usually happens if started without cloud-manager) sudo kubectl taint nodes --all node.cloudprovider.kubernetes.io/uninitialized=false:NoSchedule-","title":"Setup K3s/Flannel"},{"location":"k3s_quick_start_flannel/#how-to-deploy-kube-loxilb","text":"kube-loxilb is used to deploy loxilb with Kubernetes. wget https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/ext-cluster/kube-loxilb.yaml kube-loxilb.yaml args: - --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setMode=1 In the above snippet, loxiURL uses docker interface IP of loxilb, which can be different for each setup. Apply in k8s: kubectl apply -f kube-loxilb.yaml","title":"How to deploy kube-loxilb ?"},{"location":"k3s_quick_start_flannel/#create-the-service","text":"kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/docker-k3s-cilium/tcp-svc-lb.yml","title":"Create the service"},{"location":"k3s_quick_start_flannel/#check-the-status","text":"In k3s: kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 80m tcp-lb-onearm LoadBalancer 10.43.183.123 llb-192.168.82.100 56002:30001/TCP 6m50s In loxilb docker: $ sudo docker exec -it loxilb loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-----------------------|------|-----|--------|-----------|-------|--------|--------|----------| | 192.168.82.100 | | 56002 | tcp | default_tcp-lb-onearm | 0 | rr | onearm | 10.0.2.15 | 30001 | 1 | active | 12:880 |","title":"Check the status"},{"location":"k3s_quick_start_flannel/#connect-from-hostclient","text":"$ curl http://192.168.82.100:56002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html>","title":"Connect from host/client"},{"location":"k3s_quick_start_incluster/","text":"Quick Start Guide with K3s and LoxiLB in-cluster mode This document will explain how to install a K3s cluster with loxilb as a serviceLB provider running in-cluster mode. Topology For quickly bringing up loxilb in-cluster and K3s, we will be deploying all components in a single node : loxilb and kube-loxilb components run as pods managed by kubernetes in this scenario. Setup K3s # K3s installation $ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"server --disable traefik --disable servicelb --disable-cloud-controller --kube-proxy-arg metrics-bind-address=0.0.0.0 --kubelet-arg cloud-provider=external\" K3S_KUBECONFIG_MODE=\"644\" sh - # Remove taints in k3s if any (usually happens if started without cloud-manager) $ sudo kubectl taint nodes --all node.cloudprovider.kubernetes.io/uninitialized=false:NoSchedule- How to deploy loxilb ? loxilb can be deloyed by using the following command in the K3s node sudo kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/k3s-incluster/loxilb.yml How to deploy kube-loxilb ? kube-loxilb is used as an operator to manage loxilb. wget https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/k3s-incluster/kube-loxilb.yml kube-loxilb.yaml args: #- --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setRoles=0.0.0.0 #- --monitor #- --setBGP In the above snippet, loxiURL is commented out which denotes to utilize in-cluster mode to discover loxilb pods automatically. External CIDR represents the IP pool from where serviceLB VIP will be allocated. Apply after making changes (if any) : sudo kubectl apply -f kube-loxilb.yaml Create the service sudo kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/k3s-incluster/tcp-svc-lb.yml Check the status In k3s node: ## Check the pods created $ sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-6c86858495-snvcm 1/1 Running 0 4m37s kube-system coredns-6799fbcd5-cpj6x 1/1 Running 0 4m37s kube-system metrics-server-67c658944b-42ptz 1/1 Running 0 4m37s kube-system loxilb-lb-8l85d 1/1 Running 0 3m40s kube-system kube-loxilb-6f44cdcdf5-5fdtl 1/1 Running 0 2m19s default tcp-onearm-test 1/1 Running 0 88s ## Check the services created $ sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 5m12s tcp-lb-onearm LoadBalancer 10.43.47.60 llb-192.168.82.100 56002:30001/TCP 108s In loxilb pod, we can check internal LB rules: $ sudo kubectl exec -it -n kube-system loxilb-lb-8l85d -- loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-----------------------|------|-----|--------|-----------|-------|--------|--------|----------| | 192.168.82.100 | | 56002 | tcp | default_tcp-lb-onearm | 0 | rr | onearm | 10.0.2.15 | 30001 | 1 | active | 39:2874 | Connect from host/client $ curl http://192.168.82.100:56002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> For more detailed information on incluster deployment of loxilb with bgp in a full-blown cluster, kindly follow this blog .","title":"in-cluster pod - K3s/loxilb in-cluster mode"},{"location":"k3s_quick_start_incluster/#quick-start-guide-with-k3s-and-loxilb-in-cluster-mode","text":"This document will explain how to install a K3s cluster with loxilb as a serviceLB provider running in-cluster mode.","title":"Quick Start Guide with K3s and LoxiLB in-cluster mode"},{"location":"k3s_quick_start_incluster/#topology","text":"For quickly bringing up loxilb in-cluster and K3s, we will be deploying all components in a single node : loxilb and kube-loxilb components run as pods managed by kubernetes in this scenario.","title":"Topology"},{"location":"k3s_quick_start_incluster/#setup-k3s","text":"# K3s installation $ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"server --disable traefik --disable servicelb --disable-cloud-controller --kube-proxy-arg metrics-bind-address=0.0.0.0 --kubelet-arg cloud-provider=external\" K3S_KUBECONFIG_MODE=\"644\" sh - # Remove taints in k3s if any (usually happens if started without cloud-manager) $ sudo kubectl taint nodes --all node.cloudprovider.kubernetes.io/uninitialized=false:NoSchedule-","title":"Setup K3s"},{"location":"k3s_quick_start_incluster/#how-to-deploy-loxilb","text":"loxilb can be deloyed by using the following command in the K3s node sudo kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/k3s-incluster/loxilb.yml","title":"How to deploy loxilb ?"},{"location":"k3s_quick_start_incluster/#how-to-deploy-kube-loxilb","text":"kube-loxilb is used as an operator to manage loxilb. wget https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/k3s-incluster/kube-loxilb.yml kube-loxilb.yaml args: #- --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setRoles=0.0.0.0 #- --monitor #- --setBGP In the above snippet, loxiURL is commented out which denotes to utilize in-cluster mode to discover loxilb pods automatically. External CIDR represents the IP pool from where serviceLB VIP will be allocated. Apply after making changes (if any) : sudo kubectl apply -f kube-loxilb.yaml","title":"How to deploy kube-loxilb ?"},{"location":"k3s_quick_start_incluster/#create-the-service","text":"sudo kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/k3s-incluster/tcp-svc-lb.yml","title":"Create the service"},{"location":"k3s_quick_start_incluster/#check-the-status","text":"In k3s node: ## Check the pods created $ sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-6c86858495-snvcm 1/1 Running 0 4m37s kube-system coredns-6799fbcd5-cpj6x 1/1 Running 0 4m37s kube-system metrics-server-67c658944b-42ptz 1/1 Running 0 4m37s kube-system loxilb-lb-8l85d 1/1 Running 0 3m40s kube-system kube-loxilb-6f44cdcdf5-5fdtl 1/1 Running 0 2m19s default tcp-onearm-test 1/1 Running 0 88s ## Check the services created $ sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 5m12s tcp-lb-onearm LoadBalancer 10.43.47.60 llb-192.168.82.100 56002:30001/TCP 108s In loxilb pod, we can check internal LB rules: $ sudo kubectl exec -it -n kube-system loxilb-lb-8l85d -- loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-----------------------|------|-----|--------|-----------|-------|--------|--------|----------| | 192.168.82.100 | | 56002 | tcp | default_tcp-lb-onearm | 0 | rr | onearm | 10.0.2.15 | 30001 | 1 | active | 39:2874 |","title":"Check the status"},{"location":"k3s_quick_start_incluster/#connect-from-hostclient","text":"$ curl http://192.168.82.100:56002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> For more detailed information on incluster deployment of loxilb with bgp in a full-blown cluster, kindly follow this blog .","title":"Connect from host/client"},{"location":"k8s_bgp_policy_crd/","text":"License This document is based on the original work by GOBGP . Changes have been made to the original document. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Policy Configuration This page explains LoxiLB with GoBGP policy feature for controlling the route advertisement. It might be called Route Map in other BGP implementations. And This document was written with reference to this goBGP official document. We explain the overview firstly, then the details. Prerequisites Assumed that you run loxilb with -b option. Or If you control loxilb through kube-loxilb, be sure to set the --set-bgp option in the kube-loxilb.yaml file. docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest -b or in the kube-loxilb.yaml And adding - --enableBGPCRDs option in kube-loxilb.yaml args: - --loxiURL=http://12.12.12.1:11111 - --externalCIDR=123.123.123.1/24 - --setBGP=65100 - --enableBGPCRDs And apply CRD yamls as first step. kubectl apply -f manifest/crds/bgp-policy-apply-service.yaml kubectl apply -f manifest/crds/bgp-policy-defined-sets-service.yaml kubectl apply -f manifest/crds/bgp-policy-definition-service.yaml (Note) Currently, gobgp does not support the Policy command in global state. Therefore, only the policy for neighbors is applied, and we plan to apply the global policy through additional development. To apply a policy in a neighbor, you must form a peer by adding the route-server-client option when using gobgp in loxilb. This does not provide a separate API and will be provided in the future. For examples in gobgp, please refer to the following documents . Contents Policy Configuration Prerequisites Contents Overview Route Server Policy Model Policy Structure Configure Policies 1. Defining defined-sets prefix-sets Examples neighbor-sets Examples 2. Defining bgp-defined-sets community-sets Examples ext-community-sets Examples as-path-sets Examples 3. Defining policy-definitions Execution condition of Action Examples 4. Attaching policy 4.1. Attach policy to route-server-client Policy and Soft Reset Overview Policy is a way to control how BGP routes inserted to RIB or advertised to peers. Policy has two parts, Condition and Action . When a policy is configured, Action is applied to routes which meet Condition before routes proceed to next step. GoBGP supports Condition like prefix , neighbor (source/destination of the route), aspath etc.., and Action like accept , reject , MED/aspath/community manipulation etc... You can configure policy by configuration file, CLI or gRPC API. Here, we show how to configure policy via configuration file. Route Server Policy Model The following figure shows how policy works in route server BGP configuration . In route server mode, Import and Export policies are defined with respect to a peer. The Import policy defines what routes will be imported into the master RIB. The Export policy defines what routes will be exported from the master RIB. You can check each policy by the following commands in the loxilb. $ gobgp neighbor <neighbor-addr> policy import $ gobgp neighbor <neighbor-addr> policy export Policy Structure A policy consists of statements. Each statement has condition(s) and action(s). Conditions are categorized into attributes below: prefix neighbor aspath aspath length community extended community rpki validation result route type (internal/external/local) large community afi-safi in As showed in the figure above, some of the conditions point to defined sets, which are a container for each condition item (e.g. prefixes). Actions are categorized into attributes below: accept or reject add/replace/remove community or remove all communities add/subtract or replace MED value set next-hop (specific address/own local address/don't modify) set local-pref prepend AS number in the AS_PATH attribute When ALL conditions in the statement are true , the action(s) in the statement are executed. You can check policy configuration by the following commands. $ kubectl get bgppolicydefinedsetsservice $ kubectl get bgppolicydefinitionservice $ kubectl get bgppolicyapplyservice Configure Policies Policy Configuration comes from two parts, definition and attachment. For definition, we have defined-sets and policy-definition . defined-sets defines condition item for some of the condition type. policy-definitions defines policies based on actions and conditions. defined-sets A single defined-sets entry has prefix match that is named prefix-sets and neighbor match part that is named neighbor-sets . It also has bgp-defined-sets , a subset of defined-sets that defines conditions referring to BGP attributes such as aspath. This defined-sets has a name and it's used to refer to defined-sets items from outside. policy-definitions policy-definitions is a list of policy. A single element has statements part that combines conditions with an action. Below are the steps for policy configuration define defined-sets define prefix-sets define neighbor-sets define bgp-defined-sets define community-sets define ext-community-sets define as-path-setList define large-community-sets define policy-definitions attach neighbor 1. Defining defined-sets defined-sets has prefix information and neighbor information in prefix-sets and neighbor-sets section, and GoBGP uses these information to evaluate routes. Defining defined-sets is needed at first. prefix-sets and neighbor-sets section are prefix match part and neighbor match part. defined-sets example ```yaml # prefix match part apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-prefix spec: name: \"ps1\" definedType: \"prefix\" prefixList: - ipPrefix: \"10.33.0.0/16\" masklengthRange: \"21..24\" neighbor match part apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-neighbor spec: name: \"ns1\" definedType: \"neighbor\" List: - \"10.0.255.1/32\" ``` prefix-sets prefix-sets has prefix-set-list, and prefix-set-list has prefix-set-name and prefix-list as its element. prefix-set-list is used as a condition. Note that prefix-sets has either v4 or v6 addresses. prefix has 1 element and list of sub-elements. Element Description Example Optional name name of prefix-set \"ps1\" prefixList list of prefix and range of length PrefixList has 2 elements. Element Description Example Optional ipPrefix prefix value \"10.33.0.0/16\" masklengthRange range of length \"21..24\" Yes Examples example 1 Match routes whose high order 2 octets of NLRI is 10.33 and its prefix length is between from 21 to 24 If you define a prefix-list that doesn't have MasklengthRange, it matches routes that have just 10.33.0.0/16 as NLRI. # example 1 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-prefix spec: name: \"ps1\" definedType: \"prefix\" prefixList: - ipPrefix: \"10.33.0.0/16\" masklengthRange: \"21..24\" example 2 If you want to evaluate multiple routes with a single prefix-set-list, you can do this by adding an another prefix-list like this: This prefix-set-list match checks if a route has 10.33.0.0/21 to 24 or 10.50.0.0/21 to 24. # example 2 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-prefix spec: name: \"ps1\" definedType: \"prefix\" prefixList: - ipPrefix: \"10.33.0.0/16\" masklengthRange: \"21..24\" - ipPrefix: \"10.50.0.0/16\" masklengthRange: \"21..24\" example 3 prefix-set-name under prefix-set-list is reference to a single prefix-set. If you want to add different prefix-set more, you can add other blocks that form the same structure with example 1. apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-prefix spec: name: \"ps1\" definedType: \"prefix\" prefixList: - ipPrefix: \"10.33.0.0/16\" masklengthRange: \"21..24\" --- apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-prefix spec: name: \"ps2\" definedType: \"prefix\" prefixList: - ipPrefix: \"10.50.0.0/16\" masklengthRange: \"21..24\" neighbor-sets neighbor-sets has neighbor-set-list, and neighbor-set-list has neighbor-set-name and neighbor-info-list as its element. It is necessary to specify a neighbor address in neighbor-info-list. neighbor-set-list is used as a condition. Attention: an empty neighbor-set will match against ANYTHING and not invert based on the match option neighbor has 1 element and list of sub-elements. Element Description Example Optional name name of neighbor \"ns1\" List list of neighbor address neighbor-info-list has 1 element. Element Description Example Optional - neighbor address \"10.0.255.1\" Examples example 1 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-neighbor spec: name: \"ns1\" definedType: \"neighbor\" List: - \"10.0.255.1/32\" --- apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-neighbor spec: name: \"ns2\" definedType: \"neighbor\" List: - \"10.0.0.0/24\" example 2 As with prefix-set-list, neighbor-set-list can have multiple neighbor-info-list like this. # example 2 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-neighbor spec: name: \"ns1\" definedType: \"neighbor\" List: - \"10.0.255.1/32\" - \"10.0.255.2/32\" ``` - example 3 - As with prefix-set-list, multiple neighbor-set-lists can be defined. ```yaml apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-neighbor spec: name: \"ns1\" definedType: \"neighbor\" List: - \"10.0.255.1/32\" --- apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-neighbor spec: name: \"ns2\" definedType: \"neighbor\" List: - \"10.0.254.1/32\" 2. Defining bgp-defined-sets bgp-defined-sets has Community information, Extended Community information and AS_PATH information in each Sets section respectively. And it is a child element of defined-sets. community-sets, ext-community-sets and as-path-sets section are each match part. Like prefix-sets and neighbor-sets, each can have multiple sets and each set can have multiple values. bgp-defined-sets example # Community match part apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-community spec: name: \"community1\" definedType: \"community\" List: - \"65100:10\" # Extended Community match part apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-extcommunity spec: name: \"ecommunity1\" definedType: \"extcommunity\" List: - \"RT:65100:100\" # AS_PATH match part apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-aspath spec: name: \"aspath1\" definedType: \"asPath\" List: - \"^65100\" # Large Community match part apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-largecommunity spec: name: \"lcommunity1\" definedType: \"largecommunity\" List: - \"65100:100:100\" community-sets community-sets has community-set-name and community-list as its element. The Community value are used to evaluate communities held by the destination. Element Description Example Optional name name of CommunitySet \"community1\" List list of community value community-list has 1 element. Element Description Example Optional - community value \"65100:10\" You can use regular expressions to specify community in community-list. Examples example 1 Match routes which has \"65100:10\" as a community value. # example 1 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-community spec: name: \"community1\" definedType: \"community\" List: - \"65100:10\" example 2 Specifying community by regular expression You can use regular expressions based on POSIX 1003.2 regular expressions. # example 2 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-community spec: name: \"community2\" definedType: \"community\" List: - \"6[0-9]+:[0-9]+\" ext-community-sets ext-community-sets has ext-community-set-name and ext-community-list as its element. The values are used to evaluate extended communities held by the destination. Element Description Example Optional name name of ExtCommunitySet \"ecommunity1\" List list of extended community value List has 1 element. Element Description Example Optional - extended community value \"RT:65001:200\" You can use regular expressions to specify extended community in ext-community-list. However, the first one element separated by (part of \"RT\") does not support to the regular expression. The part of \"RT\" indicates a subtype of extended community and subtypes that can be used are as follows: RT: mean the route target. SoO: mean the site of origin(route origin). encap: mean the encapsulation tunnel type, currently gobgp supports the following encap tunnels: l2tp3 gre ip-in-ip vxlan nvgre mpls mpls-in-gre vxlan-gre mpls-in-udp sr-policy geneve LB: mean the link-bandwidth (in bytes). Examples example 1 Match routes which has \"RT:65001:200\" as a extended community value. # example 1 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-extcommunity spec: name: \"ecommunity1\" definedType: \"extcommunity\" List: - \"RT:65100:100\" example 2 Specifying extended community by regular expression You can use regular expressions that is available in Golang. # example 2 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-extcommunity spec: name: \"ecommunity2\" definedType: \"extcommunity\" List: - \"RT:6[0-9]+:[0-9]+\" as-path-sets as-path-sets has as-path-set-name and as-path-list as its element. The numbers are used to evaluate AS numbers in the destination's AS_PATH attribute. Element Description Example Optional name name of as-path-set \"aspath1\" List list of as path value List has 1 elements. Element Description Example Optional - as path value \"^65100\" The AS path regular expression is compatible with Quagga and Cisco. Note Character _ has special meaning. It is abbreviation for (^|[,{}() ]|$) . Some examples follow: From: ^65100_ means the route is passed from AS 65100 directly. Any: _65100_ means the route comes through AS 65100. Origin: _65100$ means the route is originated by AS 65100. Only: ^65100$ means the route is originated by AS 65100 and comes from it directly. ^65100_65001 65100_[0-9]+_.*$ ^6[0-9]_5.*_65.?00$ Examples example 1 Match routes which come from AS 65100. # example 1 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-aspath spec: name: \"aspath1\" definedType: \"asPath\" List: - \"^65100\" example 2 Match routes which come Origin AS 65100 and use regular expressions to other AS. # example 2 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-aspath spec: name: \"aspath1\" definedType: \"asPath\" List: - \"[0-9]+_65[0-9]+_65100$\" 3. Defining policy-definitions policy-definitions consists of condition and action. Condition part is used to evaluate routes from neighbors, if matched, action will be applied. an example of policy-definitions apiVersion: bgppolicydefinition.loxilb.io/v1 kind: BGPPolicyDefinitionService metadata: name: example-policy spec: name: example-policy statements: - name: statement1 conditions: matchPrefixSet: prefixSet: ps1 matchSetOptions: any matchNeighborSet: neighborSet: ns1 matchSetOptions: invert bgpConditions: matchCommunitySet: communitySet: community1 matchSetOptions: any matchExtCommunitySet: communitySet: ecommunity1 matchSetOptions: any matchAsPathSet: asPathSet: aspath1 matchSetOptions: any asPathLength: operator: eq value: 2 afiSafiIn: - l3vpn-ipv4-unicast - ipv4-unicast actions: routeDisposition: accept-route bgpActions: setMed: \"-200\" setAsPathPrepend: as: \"65005\" repeatN: 5 setCommunity: options: add setCommunityMethod: communitiesList: - 65100:20 The elements of policy-definitions are as follows: policy-definitions Element Description Example name policy's name \"example-policy\" - statements Element Description Example name statements's name \"statement1\" - conditions - match-prefix-set Element Description Example prefixSet name for defined-sets.prefix-sets.prefix-set-list that is used in this policy \"ps1\" matchSetOptions option for the check: \"any\" or \"invert\". default is \"any\" \"any\" - conditions - match-neighbor-set Element Description Example neighborSet name for defined-sets.neighbor-sets.neighbor-set-list that is used in this policy \"ns1\" matchSetOptions option for the check: \"any\" or \"invert\". default is \"any\" \"any\" - conditions - bgp-conditions - match-community-set Element Description Example communitySet name for defined-sets.bgp-defined-sets.community-sets.CommunitySetList that is used in this policy \"community1\" matchSetOptions option for the check: \"any\" or \"all\" or \"invert\". default is \"any\" \"invert\" - conditions - bgp-conditions - match-ext-community-set Element Description Example communitySet name for defined-sets.bgp-defined-sets.ext-community-sets that is used in this policy \"ecommunity1\" matchSetOptions option for the check: \"any\" or \"all\" or \"invert\". default is \"any\" \"invert\" - conditions - bgp-conditions - match-as-path-set Element Description Example asPathSet name for defined-sets.bgp-defined-sets.as-path-sets that is used in this policy \"aspath1\" matchSetOptions option for the check: \"any\" or \"all\" or \"invert\". default is \"any\" \"invert\" - conditions - bgp-conditions - match-as-path-length Element Description Example operator operator to compare the length of AS number in AS_PATH attribute. \"eq\",\"ge\",\"le\" can be used. \"eq\" means that length of AS number is equal to Value element \"ge\" means that length of AS number is equal or greater than the Value element \"le\" means that length of AS number is equal or smaller than the Value element \"eq\" value value used to compare with the length of AS number in AS_PATH attribute 2 - statements - actions Element Description Example routeDisposition stop following policy/statement evaluation and accept/reject the route: \"accept-route\" or \"reject-route\" \"accept-route\" - statements - actions - bgp-actions Element Description Example setMed set-med used to change the med value of the route. If only numbers have been specified, replace the med value of route. if number and operater(+ or -) have been specified, adding or subtracting the med value of route. \"-200\" - statements - actions - bgp-actions - set-community Element Description Example options operator to manipulate Community attribute in the route \"ADD\" communities communities used to manipulate the route's community according to options below \"65100:20\" - statements - actions - bgp-actions - set-as-path-prepend Element Description Example as AS number to prepend. You can use \"last-as\" to prepend the leftmost AS number in the aspath attribute. \"65100\" repeatN repeat count to prepend AS 5 Execution condition of Action Action statement is executed when the result of each Condition, including match-set-options is all true. match-set-options is defined how to determine the match result, in the condition with multiple evaluation set as follows: Value Description any match is true if given value matches any member of the defined set all match is true if given value matches all members of the defined set invert match is true if given value does not match any member of the defined set Examples example 1 This policy definition has prefix-set ps1 and neighbor-set ns1 as its condition and routes matches the condition is rejected. # example 1 apiVersion: bgppolicydefinition.loxilb.io/v1 kind: BGPPolicyDefinitionService metadata: name: policy1 spec: name: policy1 statements: - name: statement1 conditions: matchPrefixSet: prefixSet: ps1 matchSetOptions: any matchNeighborSet: neighborSet: ns1 matchSetOptions: any actions: routeDisposition: reject-route example 2 policy-definition has two statements If a route matches the condition inside the first statement(1), GoBGP applies its action and quits the policy evaluation. # example 2 apiVersion: bgppolicydefinition.loxilb.io/v1 kind: BGPPolicyDefinitionService metadata: name: policy1 spec: name: policy1 statements: - name: statement1 conditions: matchPrefixSet: prefixSet: ps1 matchSetOptions: any matchNeighborSet: neighborSet: ns1 matchSetOptions: any actions: routeDisposition: reject-route - name: statement2 conditions: matchPrefixSet: prefixSet: ps2 matchSetOptions: any matchNeighborSet: neighborSet: ns2 matchSetOptions: any actions: routeDisposition: reject-route example 3 If you want to add other policies, just add policy-definitions block following the first one like this apiVersion: bgppolicydefinition.loxilb.io/v1 kind: BGPPolicyDefinitionService metadata: name: policy1 spec: name: policy1 statements: - name: statement1 conditions: matchPrefixSet: prefixSet: ps1 matchSetOptions: any matchNeighborSet: neighborSet: ns1 matchSetOptions: any actions: routeDisposition: reject-route --- apiVersion: bgppolicydefinition.loxilb.io/v1 kind: BGPPolicyDefinitionService metadata: name: policy2 spec: name: policy2 - name: statement2 conditions: matchPrefixSet: prefixSet: ps2 matchSetOptions: any matchNeighborSet: neighborSet: ns2 matchSetOptions: any actions: routeDisposition: reject-route example 4 This PolicyDefinition has multiple conditions including BgpConditions as follows: prefix-set: ps1 neighbor-set: ns1 community-set: community1 ext-community-set: ecommunity1 as-path-set: aspath1 as-path length: equal 2 If a route matches all these conditions, it will be accepted with community \"65100:20\", next-hop 10.0.0.1, local-pref 110, med subtracted 200, as-path prepended 65005 five times. # example 4 apiVersion: bgppolicydefinition.loxilb.io/v1 kind: BGPPolicyDefinitionService metadata: name: example-policy spec: name: example-policy statements: - name: statement1 conditions: matchPrefixSet: prefixSet: ps1 matchSetOptions: any matchNeighborSet: neighborSet: ns1 matchSetOptions: invert bgpConditions: matchCommunitySet: communitySet: community1 matchSetOptions: any matchExtCommunitySet: communitySet: ecommunity1 matchSetOptions: any matchAsPathSet: asPathSet: aspath1 matchSetOptions: any asPathLength: operator: eq value: 2 afiSafiIn: - l3vpn-ipv4-unicast - ipv4-unicast actions: routeDisposition: accept-route bgpActions: setMed: \"-200\" setAsPathPrepend: as: \"65005\" repeatN: 5 setCommunity: options: add setCommunityMethod: communitiesList: - 65100:20 4. Attaching policy Here we explain how to attach defined policies to neighbor local rib . 4.1. Attach policy to route-server-client You can use policies defined above as Import or Export or In policy by attaching them to neighbors which is configured to be route-server client. To attach policies to neighbors, you need to add policy's name to neighbors.apply-policy in the neighbor's setting. This example attaches policy1 to Import policy and policy2 to Export policy and policy3 is used as the In policy. apiVersion: bgppolicyapply.loxilb.io/v1 kind: BGPPolicyApplyService metadata: name: policy-apply spec: ipAddress: \"10.0.255.2\" policyType: \"import\" polices: - \"policy1\" routeAction: \"accept\" neighbors has a section to specify policies and the section's name is apply-policy. The apply-policy has 4 elements. Element Description Example ipAddress neighbor IP address \"10.0.255.2\" policyType option for the Policy type: \"import\" or \"export\" . \"import\" polices The list of the policy - \"policy1\" routeAction action when the route doesn't match any policy or none of the matched policy specifies route-disposition: \"accept\" or \"reject\". \"accept\" Policy and Soft Reset When you change an import policy and reset the inbound routing table (aka soft reset in), a withdraw for a route rejected by the latest import policies will be sent to peers. However, when you change an export policy and reset the outbound routing table (aka soft reset out), even if a route is rejected by the latest export policies, a withdraw for the route will not be sent. The outbound routing table doesn't exist for saving memory usage, it's impossible to know whether the route was actually sent to peer or the route also was rejected by the previous export policies and not sent. GoBGP doesn't send such withdraw rather than possible unwilling leaking information.","title":"License"},{"location":"k8s_bgp_policy_crd/#license","text":"This document is based on the original work by GOBGP . Changes have been made to the original document. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"k8s_bgp_policy_crd/#policy-configuration","text":"This page explains LoxiLB with GoBGP policy feature for controlling the route advertisement. It might be called Route Map in other BGP implementations. And This document was written with reference to this goBGP official document. We explain the overview firstly, then the details.","title":"Policy Configuration"},{"location":"k8s_bgp_policy_crd/#prerequisites","text":"Assumed that you run loxilb with -b option. Or If you control loxilb through kube-loxilb, be sure to set the --set-bgp option in the kube-loxilb.yaml file. docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest -b or in the kube-loxilb.yaml And adding - --enableBGPCRDs option in kube-loxilb.yaml args: - --loxiURL=http://12.12.12.1:11111 - --externalCIDR=123.123.123.1/24 - --setBGP=65100 - --enableBGPCRDs And apply CRD yamls as first step. kubectl apply -f manifest/crds/bgp-policy-apply-service.yaml kubectl apply -f manifest/crds/bgp-policy-defined-sets-service.yaml kubectl apply -f manifest/crds/bgp-policy-definition-service.yaml (Note) Currently, gobgp does not support the Policy command in global state. Therefore, only the policy for neighbors is applied, and we plan to apply the global policy through additional development. To apply a policy in a neighbor, you must form a peer by adding the route-server-client option when using gobgp in loxilb. This does not provide a separate API and will be provided in the future. For examples in gobgp, please refer to the following documents .","title":"Prerequisites"},{"location":"k8s_bgp_policy_crd/#contents","text":"Policy Configuration Prerequisites Contents Overview Route Server Policy Model Policy Structure Configure Policies 1. Defining defined-sets prefix-sets Examples neighbor-sets Examples 2. Defining bgp-defined-sets community-sets Examples ext-community-sets Examples as-path-sets Examples 3. Defining policy-definitions Execution condition of Action Examples 4. Attaching policy 4.1. Attach policy to route-server-client Policy and Soft Reset","title":"Contents"},{"location":"k8s_bgp_policy_crd/#overview","text":"Policy is a way to control how BGP routes inserted to RIB or advertised to peers. Policy has two parts, Condition and Action . When a policy is configured, Action is applied to routes which meet Condition before routes proceed to next step. GoBGP supports Condition like prefix , neighbor (source/destination of the route), aspath etc.., and Action like accept , reject , MED/aspath/community manipulation etc... You can configure policy by configuration file, CLI or gRPC API. Here, we show how to configure policy via configuration file.","title":"Overview"},{"location":"k8s_bgp_policy_crd/#route-server-policy-model","text":"The following figure shows how policy works in route server BGP configuration . In route server mode, Import and Export policies are defined with respect to a peer. The Import policy defines what routes will be imported into the master RIB. The Export policy defines what routes will be exported from the master RIB. You can check each policy by the following commands in the loxilb. $ gobgp neighbor <neighbor-addr> policy import $ gobgp neighbor <neighbor-addr> policy export","title":"Route Server Policy Model"},{"location":"k8s_bgp_policy_crd/#policy-structure","text":"A policy consists of statements. Each statement has condition(s) and action(s). Conditions are categorized into attributes below: prefix neighbor aspath aspath length community extended community rpki validation result route type (internal/external/local) large community afi-safi in As showed in the figure above, some of the conditions point to defined sets, which are a container for each condition item (e.g. prefixes). Actions are categorized into attributes below: accept or reject add/replace/remove community or remove all communities add/subtract or replace MED value set next-hop (specific address/own local address/don't modify) set local-pref prepend AS number in the AS_PATH attribute When ALL conditions in the statement are true , the action(s) in the statement are executed. You can check policy configuration by the following commands. $ kubectl get bgppolicydefinedsetsservice $ kubectl get bgppolicydefinitionservice $ kubectl get bgppolicyapplyservice","title":"Policy Structure"},{"location":"k8s_bgp_policy_crd/#configure-policies","text":"Policy Configuration comes from two parts, definition and attachment. For definition, we have defined-sets and policy-definition . defined-sets defines condition item for some of the condition type. policy-definitions defines policies based on actions and conditions. defined-sets A single defined-sets entry has prefix match that is named prefix-sets and neighbor match part that is named neighbor-sets . It also has bgp-defined-sets , a subset of defined-sets that defines conditions referring to BGP attributes such as aspath. This defined-sets has a name and it's used to refer to defined-sets items from outside. policy-definitions policy-definitions is a list of policy. A single element has statements part that combines conditions with an action. Below are the steps for policy configuration define defined-sets define prefix-sets define neighbor-sets define bgp-defined-sets define community-sets define ext-community-sets define as-path-setList define large-community-sets define policy-definitions attach neighbor","title":"Configure Policies"},{"location":"k8s_bgp_policy_crd/#1-defining-defined-sets","text":"defined-sets has prefix information and neighbor information in prefix-sets and neighbor-sets section, and GoBGP uses these information to evaluate routes. Defining defined-sets is needed at first. prefix-sets and neighbor-sets section are prefix match part and neighbor match part. defined-sets example ```yaml # prefix match part apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-prefix spec: name: \"ps1\" definedType: \"prefix\" prefixList: - ipPrefix: \"10.33.0.0/16\" masklengthRange: \"21..24\"","title":"1. Defining defined-sets"},{"location":"k8s_bgp_policy_crd/#neighbor-match-part","text":"apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-neighbor spec: name: \"ns1\" definedType: \"neighbor\" List: - \"10.0.255.1/32\" ```","title":"neighbor match part"},{"location":"k8s_bgp_policy_crd/#prefix-sets","text":"prefix-sets has prefix-set-list, and prefix-set-list has prefix-set-name and prefix-list as its element. prefix-set-list is used as a condition. Note that prefix-sets has either v4 or v6 addresses. prefix has 1 element and list of sub-elements. Element Description Example Optional name name of prefix-set \"ps1\" prefixList list of prefix and range of length PrefixList has 2 elements. Element Description Example Optional ipPrefix prefix value \"10.33.0.0/16\" masklengthRange range of length \"21..24\" Yes","title":"prefix-sets"},{"location":"k8s_bgp_policy_crd/#examples","text":"example 1 Match routes whose high order 2 octets of NLRI is 10.33 and its prefix length is between from 21 to 24 If you define a prefix-list that doesn't have MasklengthRange, it matches routes that have just 10.33.0.0/16 as NLRI. # example 1 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-prefix spec: name: \"ps1\" definedType: \"prefix\" prefixList: - ipPrefix: \"10.33.0.0/16\" masklengthRange: \"21..24\" example 2 If you want to evaluate multiple routes with a single prefix-set-list, you can do this by adding an another prefix-list like this: This prefix-set-list match checks if a route has 10.33.0.0/21 to 24 or 10.50.0.0/21 to 24. # example 2 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-prefix spec: name: \"ps1\" definedType: \"prefix\" prefixList: - ipPrefix: \"10.33.0.0/16\" masklengthRange: \"21..24\" - ipPrefix: \"10.50.0.0/16\" masklengthRange: \"21..24\" example 3 prefix-set-name under prefix-set-list is reference to a single prefix-set. If you want to add different prefix-set more, you can add other blocks that form the same structure with example 1. apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-prefix spec: name: \"ps1\" definedType: \"prefix\" prefixList: - ipPrefix: \"10.33.0.0/16\" masklengthRange: \"21..24\" --- apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-prefix spec: name: \"ps2\" definedType: \"prefix\" prefixList: - ipPrefix: \"10.50.0.0/16\" masklengthRange: \"21..24\"","title":"Examples"},{"location":"k8s_bgp_policy_crd/#neighbor-sets","text":"neighbor-sets has neighbor-set-list, and neighbor-set-list has neighbor-set-name and neighbor-info-list as its element. It is necessary to specify a neighbor address in neighbor-info-list. neighbor-set-list is used as a condition. Attention: an empty neighbor-set will match against ANYTHING and not invert based on the match option neighbor has 1 element and list of sub-elements. Element Description Example Optional name name of neighbor \"ns1\" List list of neighbor address neighbor-info-list has 1 element. Element Description Example Optional - neighbor address \"10.0.255.1\"","title":"neighbor-sets"},{"location":"k8s_bgp_policy_crd/#examples_1","text":"example 1 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-neighbor spec: name: \"ns1\" definedType: \"neighbor\" List: - \"10.0.255.1/32\" --- apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-neighbor spec: name: \"ns2\" definedType: \"neighbor\" List: - \"10.0.0.0/24\" example 2 As with prefix-set-list, neighbor-set-list can have multiple neighbor-info-list like this. # example 2 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-neighbor spec: name: \"ns1\" definedType: \"neighbor\" List: - \"10.0.255.1/32\" - \"10.0.255.2/32\" ``` - example 3 - As with prefix-set-list, multiple neighbor-set-lists can be defined. ```yaml apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-neighbor spec: name: \"ns1\" definedType: \"neighbor\" List: - \"10.0.255.1/32\" --- apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-neighbor spec: name: \"ns2\" definedType: \"neighbor\" List: - \"10.0.254.1/32\"","title":"Examples"},{"location":"k8s_bgp_policy_crd/#2-defining-bgp-defined-sets","text":"bgp-defined-sets has Community information, Extended Community information and AS_PATH information in each Sets section respectively. And it is a child element of defined-sets. community-sets, ext-community-sets and as-path-sets section are each match part. Like prefix-sets and neighbor-sets, each can have multiple sets and each set can have multiple values. bgp-defined-sets example # Community match part apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-community spec: name: \"community1\" definedType: \"community\" List: - \"65100:10\" # Extended Community match part apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-extcommunity spec: name: \"ecommunity1\" definedType: \"extcommunity\" List: - \"RT:65100:100\" # AS_PATH match part apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-aspath spec: name: \"aspath1\" definedType: \"asPath\" List: - \"^65100\" # Large Community match part apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-largecommunity spec: name: \"lcommunity1\" definedType: \"largecommunity\" List: - \"65100:100:100\"","title":"2. Defining bgp-defined-sets"},{"location":"k8s_bgp_policy_crd/#community-sets","text":"community-sets has community-set-name and community-list as its element. The Community value are used to evaluate communities held by the destination. Element Description Example Optional name name of CommunitySet \"community1\" List list of community value community-list has 1 element. Element Description Example Optional - community value \"65100:10\" You can use regular expressions to specify community in community-list.","title":"community-sets"},{"location":"k8s_bgp_policy_crd/#examples_2","text":"example 1 Match routes which has \"65100:10\" as a community value. # example 1 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-community spec: name: \"community1\" definedType: \"community\" List: - \"65100:10\" example 2 Specifying community by regular expression You can use regular expressions based on POSIX 1003.2 regular expressions. # example 2 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-community spec: name: \"community2\" definedType: \"community\" List: - \"6[0-9]+:[0-9]+\"","title":"Examples"},{"location":"k8s_bgp_policy_crd/#ext-community-sets","text":"ext-community-sets has ext-community-set-name and ext-community-list as its element. The values are used to evaluate extended communities held by the destination. Element Description Example Optional name name of ExtCommunitySet \"ecommunity1\" List list of extended community value List has 1 element. Element Description Example Optional - extended community value \"RT:65001:200\" You can use regular expressions to specify extended community in ext-community-list. However, the first one element separated by (part of \"RT\") does not support to the regular expression. The part of \"RT\" indicates a subtype of extended community and subtypes that can be used are as follows: RT: mean the route target. SoO: mean the site of origin(route origin). encap: mean the encapsulation tunnel type, currently gobgp supports the following encap tunnels: l2tp3 gre ip-in-ip vxlan nvgre mpls mpls-in-gre vxlan-gre mpls-in-udp sr-policy geneve LB: mean the link-bandwidth (in bytes).","title":"ext-community-sets"},{"location":"k8s_bgp_policy_crd/#examples_3","text":"example 1 Match routes which has \"RT:65001:200\" as a extended community value. # example 1 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-extcommunity spec: name: \"ecommunity1\" definedType: \"extcommunity\" List: - \"RT:65100:100\" example 2 Specifying extended community by regular expression You can use regular expressions that is available in Golang. # example 2 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-extcommunity spec: name: \"ecommunity2\" definedType: \"extcommunity\" List: - \"RT:6[0-9]+:[0-9]+\"","title":"Examples"},{"location":"k8s_bgp_policy_crd/#as-path-sets","text":"as-path-sets has as-path-set-name and as-path-list as its element. The numbers are used to evaluate AS numbers in the destination's AS_PATH attribute. Element Description Example Optional name name of as-path-set \"aspath1\" List list of as path value List has 1 elements. Element Description Example Optional - as path value \"^65100\" The AS path regular expression is compatible with Quagga and Cisco. Note Character _ has special meaning. It is abbreviation for (^|[,{}() ]|$) . Some examples follow: From: ^65100_ means the route is passed from AS 65100 directly. Any: _65100_ means the route comes through AS 65100. Origin: _65100$ means the route is originated by AS 65100. Only: ^65100$ means the route is originated by AS 65100 and comes from it directly. ^65100_65001 65100_[0-9]+_.*$ ^6[0-9]_5.*_65.?00$","title":"as-path-sets"},{"location":"k8s_bgp_policy_crd/#examples_4","text":"example 1 Match routes which come from AS 65100. # example 1 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-aspath spec: name: \"aspath1\" definedType: \"asPath\" List: - \"^65100\" example 2 Match routes which come Origin AS 65100 and use regular expressions to other AS. # example 2 apiVersion: bgppolicydefinedsets.loxilb.io/v1 kind: BGPPolicyDefinedSetsService metadata: name: policy-aspath spec: name: \"aspath1\" definedType: \"asPath\" List: - \"[0-9]+_65[0-9]+_65100$\"","title":"Examples"},{"location":"k8s_bgp_policy_crd/#3-defining-policy-definitions","text":"policy-definitions consists of condition and action. Condition part is used to evaluate routes from neighbors, if matched, action will be applied. an example of policy-definitions apiVersion: bgppolicydefinition.loxilb.io/v1 kind: BGPPolicyDefinitionService metadata: name: example-policy spec: name: example-policy statements: - name: statement1 conditions: matchPrefixSet: prefixSet: ps1 matchSetOptions: any matchNeighborSet: neighborSet: ns1 matchSetOptions: invert bgpConditions: matchCommunitySet: communitySet: community1 matchSetOptions: any matchExtCommunitySet: communitySet: ecommunity1 matchSetOptions: any matchAsPathSet: asPathSet: aspath1 matchSetOptions: any asPathLength: operator: eq value: 2 afiSafiIn: - l3vpn-ipv4-unicast - ipv4-unicast actions: routeDisposition: accept-route bgpActions: setMed: \"-200\" setAsPathPrepend: as: \"65005\" repeatN: 5 setCommunity: options: add setCommunityMethod: communitiesList: - 65100:20 The elements of policy-definitions are as follows: policy-definitions Element Description Example name policy's name \"example-policy\" - statements Element Description Example name statements's name \"statement1\" - conditions - match-prefix-set Element Description Example prefixSet name for defined-sets.prefix-sets.prefix-set-list that is used in this policy \"ps1\" matchSetOptions option for the check: \"any\" or \"invert\". default is \"any\" \"any\" - conditions - match-neighbor-set Element Description Example neighborSet name for defined-sets.neighbor-sets.neighbor-set-list that is used in this policy \"ns1\" matchSetOptions option for the check: \"any\" or \"invert\". default is \"any\" \"any\" - conditions - bgp-conditions - match-community-set Element Description Example communitySet name for defined-sets.bgp-defined-sets.community-sets.CommunitySetList that is used in this policy \"community1\" matchSetOptions option for the check: \"any\" or \"all\" or \"invert\". default is \"any\" \"invert\" - conditions - bgp-conditions - match-ext-community-set Element Description Example communitySet name for defined-sets.bgp-defined-sets.ext-community-sets that is used in this policy \"ecommunity1\" matchSetOptions option for the check: \"any\" or \"all\" or \"invert\". default is \"any\" \"invert\" - conditions - bgp-conditions - match-as-path-set Element Description Example asPathSet name for defined-sets.bgp-defined-sets.as-path-sets that is used in this policy \"aspath1\" matchSetOptions option for the check: \"any\" or \"all\" or \"invert\". default is \"any\" \"invert\" - conditions - bgp-conditions - match-as-path-length Element Description Example operator operator to compare the length of AS number in AS_PATH attribute. \"eq\",\"ge\",\"le\" can be used. \"eq\" means that length of AS number is equal to Value element \"ge\" means that length of AS number is equal or greater than the Value element \"le\" means that length of AS number is equal or smaller than the Value element \"eq\" value value used to compare with the length of AS number in AS_PATH attribute 2 - statements - actions Element Description Example routeDisposition stop following policy/statement evaluation and accept/reject the route: \"accept-route\" or \"reject-route\" \"accept-route\" - statements - actions - bgp-actions Element Description Example setMed set-med used to change the med value of the route. If only numbers have been specified, replace the med value of route. if number and operater(+ or -) have been specified, adding or subtracting the med value of route. \"-200\" - statements - actions - bgp-actions - set-community Element Description Example options operator to manipulate Community attribute in the route \"ADD\" communities communities used to manipulate the route's community according to options below \"65100:20\" - statements - actions - bgp-actions - set-as-path-prepend Element Description Example as AS number to prepend. You can use \"last-as\" to prepend the leftmost AS number in the aspath attribute. \"65100\" repeatN repeat count to prepend AS 5","title":"3. Defining policy-definitions"},{"location":"k8s_bgp_policy_crd/#execution-condition-of-action","text":"Action statement is executed when the result of each Condition, including match-set-options is all true. match-set-options is defined how to determine the match result, in the condition with multiple evaluation set as follows: Value Description any match is true if given value matches any member of the defined set all match is true if given value matches all members of the defined set invert match is true if given value does not match any member of the defined set","title":"Execution condition of Action"},{"location":"k8s_bgp_policy_crd/#examples_5","text":"example 1 This policy definition has prefix-set ps1 and neighbor-set ns1 as its condition and routes matches the condition is rejected. # example 1 apiVersion: bgppolicydefinition.loxilb.io/v1 kind: BGPPolicyDefinitionService metadata: name: policy1 spec: name: policy1 statements: - name: statement1 conditions: matchPrefixSet: prefixSet: ps1 matchSetOptions: any matchNeighborSet: neighborSet: ns1 matchSetOptions: any actions: routeDisposition: reject-route example 2 policy-definition has two statements If a route matches the condition inside the first statement(1), GoBGP applies its action and quits the policy evaluation. # example 2 apiVersion: bgppolicydefinition.loxilb.io/v1 kind: BGPPolicyDefinitionService metadata: name: policy1 spec: name: policy1 statements: - name: statement1 conditions: matchPrefixSet: prefixSet: ps1 matchSetOptions: any matchNeighborSet: neighborSet: ns1 matchSetOptions: any actions: routeDisposition: reject-route - name: statement2 conditions: matchPrefixSet: prefixSet: ps2 matchSetOptions: any matchNeighborSet: neighborSet: ns2 matchSetOptions: any actions: routeDisposition: reject-route example 3 If you want to add other policies, just add policy-definitions block following the first one like this apiVersion: bgppolicydefinition.loxilb.io/v1 kind: BGPPolicyDefinitionService metadata: name: policy1 spec: name: policy1 statements: - name: statement1 conditions: matchPrefixSet: prefixSet: ps1 matchSetOptions: any matchNeighborSet: neighborSet: ns1 matchSetOptions: any actions: routeDisposition: reject-route --- apiVersion: bgppolicydefinition.loxilb.io/v1 kind: BGPPolicyDefinitionService metadata: name: policy2 spec: name: policy2 - name: statement2 conditions: matchPrefixSet: prefixSet: ps2 matchSetOptions: any matchNeighborSet: neighborSet: ns2 matchSetOptions: any actions: routeDisposition: reject-route example 4 This PolicyDefinition has multiple conditions including BgpConditions as follows: prefix-set: ps1 neighbor-set: ns1 community-set: community1 ext-community-set: ecommunity1 as-path-set: aspath1 as-path length: equal 2 If a route matches all these conditions, it will be accepted with community \"65100:20\", next-hop 10.0.0.1, local-pref 110, med subtracted 200, as-path prepended 65005 five times. # example 4 apiVersion: bgppolicydefinition.loxilb.io/v1 kind: BGPPolicyDefinitionService metadata: name: example-policy spec: name: example-policy statements: - name: statement1 conditions: matchPrefixSet: prefixSet: ps1 matchSetOptions: any matchNeighborSet: neighborSet: ns1 matchSetOptions: invert bgpConditions: matchCommunitySet: communitySet: community1 matchSetOptions: any matchExtCommunitySet: communitySet: ecommunity1 matchSetOptions: any matchAsPathSet: asPathSet: aspath1 matchSetOptions: any asPathLength: operator: eq value: 2 afiSafiIn: - l3vpn-ipv4-unicast - ipv4-unicast actions: routeDisposition: accept-route bgpActions: setMed: \"-200\" setAsPathPrepend: as: \"65005\" repeatN: 5 setCommunity: options: add setCommunityMethod: communitiesList: - 65100:20","title":"Examples"},{"location":"k8s_bgp_policy_crd/#4-attaching-policy","text":"Here we explain how to attach defined policies to neighbor local rib .","title":"4. Attaching policy"},{"location":"k8s_bgp_policy_crd/#41-attach-policy-to-route-server-client","text":"You can use policies defined above as Import or Export or In policy by attaching them to neighbors which is configured to be route-server client. To attach policies to neighbors, you need to add policy's name to neighbors.apply-policy in the neighbor's setting. This example attaches policy1 to Import policy and policy2 to Export policy and policy3 is used as the In policy. apiVersion: bgppolicyapply.loxilb.io/v1 kind: BGPPolicyApplyService metadata: name: policy-apply spec: ipAddress: \"10.0.255.2\" policyType: \"import\" polices: - \"policy1\" routeAction: \"accept\" neighbors has a section to specify policies and the section's name is apply-policy. The apply-policy has 4 elements. Element Description Example ipAddress neighbor IP address \"10.0.255.2\" policyType option for the Policy type: \"import\" or \"export\" . \"import\" polices The list of the policy - \"policy1\" routeAction action when the route doesn't match any policy or none of the matched policy specifies route-disposition: \"accept\" or \"reject\". \"accept\"","title":"4.1. Attach policy to route-server-client"},{"location":"k8s_bgp_policy_crd/#policy-and-soft-reset","text":"When you change an import policy and reset the inbound routing table (aka soft reset in), a withdraw for a route rejected by the latest import policies will be sent to peers. However, when you change an export policy and reset the outbound routing table (aka soft reset out), even if a route is rejected by the latest export policies, a withdraw for the route will not be sent. The outbound routing table doesn't exist for saving memory usage, it's impossible to know whether the route was actually sent to peer or the route also was rejected by the previous export policies and not sent. GoBGP doesn't send such withdraw rather than possible unwilling leaking information.","title":"Policy and Soft Reset"},{"location":"kube-loxilb/","text":"What is kube-loxilb ? kube-loxilb is loxilb's implementation of kubernetes service load-balancer spec which includes support for load-balancer class, advanced IPAM (shared or exclusive) etc. kube-loxilb runs as a deloyment set in kube-system namespace. It is a control-plane component that always runs inside k8s cluster and watches k8s system for changes to nodes/end-points/reachability/LB services etc. It acts as a K8s Operator of loxilb . The loxilb component takes care of doing actual job of providing service connectivity and load-balancing. So, from deployment perspective we need to run kube-loxilb inside K8s cluster but we have option to deploy loxilb in-cluster or external to the cluster. The preferred way is to run kube-loxilb component inside the cluster and provision loxilb docker in any external node/vm as mentioned in this guide. The rationale is to provide users a similar look and feel whether running loxilb in an on-prem or public cloud environment. Public-cloud environments usually run load-balancers/firewalls externally in order to provide a secure/dmz perimeter layer outside actual workloads. But users are free to choose any mode (in-cluster mode or external mode) as per convenience and their system architecture. The following blogs give detailed steps for : Running loxilb in external node with AWS EKS Running in-cluster LB with K3s for on-prem use-cases This usually leads to another query - In external mode, who will be responsible for managing this entity ? On public cloud(s), it is as simple as spawning a new instance in your VPC and launch loxilb docker in it. For on-prem cases, you need to run loxilb docker in a spare node/vm as applicable. loxilb docker is a self-contained entity and easily managed with well-known tools like docker, containerd, podman etc. It can be independently restarted/upgraded anytime and kube-loxilb will make sure all the k8s LB services are properly configured each time. When deploying in-cluster mode, everything is managed by Kubernetes itself with little-to-no manual intervention. Overall topology For external mode, the overall topology including all components should be similar to the following : For in-cluster mode, the overall topology including all components should be similar to the following : How to deploy kube-loxilb ? If you have chosen external-mode, please make sure loxilb docker is downloaded and installed properly in a node external to your cluster. One can follow guides here or refer to various other documentation . It is important to have network connectivity from this node to the k8s cluster nodes (where kube-loxilb will eventually run) as seen in the above figure. (PS - This step can be skipped if running in-cluster mode) Download the kube-loxilb config yaml : wget https://github.com/loxilb-io/kube-loxilb/raw/main/manifest/ext-cluster/kube-loxilb.yaml Modify arguments as per user's needs : args: - --loxiURL=http://12.12.12.1:11111 - --externalCIDR=123.123.123.1/24 #- --externalSecondaryCIDRs=124.124.124.1/24,125.125.125.1/24 #- --externalCIDR6=3ffe::1/96 #- --monitor #- --setBGP=65100 #- --extBGPPeers=50.50.50.1:65101,51.51.51.1:65102 #- --setRoles=0.0.0.0 #- --setLBMode=1 #- --setUniqueIP=false The arguments have the following meaning : Name Description loxiURL API server address of loxilb. This is the docker IP address loxilb docker of Step 1. If unspecified, kube-loxilb assumes loxilb is running in-cluster mode and autoconfigures this. externalCIDR CIDR or IPAddress range to allocate addresses from. By default address allocated are shared for different services(shared Mode) externalCIDR6 Ipv6 CIDR or IPAddress range to allocate addresses from. By default address allocated are shared for different services(shared Mode) monitor Enable liveness probe for the LB end-points (default : unset) setBGP Use specified BGP AS-ID to advertise this service. If not specified BGP will be disabled. Please check here how it works. extBGPPeers Specifies external BGP peers with appropriate remote AS setRoles If present, kube-loxilb arbitrates loxilb role(s) in cluster-mode. Further, it sets a special VIP (selected as sourceIP) to communicate with end-points in full-nat mode. setLBMode 0, 1, 2 0 - default (only DNAT, preserves source-IP) 1 - onearm (source IP is changed to load balancer\u2019s interface IP) 2 - fullNAT (sourceIP is changed to virtual IP) setUniqueIP Allocate unique service-IP per LB service (default : false) externalSecondaryCIDRs Secondary CIDR or IPAddress ranges to allocate addresses from in case of multi-homing support Many of the above flags and arguments can be overriden on a per-service basis based on loxilb specific annotation as mentioned below. kube-loxilb supported annotations: Annotations Description loxilb.io/multus-nets When using multus, the multus network can also be used as a service endpoint.Register the multus network name to be used. Example: apiVersion: v1 kind: Service metadata: name: multus-service annotations: loxilb.io/multus-nets: macvlan1,macvlan2 spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: app: pod-01 ports: - port: 55002 targetPort: 5002 type: LoadBalancer loxilb.io/num-secondary-networks When using the SCTP multi-homing function, you can specify the number of secondary IPs(upto 3) to be assigned to the service. When used with the loxilb.io/secondaryIPs annotation, the value set in loxilb.io/num-secondary-networks is ignored. (loxilb.io/secondaryIPs annotation takes precedence) Example: metadata: name: sctp-lb1 annotations: loxilb.io/num-secondary-networks: \u201c2\u201d spec: loadBalancerClass: loxilb.io/loxilb selector: what: sctp-test ports: - port: 55002 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/secondaryIPs When using the SCTP multi-homing function, specify the secondary IP to be assigned to the service. Multiple IPs(upto 3) can be specified at the same time using a comma(,). When used with the loxilb.io/num-secondary-networks annotation, loxilb.io/secondaryIPs takes priority.) Example: metadata: name: sctp-lb-secips annotations: loxilb.io/lbmode: \"fullnat\" loxilb.io/secondaryIPs: \"1.1.1.1,2.2.2.2\" spec: loadBalancerClass: loxilb.io/loxilb selector: what: sctp-lb-secips ports: - port: 56004 targetPort: 9999 protocol: SCTP type: LoadBalancer loxilb.io/staticIP Specifies the External IP to assign to the LoadBalancer service. By default, an external IP is assigned within the externalCIDR range set in kube-loxilb, but using the annotation, IPs outside the range can also be statically specified. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb-fullnat annotations: loxilb.io/lbmode: \"fullnat\" loxilb.io/staticIP: \"192.168.255.254\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-fullnat-test ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/liveness Set LoxiLB to perform a health check (probe) based endpoint selection(If flag is set, only active endpoints will be selected). The default value is no, and when the value is set to yes, the probe function of the corresponding service is activated. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb-fullnat annotations: loxilb.io/liveness : \"yes\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-fullnat-test ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/lbmode Set LB mode individually for each service. Select one among types of values \u200b\u200bthat can be specified: \u201cdefault\u201d, \u201conearm\u201d, \u201cfullnat\u201d or \"dsr\". Please refer to this document for more details. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb-fullnat annotations: loxilb.io/lbmode: \"fullnat\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-fullnat-test ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/ipam Specify which IPAM mode the service will use. Select one of three options: \u201cipv4\u201d, \u201cipv6\u201d, or \u201cipv6to4\u201d. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/ipam : \"ipv4\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/timeout Set the session retention time for the service. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/timeout : \"60\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/probetype Specifies the protocol type to use for endpoint probe operations. You can select one of \u201cudp\u201d, \u201ctcp\u201d, \u201chttps\u201d, \u201chttp\u201d, \u201csctp\u201d, \u201cping\u201d, or \u201cnone\u201d. Probetype is set to protocol type, if you are using lbMode as \"fullnat\" or \"onearm\". To set it off, use probetype : \"none\" Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/probetype : \"ping\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/probeport Set the port to use for probe operation. It is not applied if the loxilb.io/probetype annotation is not used or if it is of type icmp or none. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/probetype : \"tcp\" loxilb.io/probeport : \"3000\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/probereq Specifies API for the probe request. It is not applied if the loxilb.io/probetype annotation is not used or if it is of type icmp or none. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/probetype : \"tcp\" loxilb.io/probeport : \"3000\" loxilb.io/probereq : \"health\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/proberesp Specifies the response to the probe request. It is not applied if the loxilb.io/probetype annotation is not used or if it is of type icmp or none. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/probetype : \"tcp\" loxilb.io/probeport : \"3000\" loxilb.io/probereq : \"health\" loxilb.io/proberesp : \"ok\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/probetimeout Specifies the timeout for starting a probe request (in seconds). The default value is 60 seconds Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/liveness : \"yes\" loxilb.io/probetimeout : \"10\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/proberetries Specifies the number of probe request retries before considering an endpoint as inoperative. The default value is 2 Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/liveness : \"yes\" loxilb.io/probetimeout : \"10\" loxilb.io/proberetries : \"3\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/epselect Specifies the algorithm for end-point slection e.g \"rr\", \"hash\", \"persist\", \"lc\" etc. The default value is roundrobin. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/liveness : \"yes\" loxilb.io/probetimeout : \"10\" loxilb.io/proberetries : \"3\" loxilb.io/epselect : \"hash\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/prefLocalPod Specifies whether to always prefer to select a local pod in in-cluster mode. The default value is false. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/prefLocalPod : \"yes\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer Apply the yaml after making necessary changes : kubectl apply -f kube-loxilb.yaml * The above should make sure kube-loxilb is successfully running. Check kube-loxilb is running : k8s@master:~$ sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-84db5d44d9-pczhz 1/1 Running 0 16h kube-system coredns-6799fbcd5-44qpx 1/1 Running 0 16h kube-system metrics-server-67c658944b-t4x5d 1/1 Running 0 16h kube-system kube-loxilb-5fb5566999-ll4gs 1/1 Running 0 14h Finally to create service LB for a workload, we can use and apply the following template yaml ( Note - Check loadBalancerClass and other loxilb specific annotation) : apiVersion: v1 kind: Service metadata: name: iperf-service annotations: # If there is a need to do liveness check from loxilb loxilb.io/liveness: \"yes\" # Specify LB mode - one of default, onearm or fullnat loxilb.io/lbmode: \"default\" # Specify loxilb IPAM mode - one of ipv4, ipv6 or ipv6to4 loxilb.io/ipam: \"ipv4\" # Specify number of secondary networks for multi-homing # Only valid for SCTP currently # loxilb.io/num-secondary-networks: \"2 # Specify a static externalIP for this service # loxilb.io/staticIP: \"123.123.123.2\" spec: loadBalancerClass: loxilb.io/loxilb selector: what: perf-test ports: - port: 55001 targetPort: 5001 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: iperf1 labels: what: perf-test spec: containers: - name: iperf image: eyes852/ubuntu-iperf-test:0.5 command: - iperf - \"-s\" ports: - containerPort: 5001 Users can change the above as per their needs. Verify LB service is created k8s@master:~$ sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 13h iperf1 LoadBalancer 10.43.8.156 llb-192.168.80.20 55001:5001/TCP 8m20s For more example yaml templates, kindly refer to kube-loxilb's manifest directory Additional steps to deploy loxilb (in-cluster) mode To run loxilb in-cluster mode, the URL argument in kube-loxilb.yaml needs to be commented out: args: #- --loxiURL=http://12.12.12.1:11111 - --externalCIDR=123.123.123.1/24 This enables a self-discovery mode of kube-loxilb where it can find and reach loxilb pods running inside the cluster. Last but not the least we need to create the loxilb pods in cluster : sudo kubectl apply -f https://github.com/loxilb-io/kube-loxilb/raw/main/manifest/in-cluster/loxilb.yaml Once all the pods are created, the same can be verified as follows (you can see both kube-loxilb and loxilb components running: k8s@master:~$ sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-84db5d44d9-pczhz 1/1 Running 0 16h kube-system coredns-6799fbcd5-44qpx 1/1 Running 0 16h kube-system metrics-server-67c658944b-t4x5d 1/1 Running 0 16h kube-system kube-loxilb-5fb5566999-ll4gs 1/1 Running 0 14h kube-system loxilb-lb-mklj2 1/1 Running 0 13h kube-system loxilb-lb-stp5k 1/1 Running 0 13h kube-system loxilb-lb-j8fc6 1/1 Running 0 13h kube-system loxilb-lb-5m85p 1/1 Running 0 13h Thereafter, the process of service creation remains the same as explained in previous sections. How to use kube-loxilb CRDs ? Kube-loxilb provides Custom Resource Definition (CRD). Current the following operations are supported (which would be continually updated): - Add a BGP Peer - Delete a BGP Peer An example of CRD is stored in manifest/crds. Setting up a BGP Peer as an example is as follows: Pre-Processing (Register kube-loxilb CRDs with K8s). Apply lbpeercrd.yaml as first step kubectl apply -f manifest/crds/lbpeercrd.yaml CRD definition You need to create a yaml file that adds a peer for BGP. The example below is an example of creating a Peer with a RemoteAS number of Peer IP address 65123 at 123.123.123.2. Create a file named bgp-peer.yaml and add the contents below. apiVersion: \"bgppeer.loxilb.io/v1\" kind: BGPPeerService metadata: name: bgp-peer-test spec: ipAddress: 123.123.123.2 remoteAs: 65123 remotePort: 179 Apply CRD to add a new BGP Peer kubectl apply -f bgp-peer.yaml Verify the applied CRD You can check it in two ways. The first one can be checked through loxicmd(in loxilb container), and the second one can be checked through kubectl. # loxicmd kubectl exec -it {loxilb} -n kube-system -- loxicmd get bgpneigh | PEER | AS | UP/DOWN | STATE | |----------------|-------|-------------|-------------| | 123.123.123.2 | 65123 | never | ACTIVE | # kubectl kubectl get bgppeerservice NAME PEER AS bgp-peer-test 123.123.123.2 65123","title":"Understanding loxilb deployment in K8s with kube-loxilb"},{"location":"kube-loxilb/#what-is-kube-loxilb","text":"kube-loxilb is loxilb's implementation of kubernetes service load-balancer spec which includes support for load-balancer class, advanced IPAM (shared or exclusive) etc. kube-loxilb runs as a deloyment set in kube-system namespace. It is a control-plane component that always runs inside k8s cluster and watches k8s system for changes to nodes/end-points/reachability/LB services etc. It acts as a K8s Operator of loxilb . The loxilb component takes care of doing actual job of providing service connectivity and load-balancing. So, from deployment perspective we need to run kube-loxilb inside K8s cluster but we have option to deploy loxilb in-cluster or external to the cluster. The preferred way is to run kube-loxilb component inside the cluster and provision loxilb docker in any external node/vm as mentioned in this guide. The rationale is to provide users a similar look and feel whether running loxilb in an on-prem or public cloud environment. Public-cloud environments usually run load-balancers/firewalls externally in order to provide a secure/dmz perimeter layer outside actual workloads. But users are free to choose any mode (in-cluster mode or external mode) as per convenience and their system architecture. The following blogs give detailed steps for : Running loxilb in external node with AWS EKS Running in-cluster LB with K3s for on-prem use-cases This usually leads to another query - In external mode, who will be responsible for managing this entity ? On public cloud(s), it is as simple as spawning a new instance in your VPC and launch loxilb docker in it. For on-prem cases, you need to run loxilb docker in a spare node/vm as applicable. loxilb docker is a self-contained entity and easily managed with well-known tools like docker, containerd, podman etc. It can be independently restarted/upgraded anytime and kube-loxilb will make sure all the k8s LB services are properly configured each time. When deploying in-cluster mode, everything is managed by Kubernetes itself with little-to-no manual intervention.","title":"What is kube-loxilb ?"},{"location":"kube-loxilb/#overall-topology","text":"For external mode, the overall topology including all components should be similar to the following : For in-cluster mode, the overall topology including all components should be similar to the following :","title":"Overall topology"},{"location":"kube-loxilb/#how-to-deploy-kube-loxilb","text":"If you have chosen external-mode, please make sure loxilb docker is downloaded and installed properly in a node external to your cluster. One can follow guides here or refer to various other documentation . It is important to have network connectivity from this node to the k8s cluster nodes (where kube-loxilb will eventually run) as seen in the above figure. (PS - This step can be skipped if running in-cluster mode) Download the kube-loxilb config yaml : wget https://github.com/loxilb-io/kube-loxilb/raw/main/manifest/ext-cluster/kube-loxilb.yaml Modify arguments as per user's needs : args: - --loxiURL=http://12.12.12.1:11111 - --externalCIDR=123.123.123.1/24 #- --externalSecondaryCIDRs=124.124.124.1/24,125.125.125.1/24 #- --externalCIDR6=3ffe::1/96 #- --monitor #- --setBGP=65100 #- --extBGPPeers=50.50.50.1:65101,51.51.51.1:65102 #- --setRoles=0.0.0.0 #- --setLBMode=1 #- --setUniqueIP=false The arguments have the following meaning : Name Description loxiURL API server address of loxilb. This is the docker IP address loxilb docker of Step 1. If unspecified, kube-loxilb assumes loxilb is running in-cluster mode and autoconfigures this. externalCIDR CIDR or IPAddress range to allocate addresses from. By default address allocated are shared for different services(shared Mode) externalCIDR6 Ipv6 CIDR or IPAddress range to allocate addresses from. By default address allocated are shared for different services(shared Mode) monitor Enable liveness probe for the LB end-points (default : unset) setBGP Use specified BGP AS-ID to advertise this service. If not specified BGP will be disabled. Please check here how it works. extBGPPeers Specifies external BGP peers with appropriate remote AS setRoles If present, kube-loxilb arbitrates loxilb role(s) in cluster-mode. Further, it sets a special VIP (selected as sourceIP) to communicate with end-points in full-nat mode. setLBMode 0, 1, 2 0 - default (only DNAT, preserves source-IP) 1 - onearm (source IP is changed to load balancer\u2019s interface IP) 2 - fullNAT (sourceIP is changed to virtual IP) setUniqueIP Allocate unique service-IP per LB service (default : false) externalSecondaryCIDRs Secondary CIDR or IPAddress ranges to allocate addresses from in case of multi-homing support Many of the above flags and arguments can be overriden on a per-service basis based on loxilb specific annotation as mentioned below. kube-loxilb supported annotations: Annotations Description loxilb.io/multus-nets When using multus, the multus network can also be used as a service endpoint.Register the multus network name to be used. Example: apiVersion: v1 kind: Service metadata: name: multus-service annotations: loxilb.io/multus-nets: macvlan1,macvlan2 spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: app: pod-01 ports: - port: 55002 targetPort: 5002 type: LoadBalancer loxilb.io/num-secondary-networks When using the SCTP multi-homing function, you can specify the number of secondary IPs(upto 3) to be assigned to the service. When used with the loxilb.io/secondaryIPs annotation, the value set in loxilb.io/num-secondary-networks is ignored. (loxilb.io/secondaryIPs annotation takes precedence) Example: metadata: name: sctp-lb1 annotations: loxilb.io/num-secondary-networks: \u201c2\u201d spec: loadBalancerClass: loxilb.io/loxilb selector: what: sctp-test ports: - port: 55002 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/secondaryIPs When using the SCTP multi-homing function, specify the secondary IP to be assigned to the service. Multiple IPs(upto 3) can be specified at the same time using a comma(,). When used with the loxilb.io/num-secondary-networks annotation, loxilb.io/secondaryIPs takes priority.) Example: metadata: name: sctp-lb-secips annotations: loxilb.io/lbmode: \"fullnat\" loxilb.io/secondaryIPs: \"1.1.1.1,2.2.2.2\" spec: loadBalancerClass: loxilb.io/loxilb selector: what: sctp-lb-secips ports: - port: 56004 targetPort: 9999 protocol: SCTP type: LoadBalancer loxilb.io/staticIP Specifies the External IP to assign to the LoadBalancer service. By default, an external IP is assigned within the externalCIDR range set in kube-loxilb, but using the annotation, IPs outside the range can also be statically specified. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb-fullnat annotations: loxilb.io/lbmode: \"fullnat\" loxilb.io/staticIP: \"192.168.255.254\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-fullnat-test ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/liveness Set LoxiLB to perform a health check (probe) based endpoint selection(If flag is set, only active endpoints will be selected). The default value is no, and when the value is set to yes, the probe function of the corresponding service is activated. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb-fullnat annotations: loxilb.io/liveness : \"yes\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-fullnat-test ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/lbmode Set LB mode individually for each service. Select one among types of values \u200b\u200bthat can be specified: \u201cdefault\u201d, \u201conearm\u201d, \u201cfullnat\u201d or \"dsr\". Please refer to this document for more details. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb-fullnat annotations: loxilb.io/lbmode: \"fullnat\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-fullnat-test ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/ipam Specify which IPAM mode the service will use. Select one of three options: \u201cipv4\u201d, \u201cipv6\u201d, or \u201cipv6to4\u201d. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/ipam : \"ipv4\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/timeout Set the session retention time for the service. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/timeout : \"60\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/probetype Specifies the protocol type to use for endpoint probe operations. You can select one of \u201cudp\u201d, \u201ctcp\u201d, \u201chttps\u201d, \u201chttp\u201d, \u201csctp\u201d, \u201cping\u201d, or \u201cnone\u201d. Probetype is set to protocol type, if you are using lbMode as \"fullnat\" or \"onearm\". To set it off, use probetype : \"none\" Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/probetype : \"ping\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/probeport Set the port to use for probe operation. It is not applied if the loxilb.io/probetype annotation is not used or if it is of type icmp or none. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/probetype : \"tcp\" loxilb.io/probeport : \"3000\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/probereq Specifies API for the probe request. It is not applied if the loxilb.io/probetype annotation is not used or if it is of type icmp or none. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/probetype : \"tcp\" loxilb.io/probeport : \"3000\" loxilb.io/probereq : \"health\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/proberesp Specifies the response to the probe request. It is not applied if the loxilb.io/probetype annotation is not used or if it is of type icmp or none. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/probetype : \"tcp\" loxilb.io/probeport : \"3000\" loxilb.io/probereq : \"health\" loxilb.io/proberesp : \"ok\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/probetimeout Specifies the timeout for starting a probe request (in seconds). The default value is 60 seconds Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/liveness : \"yes\" loxilb.io/probetimeout : \"10\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/proberetries Specifies the number of probe request retries before considering an endpoint as inoperative. The default value is 2 Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/liveness : \"yes\" loxilb.io/probetimeout : \"10\" loxilb.io/proberetries : \"3\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/epselect Specifies the algorithm for end-point slection e.g \"rr\", \"hash\", \"persist\", \"lc\" etc. The default value is roundrobin. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/liveness : \"yes\" loxilb.io/probetimeout : \"10\" loxilb.io/proberetries : \"3\" loxilb.io/epselect : \"hash\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer loxilb.io/prefLocalPod Specifies whether to always prefer to select a local pod in in-cluster mode. The default value is false. Example: apiVersion: v1 kind: Service metadata: name: sctp-lb annotations: loxilb.io/prefLocalPod : \"yes\" spec: loadBalancerClass: loxilb.io/loxilb externalTrafficPolicy: Local selector: what: sctp-lb ports: - port: 56004 protocol: SCTP targetPort: 9999 type: LoadBalancer Apply the yaml after making necessary changes : kubectl apply -f kube-loxilb.yaml * The above should make sure kube-loxilb is successfully running. Check kube-loxilb is running : k8s@master:~$ sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-84db5d44d9-pczhz 1/1 Running 0 16h kube-system coredns-6799fbcd5-44qpx 1/1 Running 0 16h kube-system metrics-server-67c658944b-t4x5d 1/1 Running 0 16h kube-system kube-loxilb-5fb5566999-ll4gs 1/1 Running 0 14h Finally to create service LB for a workload, we can use and apply the following template yaml ( Note - Check loadBalancerClass and other loxilb specific annotation) : apiVersion: v1 kind: Service metadata: name: iperf-service annotations: # If there is a need to do liveness check from loxilb loxilb.io/liveness: \"yes\" # Specify LB mode - one of default, onearm or fullnat loxilb.io/lbmode: \"default\" # Specify loxilb IPAM mode - one of ipv4, ipv6 or ipv6to4 loxilb.io/ipam: \"ipv4\" # Specify number of secondary networks for multi-homing # Only valid for SCTP currently # loxilb.io/num-secondary-networks: \"2 # Specify a static externalIP for this service # loxilb.io/staticIP: \"123.123.123.2\" spec: loadBalancerClass: loxilb.io/loxilb selector: what: perf-test ports: - port: 55001 targetPort: 5001 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: iperf1 labels: what: perf-test spec: containers: - name: iperf image: eyes852/ubuntu-iperf-test:0.5 command: - iperf - \"-s\" ports: - containerPort: 5001 Users can change the above as per their needs. Verify LB service is created k8s@master:~$ sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 13h iperf1 LoadBalancer 10.43.8.156 llb-192.168.80.20 55001:5001/TCP 8m20s For more example yaml templates, kindly refer to kube-loxilb's manifest directory","title":"How to deploy kube-loxilb ?"},{"location":"kube-loxilb/#additional-steps-to-deploy-loxilb-in-cluster-mode","text":"To run loxilb in-cluster mode, the URL argument in kube-loxilb.yaml needs to be commented out: args: #- --loxiURL=http://12.12.12.1:11111 - --externalCIDR=123.123.123.1/24 This enables a self-discovery mode of kube-loxilb where it can find and reach loxilb pods running inside the cluster. Last but not the least we need to create the loxilb pods in cluster : sudo kubectl apply -f https://github.com/loxilb-io/kube-loxilb/raw/main/manifest/in-cluster/loxilb.yaml Once all the pods are created, the same can be verified as follows (you can see both kube-loxilb and loxilb components running: k8s@master:~$ sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-84db5d44d9-pczhz 1/1 Running 0 16h kube-system coredns-6799fbcd5-44qpx 1/1 Running 0 16h kube-system metrics-server-67c658944b-t4x5d 1/1 Running 0 16h kube-system kube-loxilb-5fb5566999-ll4gs 1/1 Running 0 14h kube-system loxilb-lb-mklj2 1/1 Running 0 13h kube-system loxilb-lb-stp5k 1/1 Running 0 13h kube-system loxilb-lb-j8fc6 1/1 Running 0 13h kube-system loxilb-lb-5m85p 1/1 Running 0 13h Thereafter, the process of service creation remains the same as explained in previous sections.","title":"Additional steps to deploy loxilb (in-cluster) mode"},{"location":"kube-loxilb/#how-to-use-kube-loxilb-crds","text":"Kube-loxilb provides Custom Resource Definition (CRD). Current the following operations are supported (which would be continually updated): - Add a BGP Peer - Delete a BGP Peer An example of CRD is stored in manifest/crds. Setting up a BGP Peer as an example is as follows: Pre-Processing (Register kube-loxilb CRDs with K8s). Apply lbpeercrd.yaml as first step kubectl apply -f manifest/crds/lbpeercrd.yaml CRD definition You need to create a yaml file that adds a peer for BGP. The example below is an example of creating a Peer with a RemoteAS number of Peer IP address 65123 at 123.123.123.2. Create a file named bgp-peer.yaml and add the contents below. apiVersion: \"bgppeer.loxilb.io/v1\" kind: BGPPeerService metadata: name: bgp-peer-test spec: ipAddress: 123.123.123.2 remoteAs: 65123 remotePort: 179 Apply CRD to add a new BGP Peer kubectl apply -f bgp-peer.yaml Verify the applied CRD You can check it in two ways. The first one can be checked through loxicmd(in loxilb container), and the second one can be checked through kubectl. # loxicmd kubectl exec -it {loxilb} -n kube-system -- loxicmd get bgpneigh | PEER | AS | UP/DOWN | STATE | |----------------|-------|-------------|-------------| | 123.123.123.2 | 65123 | never | ACTIVE | # kubectl kubectl get bgppeerservice NAME PEER AS bgp-peer-test 123.123.123.2 65123","title":"How to use kube-loxilb CRDs ?"},{"location":"lb-algo/","text":"Load-balancer algorithms in loxilb loxilb implements a variety of algortihms to achieve load-balancing and distribute incoming traffic to the server end-points 1. Round-Robin (rr) This is default algo used by loxilb. In this mode, loxilb selects the end-points configured for a service in simple round-robin fashion for each new incoming connection 2. Weighted round-robin (wrr) In this mode, loxilb selects the end-points as per weight(in terms of percentage of overall traffic connections) associated with the end-points of a service. For example, if we have three end-points, we can have 70%, 10% and 20% distribution. 3. Persistence (persist) In this mode, every client (sourceIP) will always get connected to a particular end-point. In essence there is no real load-balancing involved but it can be useful for applications which require client session-affinity e.g FTP which requires two connections with the end-point. 4. Flow-hash (hash) In this mode, loxilb will select the end-point based on 5-tuple hash on incoming traffic. This 5-tuple consists of SourceIP, SourcePort, DestinationIP, DestinationPort and IP protocol number. Please note that in this mode connections from same client can also get mapped to different end-points since SourcePort is usually selected randomly by operating systems resulting in a different hash value. 5. Least-Connections (lc) In this mode, loxilb will select end-point which has the least active connections (or least loaded) at a given point of time.","title":"loxilb load-balancer algorithms"},{"location":"lb-algo/#load-balancer-algorithms-in-loxilb","text":"loxilb implements a variety of algortihms to achieve load-balancing and distribute incoming traffic to the server end-points","title":"Load-balancer algorithms in loxilb"},{"location":"lb-algo/#1-round-robin-rr","text":"This is default algo used by loxilb. In this mode, loxilb selects the end-points configured for a service in simple round-robin fashion for each new incoming connection","title":"1. Round-Robin (rr)"},{"location":"lb-algo/#2-weighted-round-robin-wrr","text":"In this mode, loxilb selects the end-points as per weight(in terms of percentage of overall traffic connections) associated with the end-points of a service. For example, if we have three end-points, we can have 70%, 10% and 20% distribution.","title":"2. Weighted round-robin (wrr)"},{"location":"lb-algo/#3-persistence-persist","text":"In this mode, every client (sourceIP) will always get connected to a particular end-point. In essence there is no real load-balancing involved but it can be useful for applications which require client session-affinity e.g FTP which requires two connections with the end-point.","title":"3. Persistence (persist)"},{"location":"lb-algo/#4-flow-hash-hash","text":"In this mode, loxilb will select the end-point based on 5-tuple hash on incoming traffic. This 5-tuple consists of SourceIP, SourcePort, DestinationIP, DestinationPort and IP protocol number. Please note that in this mode connections from same client can also get mapped to different end-points since SourcePort is usually selected randomly by operating systems resulting in a different hash value.","title":"4. Flow-hash (hash)"},{"location":"lb-algo/#5-least-connections-lc","text":"In this mode, loxilb will select end-point which has the least active connections (or least loaded) at a given point of time.","title":"5. Least-Connections (lc)"},{"location":"lb/","text":"What is service type external load-balancer in Kubernetes ? There are many different types of Kubernetes services like NodePort, ClusterIP etc. However, service type external load-balancer provides a way of exposing your application internally and/or externally in the perspective of the k8s cluster. Usually, Kubernetes CCM provider ensures that a load balancer of some sort is created, deleted and updated in your cloud. For on-prem or edge deployments however, organizations need to provide their own CCM load-balancer functions. MetalLB (initially developed at Google) has been the choice for such cases for long. But edge services need to support so many exotic protocols in play like GTP, SCTP, SRv6 etc and integrating everything into a seamlessly working solution has been quite difficult. This is an area where loxilb aims to play a pivotal role. The following is a simple yaml config file which needs to be applied to create a service type load-balancer : \"type\": \"LoadBalancer\" { \"kind\": \"Service\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"sample-service\" }, \"spec\": { \"ports\": [{ \"port\": 9001, \"targetPort\": 5001 }], \"selector\": { \"app\": \"sample\" }, \"type\": \"LoadBalancer\" } } However, if there is no K8s CCM plugin implementing external service load-balancer, such services won't be created and remain in pending state forever.","title":"What is k8s service - load-balancer"},{"location":"lb/#what-is-service-type-external-load-balancer-in-kubernetes","text":"There are many different types of Kubernetes services like NodePort, ClusterIP etc. However, service type external load-balancer provides a way of exposing your application internally and/or externally in the perspective of the k8s cluster. Usually, Kubernetes CCM provider ensures that a load balancer of some sort is created, deleted and updated in your cloud. For on-prem or edge deployments however, organizations need to provide their own CCM load-balancer functions. MetalLB (initially developed at Google) has been the choice for such cases for long. But edge services need to support so many exotic protocols in play like GTP, SCTP, SRv6 etc and integrating everything into a seamlessly working solution has been quite difficult. This is an area where loxilb aims to play a pivotal role. The following is a simple yaml config file which needs to be applied to create a service type load-balancer : \"type\": \"LoadBalancer\" { \"kind\": \"Service\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"sample-service\" }, \"spec\": { \"ports\": [{ \"port\": 9001, \"targetPort\": 5001 }], \"selector\": { \"app\": \"sample\" }, \"type\": \"LoadBalancer\" } } However, if there is no K8s CCM plugin implementing external service load-balancer, such services won't be created and remain in pending state forever.","title":"What is service type external load-balancer in Kubernetes ?"},{"location":"loxilb-nginx-ingress/","text":"How to run loxilb with ingress In Kubernetes, there is usually a lot of overlap between network load-balancer and an Ingress functionality. This creates a lot of confusion. Overall, the differences between an Ingress and a load-balancer service can be categorized as follows: With Kubernetes ingress, we can expose multiple paths with the same service IP. This might be helpful if one is using public cloud, where one has to pay for managed LB services. Hence, creating a single service and exposing mulitple URL paths might be optimal in such use-cases. For this example, we will use ingress-nginx which is a kubernetes community driven ingress. loxilb itself also supports its own ingress ingress (not covered here), which is optimized for cases which require long-lived connections and https termination with eBPF. However, if someone needs more policy oriented ingress, ingress-nginx might be the right fit. Considerations This example is not specific to any particular managed kubernetes implementation like EKS, GKE etc but should work well with any. We will simply use K3s as a based kubernetes platform. Install K3s curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik,servicelb\" K3S_KUBECONFIG_MODE=\"644\" sh - Install loxilb Follow any of the getting started guides as per requirement. Check all the pods are up and running as expected : $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-6799fbcd5-4n4kl 1/1 Running 0 56m kube-system kube-loxilb-b466c99bb-fpgll 1/1 Running 0 56m kube-system local-path-provisioner-6f5d79df6-f52sw 1/1 Running 0 56m kube-system loxilb-lb-gbkw7 1/1 Running 0 30s kube-system metrics-server-54fd9b65b-dchv2 1/1 Running 0 56m Install ingress-nginx kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/baremetal/deploy.yaml Double confirm : $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE ingress-nginx ingress-nginx-admission-create-9vq66 0/1 Completed 0 113s ingress-nginx ingress-nginx-admission-patch-k4d74 0/1 Completed 1 113s ingress-nginx ingress-nginx-controller-845698f4f6-xq6hm 1/1 Running 0 113s kube-system coredns-6799fbcd5-4n4kl 1/1 Running 0 59m kube-system kube-loxilb-b466c99bb-fpgll 1/1 Running 0 59m kube-system local-path-provisioner-6f5d79df6-f52sw 1/1 Running 0 59m kube-system loxilb-lb-gbkw7 1/1 Running 0 3m33s kube-system metrics-server-54fd9b65b-dchv2 1/1 Running 0 59m Install service, backend app and ingress rules Create a LB service for exposing ingress ports apiVersion: v1 kind: Service metadata: name: ingress-nginx-controller-loadbalancer namespace: ingress-nginx spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx ports: - name: http port: 80 protocol: TCP targetPort: 80 - name: https port: 443 protocol: TCP targetPort: 443 type: LoadBalancer Check the services created : $ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 61m ingress-nginx ingress-nginx-controller NodePort 10.43.114.138 <none> 80:30958/TCP,443:31794/TCP 3m22s ingress-nginx ingress-nginx-controller-admission ClusterIP 10.43.107.66 <none> 443/TCP 3m22s ingress-nginx ingress-nginx-controller-loadbalancer LoadBalancer 10.43.27.248 llb-192.168.80.10 80:32218/TCP,443:32617/TCP 9s kube-system kube-dns ClusterIP 10.43.0.10 <none> 53/UDP,53/TCP,9153/TCP 61m kube-system loxilb-lb-service ClusterIP None <none> 11111/TCP,179/TCP,50051/TCP 5m2s kube-system metrics-server ClusterIP 10.43.20.55 <none> 443/TCP 61m At this point of time, all services exposed via ingress can be accessed via \"192.168.80.10\". This IP could be different as per use-case and scenario. This IP can then be associated with DNS for name based access. Create backend apps for domain1.loxilb.io and configure ingress rules with the following yaml apiVersion: apps/v1 kind: Deployment metadata: name: site spec: replicas: 1 selector: matchLabels: name: site-nginx-frontend template: metadata: labels: name: site-nginx-frontend spec: containers: - name: blog image: ghcr.io/loxilb-io/nginx:stable imagePullPolicy: Always ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: site-nginx-service spec: ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: name: site-nginx-frontend --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: site-nginx-ingress annotations: #app.kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/ssl-redirect: \"false\" spec: ingressClassName: nginx rules: - host: domain1.loxilb.io http: paths: - path: / pathType: Prefix backend: service: name: site-nginx-service port: number: 80 Double check status of pods, services and ingress: $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default site-69d64fcd49-j4qhj 1/1 Running 0 46s ingress-nginx ingress-nginx-admission-create-9vq66 0/1 Completed 0 8m21s ingress-nginx ingress-nginx-admission-patch-k4d74 0/1 Completed 1 8m21s ingress-nginx ingress-nginx-controller-845698f4f6-xq6hm 1/1 Running 0 8m21s kube-system coredns-6799fbcd5-4n4kl 1/1 Running 0 66m kube-system kube-loxilb-b466c99bb-fpgll 1/1 Running 0 66m kube-system local-path-provisioner-6f5d79df6-f52sw 1/1 Running 0 66m kube-system loxilb-lb-gbkw7 1/1 Running 0 10m kube-system metrics-server-54fd9b65b-dchv2 1/1 Running 0 66m $ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 67m default site-nginx-service ClusterIP 10.43.16.35 <none> 80/TCP 108s ingress-nginx ingress-nginx-controller NodePort 10.43.114.138 <none> 80:30958/TCP,443:31794/TCP 9m23s ingress-nginx ingress-nginx-controller-admission ClusterIP 10.43.107.66 <none> 443/TCP 9m23s ingress-nginx ingress-nginx-controller-loadbalancer LoadBalancer 10.43.27.248 llb-192.168.80.10 80:32218/TCP,443:32617/TCP 6m10s kube-system kube-dns ClusterIP 10.43.0.10 <none> 53/UDP,53/TCP,9153/TCP 67m kube-system loxilb-lb-service ClusterIP None <none> 11111/TCP,179/TCP,50051/TCP 11m kube-system metrics-server ClusterIP 10.43.20.55 <none> 443/TCP 67m $ kubectl get ingress -A NAMESPACE NAME CLASS HOSTS ADDRESS PORTS AGE default site-nginx-ingress nginx domain1.loxilb.io 10.0.2.15 80 2m10s Now, lets create backend apps for domain2.loxilb.io and configure ingress rules with the following yaml apiVersion: apps/v1 kind: Deployment metadata: name: site2 spec: replicas: 1 selector: matchLabels: name: site-nginx-frontend2 template: metadata: labels: name: site-nginx-frontend2 spec: containers: - name: blog image: ghcr.io/loxilb-io/nginx:stable imagePullPolicy: Always ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: site-nginx-service2 spec: ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: name: site-nginx-frontend2 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: site-nginx-ingress2 annotations: #app.kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/ssl-redirect: \"false\" spec: ingressClassName: nginx rules: - host: domain2.loxilb.io http: paths: - path: / pathType: Prefix backend: service: name: site-nginx-service2 port: number: 80 Again, we can check the status of pods, service and ingress: $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default site-69d64fcd49-j4qhj 1/1 Running 0 9m12s default site2-7fff6cfbbf-8d6rp 1/1 Running 0 2m34s ingress-nginx ingress-nginx-admission-create-9vq66 0/1 Completed 0 16m ingress-nginx ingress-nginx-admission-patch-k4d74 0/1 Completed 1 16m ingress-nginx ingress-nginx-controller-845698f4f6-xq6hm 1/1 Running 0 16m kube-system coredns-6799fbcd5-4n4kl 1/1 Running 0 74m kube-system kube-loxilb-b466c99bb-fpgll 1/1 Running 0 74m kube-system local-path-provisioner-6f5d79df6-f52sw 1/1 Running 0 74m kube-system loxilb-lb-gbkw7 1/1 Running 0 18m kube-system metrics-server-54fd9b65b-dchv2 1/1 Running 0 74m $ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 75m default site-nginx-service ClusterIP 10.43.16.35 <none> 80/TCP 9m32s default site-nginx-service2 ClusterIP 10.43.107.99 <none> 80/TCP 2m54s ingress-nginx ingress-nginx-controller NodePort 10.43.114.138 <none> 80:30958/TCP,443:31794/TCP 17m ingress-nginx ingress-nginx-controller-admission ClusterIP 10.43.107.66 <none> 443/TCP 17m ingress-nginx ingress-nginx-controller-loadbalancer LoadBalancer 10.43.27.248 llb-192.168.80.10 80:32218/TCP,443:32617/TCP 13m kube-system kube-dns ClusterIP 10.43.0.10 <none> 53/UDP,53/TCP,9153/TCP 75m kube-system loxilb-lb-service ClusterIP None <none> 11111/TCP,179/TCP,50051/TCP 18m kube-system metrics-server ClusterIP 10.43.20.55 <none> 443/TCP 75m $ kubectl get ingress -A NAMESPACE NAME CLASS HOSTS ADDRESS PORTS AGE default site-nginx-ingress nginx domain1.loxilb.io 10.0.2.15 80 9m49s default site-nginx-ingress2 nginx domain2.loxilb.io 10.0.2.15 80 3m11s Test If you are testing locally you can simply add the following for dns resolution in your bastion/host : $ tail -n 2 /etc/hosts 192.168.80.10 domain1.loxilb.io 192.168.80.10 domain2.loxilb.io The above step is similar to adding A records in a DNS like route53. Finally, try to access the service \"domain1.loxilb.io\" : $ curl -H \"HOST: domain1.loxilb.io\" domain1.loxilb.io <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> And then try to access services domain2.loxilb.io: $ curl -H \"HOST: domain2.loxilb.io\" domain2.loxilb.io <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html>","title":"Loxilb nginx ingress"},{"location":"loxilb-nginx-ingress/#how-to-run-loxilb-with-ingress","text":"In Kubernetes, there is usually a lot of overlap between network load-balancer and an Ingress functionality. This creates a lot of confusion. Overall, the differences between an Ingress and a load-balancer service can be categorized as follows: With Kubernetes ingress, we can expose multiple paths with the same service IP. This might be helpful if one is using public cloud, where one has to pay for managed LB services. Hence, creating a single service and exposing mulitple URL paths might be optimal in such use-cases. For this example, we will use ingress-nginx which is a kubernetes community driven ingress. loxilb itself also supports its own ingress ingress (not covered here), which is optimized for cases which require long-lived connections and https termination with eBPF. However, if someone needs more policy oriented ingress, ingress-nginx might be the right fit.","title":"How to run loxilb with ingress"},{"location":"loxilb-nginx-ingress/#considerations","text":"This example is not specific to any particular managed kubernetes implementation like EKS, GKE etc but should work well with any. We will simply use K3s as a based kubernetes platform.","title":"Considerations"},{"location":"loxilb-nginx-ingress/#install-k3s","text":"curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik,servicelb\" K3S_KUBECONFIG_MODE=\"644\" sh -","title":"Install K3s"},{"location":"loxilb-nginx-ingress/#install-loxilb","text":"Follow any of the getting started guides as per requirement. Check all the pods are up and running as expected : $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-6799fbcd5-4n4kl 1/1 Running 0 56m kube-system kube-loxilb-b466c99bb-fpgll 1/1 Running 0 56m kube-system local-path-provisioner-6f5d79df6-f52sw 1/1 Running 0 56m kube-system loxilb-lb-gbkw7 1/1 Running 0 30s kube-system metrics-server-54fd9b65b-dchv2 1/1 Running 0 56m","title":"Install loxilb"},{"location":"loxilb-nginx-ingress/#install-ingress-nginx","text":"kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/baremetal/deploy.yaml Double confirm : $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE ingress-nginx ingress-nginx-admission-create-9vq66 0/1 Completed 0 113s ingress-nginx ingress-nginx-admission-patch-k4d74 0/1 Completed 1 113s ingress-nginx ingress-nginx-controller-845698f4f6-xq6hm 1/1 Running 0 113s kube-system coredns-6799fbcd5-4n4kl 1/1 Running 0 59m kube-system kube-loxilb-b466c99bb-fpgll 1/1 Running 0 59m kube-system local-path-provisioner-6f5d79df6-f52sw 1/1 Running 0 59m kube-system loxilb-lb-gbkw7 1/1 Running 0 3m33s kube-system metrics-server-54fd9b65b-dchv2 1/1 Running 0 59m","title":"Install ingress-nginx"},{"location":"loxilb-nginx-ingress/#install-service-backend-app-and-ingress-rules","text":"Create a LB service for exposing ingress ports apiVersion: v1 kind: Service metadata: name: ingress-nginx-controller-loadbalancer namespace: ingress-nginx spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx ports: - name: http port: 80 protocol: TCP targetPort: 80 - name: https port: 443 protocol: TCP targetPort: 443 type: LoadBalancer Check the services created : $ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 61m ingress-nginx ingress-nginx-controller NodePort 10.43.114.138 <none> 80:30958/TCP,443:31794/TCP 3m22s ingress-nginx ingress-nginx-controller-admission ClusterIP 10.43.107.66 <none> 443/TCP 3m22s ingress-nginx ingress-nginx-controller-loadbalancer LoadBalancer 10.43.27.248 llb-192.168.80.10 80:32218/TCP,443:32617/TCP 9s kube-system kube-dns ClusterIP 10.43.0.10 <none> 53/UDP,53/TCP,9153/TCP 61m kube-system loxilb-lb-service ClusterIP None <none> 11111/TCP,179/TCP,50051/TCP 5m2s kube-system metrics-server ClusterIP 10.43.20.55 <none> 443/TCP 61m At this point of time, all services exposed via ingress can be accessed via \"192.168.80.10\". This IP could be different as per use-case and scenario. This IP can then be associated with DNS for name based access. Create backend apps for domain1.loxilb.io and configure ingress rules with the following yaml apiVersion: apps/v1 kind: Deployment metadata: name: site spec: replicas: 1 selector: matchLabels: name: site-nginx-frontend template: metadata: labels: name: site-nginx-frontend spec: containers: - name: blog image: ghcr.io/loxilb-io/nginx:stable imagePullPolicy: Always ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: site-nginx-service spec: ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: name: site-nginx-frontend --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: site-nginx-ingress annotations: #app.kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/ssl-redirect: \"false\" spec: ingressClassName: nginx rules: - host: domain1.loxilb.io http: paths: - path: / pathType: Prefix backend: service: name: site-nginx-service port: number: 80 Double check status of pods, services and ingress: $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default site-69d64fcd49-j4qhj 1/1 Running 0 46s ingress-nginx ingress-nginx-admission-create-9vq66 0/1 Completed 0 8m21s ingress-nginx ingress-nginx-admission-patch-k4d74 0/1 Completed 1 8m21s ingress-nginx ingress-nginx-controller-845698f4f6-xq6hm 1/1 Running 0 8m21s kube-system coredns-6799fbcd5-4n4kl 1/1 Running 0 66m kube-system kube-loxilb-b466c99bb-fpgll 1/1 Running 0 66m kube-system local-path-provisioner-6f5d79df6-f52sw 1/1 Running 0 66m kube-system loxilb-lb-gbkw7 1/1 Running 0 10m kube-system metrics-server-54fd9b65b-dchv2 1/1 Running 0 66m $ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 67m default site-nginx-service ClusterIP 10.43.16.35 <none> 80/TCP 108s ingress-nginx ingress-nginx-controller NodePort 10.43.114.138 <none> 80:30958/TCP,443:31794/TCP 9m23s ingress-nginx ingress-nginx-controller-admission ClusterIP 10.43.107.66 <none> 443/TCP 9m23s ingress-nginx ingress-nginx-controller-loadbalancer LoadBalancer 10.43.27.248 llb-192.168.80.10 80:32218/TCP,443:32617/TCP 6m10s kube-system kube-dns ClusterIP 10.43.0.10 <none> 53/UDP,53/TCP,9153/TCP 67m kube-system loxilb-lb-service ClusterIP None <none> 11111/TCP,179/TCP,50051/TCP 11m kube-system metrics-server ClusterIP 10.43.20.55 <none> 443/TCP 67m $ kubectl get ingress -A NAMESPACE NAME CLASS HOSTS ADDRESS PORTS AGE default site-nginx-ingress nginx domain1.loxilb.io 10.0.2.15 80 2m10s Now, lets create backend apps for domain2.loxilb.io and configure ingress rules with the following yaml apiVersion: apps/v1 kind: Deployment metadata: name: site2 spec: replicas: 1 selector: matchLabels: name: site-nginx-frontend2 template: metadata: labels: name: site-nginx-frontend2 spec: containers: - name: blog image: ghcr.io/loxilb-io/nginx:stable imagePullPolicy: Always ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: site-nginx-service2 spec: ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: name: site-nginx-frontend2 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: site-nginx-ingress2 annotations: #app.kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/ssl-redirect: \"false\" spec: ingressClassName: nginx rules: - host: domain2.loxilb.io http: paths: - path: / pathType: Prefix backend: service: name: site-nginx-service2 port: number: 80 Again, we can check the status of pods, service and ingress: $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default site-69d64fcd49-j4qhj 1/1 Running 0 9m12s default site2-7fff6cfbbf-8d6rp 1/1 Running 0 2m34s ingress-nginx ingress-nginx-admission-create-9vq66 0/1 Completed 0 16m ingress-nginx ingress-nginx-admission-patch-k4d74 0/1 Completed 1 16m ingress-nginx ingress-nginx-controller-845698f4f6-xq6hm 1/1 Running 0 16m kube-system coredns-6799fbcd5-4n4kl 1/1 Running 0 74m kube-system kube-loxilb-b466c99bb-fpgll 1/1 Running 0 74m kube-system local-path-provisioner-6f5d79df6-f52sw 1/1 Running 0 74m kube-system loxilb-lb-gbkw7 1/1 Running 0 18m kube-system metrics-server-54fd9b65b-dchv2 1/1 Running 0 74m $ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 75m default site-nginx-service ClusterIP 10.43.16.35 <none> 80/TCP 9m32s default site-nginx-service2 ClusterIP 10.43.107.99 <none> 80/TCP 2m54s ingress-nginx ingress-nginx-controller NodePort 10.43.114.138 <none> 80:30958/TCP,443:31794/TCP 17m ingress-nginx ingress-nginx-controller-admission ClusterIP 10.43.107.66 <none> 443/TCP 17m ingress-nginx ingress-nginx-controller-loadbalancer LoadBalancer 10.43.27.248 llb-192.168.80.10 80:32218/TCP,443:32617/TCP 13m kube-system kube-dns ClusterIP 10.43.0.10 <none> 53/UDP,53/TCP,9153/TCP 75m kube-system loxilb-lb-service ClusterIP None <none> 11111/TCP,179/TCP,50051/TCP 18m kube-system metrics-server ClusterIP 10.43.20.55 <none> 443/TCP 75m $ kubectl get ingress -A NAMESPACE NAME CLASS HOSTS ADDRESS PORTS AGE default site-nginx-ingress nginx domain1.loxilb.io 10.0.2.15 80 9m49s default site-nginx-ingress2 nginx domain2.loxilb.io 10.0.2.15 80 3m11s","title":"Install service, backend app and ingress rules"},{"location":"loxilb-nginx-ingress/#test","text":"If you are testing locally you can simply add the following for dns resolution in your bastion/host : $ tail -n 2 /etc/hosts 192.168.80.10 domain1.loxilb.io 192.168.80.10 domain2.loxilb.io The above step is similar to adding A records in a DNS like route53. Finally, try to access the service \"domain1.loxilb.io\" : $ curl -H \"HOST: domain1.loxilb.io\" domain1.loxilb.io <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> And then try to access services domain2.loxilb.io: $ curl -H \"HOST: domain2.loxilb.io\" domain2.loxilb.io <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html>","title":"Test"},{"location":"loxilbebpf/","text":"loxilb eBPF implementation details In this section, we will look into details of loxilb ebpf implementation in little details and try to check what goes on under the hood. When loxilb is build, it builds two object files as follows : llb@nd2:~/loxilb$ ls -l /opt/loxilb/ total 396 drwxrwxrwt 3 root root 0 6?? 20 11:17 dp -rw-rw-r-- 1 llb llb 305536 6?? 29 09:39 llb_ebpf_main.o -rw-rw-r-- 1 llb llb 95192 6?? 29 09:39 llb_xdp_main.o As the name suggests and based on hook point, xdp version does XDP packet processing while ebpf version is used at TC layer for TC eBPF processing. Interesting enough, the packet forwarding code is largely agnostic of its final hook point due to usage of a light abstraction layer to hide differences between eBPF and XDP layer. Now this beckons the question why separate hook points and how does it all work together ? loxilb does bulk of its processing at TC eBPF layer as this layer is most optimized for doing L4+ processing needed for loxilb operation. XDP's frame format is different than what is used by skb (linux kernel's generic socket buffer). This makes it very difficult (if not impossible) to do tcp checksum offload and other such features used by linux networking stack for quite some time now. In short, if we need to do such operations, XDP performance will be inherently slow. XDP as such is perfect for quick operations at l2 layer. loxilb uses XDP to do certain operations like mirroring. Due to how TC eBPF works, it is difficult to work with multiple packet copies and loxilb's TC eBPF offloads some functinality to XDP layer in such special cases. Loading of loxilb eBPF program loxilb's goLang based agent by default loads the loxilb ebpf programs in all the interfaces(only physical/real/bond/wireguard) available in the system. As loxilb is designed to run in its own docker/container, this is convenient for users who dont want to have to manually load/unload eBPF programs. However, it is still possible to do so manually if need arises : To load : ntc filter add dev eth1 ingress bpf da obj /opt/loxilb/llb_ebpf_main.o sec tc_packet_hook0 To unload: ntc filter del dev eth1 ingress To check: root@nd2:/home/llb# ntc filter show dev eth1 ingress filter protocol all pref 49152 bpf chain 0 filter protocol all pref 49152 bpf chain 0 handle 0x1 llb_ebpf_main.o:[tc_packet_hook0] direct-action not_in_hw id 8715 tag 43a829222e969bce jited Please not that ntc is the customized tc tool from iproute2 package which can be found in loxilb's repository Entry points of loxilb eBPF loxilb's eBPF code is usually divided into two program sections with the following entry functions : tc_packet_func This alongwith the consequent code does majority of the packet processing. If conntrack entries are in established state, this is also responsible for packet tx. However if conntrack entry for a particular packet flow is not established, it makes a bpf tail call to the tc_packet_func_slow tc_packet_func_slow This is responsible mainly for doing NAT lookup and stateful conntrack implementation. Once conntrack entry transitions to established state, the forwarding then can happen directly from tc_packet_func loxilb's XDP code is contained in the following section : xdp_packet_func This is the entry point for packet processing when hook point is XDP instead of TC eBPF Pinned Maps of loxilb eBPF All maps used by loxilb eBPF are mounted in the file-system as below : root@nd2:/home/llb/loxilb# ls -lart /opt/loxilb/dp/ total 4 drwxrwxrwt 3 root root 0 6?? 20 11:17 . drwxr-xr-x 3 root root 4096 6?? 29 10:19 .. drwx------ 3 root root 0 6?? 29 10:19 bpf root@nd2:/home/llb/loxilb# mount | grep bpf none on /opt/netlox/loxilb type bpf (rw,relatime) root@nd2:/home/llb/loxilb# ls -lart /opt/loxilb/dp/bpf/ total 0 drwxrwxrwt 3 root root 0 6?? 20 11:17 .. lrwxrwxrwx 1 root root 0 6?? 20 11:17 xdp -> /opt/loxilb/dp/bpf//tc/ drwx------ 3 root root 0 6?? 20 11:17 tc lrwxrwxrwx 1 root root 0 6?? 20 11:17 ip -> /opt/loxilb/dp/bpf//tc/ -rw------- 1 root root 0 6?? 29 10:19 xfis -rw------- 1 root root 0 6?? 29 10:19 xfck -rw------- 1 root root 0 6?? 29 10:19 xctk -rw------- 1 root root 0 6?? 29 10:19 tx_intf_stats_map -rw------- 1 root root 0 6?? 29 10:19 tx_intf_map -rw------- 1 root root 0 6?? 29 10:19 tx_bd_stats_map -rw------- 1 root root 0 6?? 29 10:19 tmac_stats_map -rw------- 1 root root 0 6?? 29 10:19 tmac_map -rw------- 1 root root 0 6?? 29 10:19 smac_map -rw------- 1 root root 0 6?? 29 10:19 rt_v6_stats_map -rw------- 1 root root 0 6?? 29 10:19 rt_v4_stats_map -rw------- 1 root root 0 6?? 29 10:19 rt_v4_map -rw------- 1 root root 0 6?? 29 10:19 polx_map -rw------- 1 root root 0 6?? 29 10:19 pkts -rw------- 1 root root 0 6?? 29 10:19 pkt_ring -rw------- 1 root root 0 6?? 29 10:19 pgm_tbl -rw------- 1 root root 0 6?? 29 10:19 nh_map -rw------- 1 root root 0 6?? 29 10:19 nat_v4_map -rw------- 1 root root 0 6?? 29 10:19 mirr_map -rw------- 1 root root 0 6?? 29 10:19 intf_stats_map -rw------- 1 root root 0 6?? 29 10:19 intf_map -rw------- 1 root root 0 6?? 29 10:19 fc_v4_stats_map -rw------- 1 root root 0 6?? 29 10:19 fc_v4_map -rw------- 1 root root 0 6?? 29 10:19 fcas -rw------- 1 root root 0 6?? 29 10:19 dmac_map -rw------- 1 root root 0 6?? 29 10:19 ct_v4_map -rw------- 1 root root 0 6?? 29 10:19 bd_stats_map -rw------- 1 root root 0 6?? 29 10:19 acl_v6_stats_map -rw------- 1 root root 0 6?? 29 10:19 acl_v4_stats_map -rw------- 1 root root 0 6?? 29 10:19 acl_v4_map Using bpftool, it is easy to check state of these maps as follows : root@nd2:/home/llb# bpftool map dump pinned /opt/loxilb/dp/bpf/intf_map [{ \"key\": { \"ifindex\": 2, \"ing_vid\": 0, \"pad\": 0 }, \"value\": { \"ca\": { \"act_type\": 11, \"ftrap\": 0, \"oif\": 0, \"cidx\": 0 }, \"\": { \"set_ifi\": { \"xdp_ifidx\": 1, \"zone\": 0, \"bd\": 3801, \"mirr\": 0, \"polid\": 0, \"r\": [0,0,0,0,0,0 ] } } } },{ \"key\": { \"ifindex\": 3, \"ing_vid\": 0, \"pad\": 0 }, \"value\": { \"ca\": { \"act_type\": 11, \"ftrap\": 0, \"oif\": 0, \"cidx\": 0 }, \"\": { \"set_ifi\": { \"xdp_ifidx\": 3, \"zone\": 0, \"bd\": 3803, \"mirr\": 0, \"polid\": 0, \"r\": [0,0,0,0,0,0 ] } } } } ] As our development progresses, we will keep updating details about these map's internals loxilb eBPF pipeline at a glance The following figure shows a very high-level diagram of packet flow through loxilb eBPF pipeline : We use eBPF tail calls to jump from one section to another majorly due to the fact that there is clear separation for CT (conntrack) functionality and packet-forwarding logic. At the same time, since kernel's built in eBPF-verifier imposes a maximum code size limit for a single program/section, it also helps to circumvent this issue.","title":"eBPF internals"},{"location":"loxilbebpf/#loxilb-ebpf-implementation-details","text":"In this section, we will look into details of loxilb ebpf implementation in little details and try to check what goes on under the hood. When loxilb is build, it builds two object files as follows : llb@nd2:~/loxilb$ ls -l /opt/loxilb/ total 396 drwxrwxrwt 3 root root 0 6?? 20 11:17 dp -rw-rw-r-- 1 llb llb 305536 6?? 29 09:39 llb_ebpf_main.o -rw-rw-r-- 1 llb llb 95192 6?? 29 09:39 llb_xdp_main.o As the name suggests and based on hook point, xdp version does XDP packet processing while ebpf version is used at TC layer for TC eBPF processing. Interesting enough, the packet forwarding code is largely agnostic of its final hook point due to usage of a light abstraction layer to hide differences between eBPF and XDP layer. Now this beckons the question why separate hook points and how does it all work together ? loxilb does bulk of its processing at TC eBPF layer as this layer is most optimized for doing L4+ processing needed for loxilb operation. XDP's frame format is different than what is used by skb (linux kernel's generic socket buffer). This makes it very difficult (if not impossible) to do tcp checksum offload and other such features used by linux networking stack for quite some time now. In short, if we need to do such operations, XDP performance will be inherently slow. XDP as such is perfect for quick operations at l2 layer. loxilb uses XDP to do certain operations like mirroring. Due to how TC eBPF works, it is difficult to work with multiple packet copies and loxilb's TC eBPF offloads some functinality to XDP layer in such special cases.","title":"loxilb eBPF implementation details"},{"location":"loxilbebpf/#loading-of-loxilb-ebpf-program","text":"loxilb's goLang based agent by default loads the loxilb ebpf programs in all the interfaces(only physical/real/bond/wireguard) available in the system. As loxilb is designed to run in its own docker/container, this is convenient for users who dont want to have to manually load/unload eBPF programs. However, it is still possible to do so manually if need arises : To load : ntc filter add dev eth1 ingress bpf da obj /opt/loxilb/llb_ebpf_main.o sec tc_packet_hook0 To unload: ntc filter del dev eth1 ingress To check: root@nd2:/home/llb# ntc filter show dev eth1 ingress filter protocol all pref 49152 bpf chain 0 filter protocol all pref 49152 bpf chain 0 handle 0x1 llb_ebpf_main.o:[tc_packet_hook0] direct-action not_in_hw id 8715 tag 43a829222e969bce jited Please not that ntc is the customized tc tool from iproute2 package which can be found in loxilb's repository","title":"Loading of loxilb eBPF program"},{"location":"loxilbebpf/#entry-points-of-loxilb-ebpf","text":"loxilb's eBPF code is usually divided into two program sections with the following entry functions : tc_packet_func This alongwith the consequent code does majority of the packet processing. If conntrack entries are in established state, this is also responsible for packet tx. However if conntrack entry for a particular packet flow is not established, it makes a bpf tail call to the tc_packet_func_slow tc_packet_func_slow This is responsible mainly for doing NAT lookup and stateful conntrack implementation. Once conntrack entry transitions to established state, the forwarding then can happen directly from tc_packet_func loxilb's XDP code is contained in the following section : xdp_packet_func This is the entry point for packet processing when hook point is XDP instead of TC eBPF","title":"Entry points of loxilb eBPF"},{"location":"loxilbebpf/#pinned-maps-of-loxilb-ebpf","text":"All maps used by loxilb eBPF are mounted in the file-system as below : root@nd2:/home/llb/loxilb# ls -lart /opt/loxilb/dp/ total 4 drwxrwxrwt 3 root root 0 6?? 20 11:17 . drwxr-xr-x 3 root root 4096 6?? 29 10:19 .. drwx------ 3 root root 0 6?? 29 10:19 bpf root@nd2:/home/llb/loxilb# mount | grep bpf none on /opt/netlox/loxilb type bpf (rw,relatime) root@nd2:/home/llb/loxilb# ls -lart /opt/loxilb/dp/bpf/ total 0 drwxrwxrwt 3 root root 0 6?? 20 11:17 .. lrwxrwxrwx 1 root root 0 6?? 20 11:17 xdp -> /opt/loxilb/dp/bpf//tc/ drwx------ 3 root root 0 6?? 20 11:17 tc lrwxrwxrwx 1 root root 0 6?? 20 11:17 ip -> /opt/loxilb/dp/bpf//tc/ -rw------- 1 root root 0 6?? 29 10:19 xfis -rw------- 1 root root 0 6?? 29 10:19 xfck -rw------- 1 root root 0 6?? 29 10:19 xctk -rw------- 1 root root 0 6?? 29 10:19 tx_intf_stats_map -rw------- 1 root root 0 6?? 29 10:19 tx_intf_map -rw------- 1 root root 0 6?? 29 10:19 tx_bd_stats_map -rw------- 1 root root 0 6?? 29 10:19 tmac_stats_map -rw------- 1 root root 0 6?? 29 10:19 tmac_map -rw------- 1 root root 0 6?? 29 10:19 smac_map -rw------- 1 root root 0 6?? 29 10:19 rt_v6_stats_map -rw------- 1 root root 0 6?? 29 10:19 rt_v4_stats_map -rw------- 1 root root 0 6?? 29 10:19 rt_v4_map -rw------- 1 root root 0 6?? 29 10:19 polx_map -rw------- 1 root root 0 6?? 29 10:19 pkts -rw------- 1 root root 0 6?? 29 10:19 pkt_ring -rw------- 1 root root 0 6?? 29 10:19 pgm_tbl -rw------- 1 root root 0 6?? 29 10:19 nh_map -rw------- 1 root root 0 6?? 29 10:19 nat_v4_map -rw------- 1 root root 0 6?? 29 10:19 mirr_map -rw------- 1 root root 0 6?? 29 10:19 intf_stats_map -rw------- 1 root root 0 6?? 29 10:19 intf_map -rw------- 1 root root 0 6?? 29 10:19 fc_v4_stats_map -rw------- 1 root root 0 6?? 29 10:19 fc_v4_map -rw------- 1 root root 0 6?? 29 10:19 fcas -rw------- 1 root root 0 6?? 29 10:19 dmac_map -rw------- 1 root root 0 6?? 29 10:19 ct_v4_map -rw------- 1 root root 0 6?? 29 10:19 bd_stats_map -rw------- 1 root root 0 6?? 29 10:19 acl_v6_stats_map -rw------- 1 root root 0 6?? 29 10:19 acl_v4_stats_map -rw------- 1 root root 0 6?? 29 10:19 acl_v4_map Using bpftool, it is easy to check state of these maps as follows : root@nd2:/home/llb# bpftool map dump pinned /opt/loxilb/dp/bpf/intf_map [{ \"key\": { \"ifindex\": 2, \"ing_vid\": 0, \"pad\": 0 }, \"value\": { \"ca\": { \"act_type\": 11, \"ftrap\": 0, \"oif\": 0, \"cidx\": 0 }, \"\": { \"set_ifi\": { \"xdp_ifidx\": 1, \"zone\": 0, \"bd\": 3801, \"mirr\": 0, \"polid\": 0, \"r\": [0,0,0,0,0,0 ] } } } },{ \"key\": { \"ifindex\": 3, \"ing_vid\": 0, \"pad\": 0 }, \"value\": { \"ca\": { \"act_type\": 11, \"ftrap\": 0, \"oif\": 0, \"cidx\": 0 }, \"\": { \"set_ifi\": { \"xdp_ifidx\": 3, \"zone\": 0, \"bd\": 3803, \"mirr\": 0, \"polid\": 0, \"r\": [0,0,0,0,0,0 ] } } } } ] As our development progresses, we will keep updating details about these map's internals","title":"Pinned Maps of loxilb eBPF"},{"location":"loxilbebpf/#loxilb-ebpf-pipeline-at-a-glance","text":"The following figure shows a very high-level diagram of packet flow through loxilb eBPF pipeline : We use eBPF tail calls to jump from one section to another majorly due to the fact that there is clear separation for CT (conntrack) functionality and packet-forwarding logic. At the same time, since kernel's built in eBPF-verifier imposes a maximum code size limit for a single program/section, it also helps to circumvent this issue.","title":"loxilb eBPF pipeline at a glance"},{"location":"microk8s_quick_start_incluster/","text":"Quick Start Guide with MicroK8s and LoxiLB in-cluster mode This document will explain how to install a MicroK8s cluster with loxilb as a serviceLB provider running in-cluster mode. Prerequisite(s) Single node with Linux Topology For quickly bringing up loxilb in-cluster and MicroK8s, we will be deploying all components in a single node : loxilb and kube-loxilb components run as pods managed by kubernetes(MicroK8s) in this scenario. Setup MicroK8s in a single-node # MicroK8s installation steps sudo apt-get update sudo apt install -y snapd sudo snap install microk8s --classic --channel=1.28/stable Check MicroK8s status $ sudo microk8s status --wait-ready microk8s is running high-availability: no datastore master nodes: 127.0.0.1:19001 datastore standby nodes: none addons: enabled: dns # (core) CoreDNS ha-cluster # (core) Configure high availability on the current node helm # (core) Helm - the package manager for Kubernetes helm3 # (core) Helm 3 - the package manager for Kubernetes disabled: cert-manager # (core) Cloud native certificate management cis-hardening # (core) Apply CIS K8s hardening community # (core) The community addons repository dashboard # (core) The Kubernetes dashboard gpu # (core) Automatic enablement of Nvidia CUDA host-access # (core) Allow Pods connecting to Host services smoothly hostpath-storage # (core) Storage class; allocates storage from host directory ingress # (core) Ingress controller for external access kube-ovn # (core) An advanced network fabric for Kubernetes mayastor # (core) OpenEBS MayaStor metrics-server # (core) K8s Metrics Server for API access to service metrics minio # (core) MinIO object storage observability # (core) A lightweight observability stack for logs, traces and metrics prometheus # (core) Prometheus operator for monitoring and logging rbac # (core) Role-Based Access Control for authorisation registry # (core) Private image registry exposed on localhost:32000 rook-ceph # (core) Distributed Ceph storage using Rook storage # (core) Alias to hostpath-storage add-on, deprecated How to deploy loxilb ? loxilb can be deloyed by using the following command in the MicroK8s node sudo microk8s kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/microk8s-incluster/loxilb.yml How to deploy kube-loxilb ? kube-loxilb is used as an operator to manage loxilb. wget https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/microk8s-incluster/kube-loxilb.yml kube-loxilb.yaml args: #- --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setRoles=0.0.0.0 #- --monitor #- --setBGP In the above snippet, loxiURL is commented out which denotes to utilize in-cluster mode to discover loxilb pods automatically. External CIDR represents the IP pool from where serviceLB VIP will be allocated. Apply after making changes (if any) : sudo microk8s kubectl apply -f kube-loxilb.yaml Create the service sudo microk8s kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/microk8s-incluster/tcp-svc-lb.yml Check status of various components in MicroK8s node In MicroK8s node: ## Check the pods created $ sudo microk8s kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-node-fjfvz 1/1 Running 0 10m kube-system coredns-864597b5fd-xtmt4 1/1 Running 0 10m kube-system calico-kube-controllers-77bd7c5b-4kldr 1/1 Running 0 10m kube-system loxilb-lb-7xctp 1/1 Running 0 9m11s kube-system kube-loxilb-6f44cdcdf5-4864j 1/1 Running 0 7m40s default tcp-onearm-test 1/1 Running 0 6m49s ## Check the services created $ sudo microk8s kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.152.183.1 <none> 443/TCP 18m tcp-lb-onearm LoadBalancer 10.152.183.216 llb-192.168.82.100 56002:32186/TCP 14m In loxilb pod, we can check internal LB rules: $ sudo microk8s kubectl exec -it -n kube-system loxilb-lb-7xctp -- loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-----------------------|------|-----|--------|-----------|-------|--------|--------|----------| | 192.168.82.100 | | 56002 | tcp | default_tcp-lb-onearm | 0 | rr | onearm | 10.0.2.15 | 32186 | 1 | active | 25:1842 | Connect from host/client $ curl http://192.168.82.100:56002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> For more detailed information on incluster deployment of loxilb with bgp in a full-blown cluster, kindly follow this blog . All of the above steps are also available as part of loxilb CICD workflow. Follow the steps below to replicate the above (please note that you will need vagrant tool installed to run: $ git clone https://github.com/loxilb-io/loxilb.git $ cd cicd/microk8s-incluster/ # To setup the single node microk8s setup and loxilb in-cluster $ ./config.sh # To validate the results $ ./validation.sh # To login to the node and check the installation $ vagrant ssh k8s-node1 # Cleanup $ ./rmconfig.sh","title":"in-cluster pod - MicroK8s/loxilb in-cluster mode"},{"location":"microk8s_quick_start_incluster/#quick-start-guide-with-microk8s-and-loxilb-in-cluster-mode","text":"This document will explain how to install a MicroK8s cluster with loxilb as a serviceLB provider running in-cluster mode.","title":"Quick Start Guide with MicroK8s and LoxiLB in-cluster mode"},{"location":"microk8s_quick_start_incluster/#prerequisites","text":"Single node with Linux","title":"Prerequisite(s)"},{"location":"microk8s_quick_start_incluster/#topology","text":"For quickly bringing up loxilb in-cluster and MicroK8s, we will be deploying all components in a single node : loxilb and kube-loxilb components run as pods managed by kubernetes(MicroK8s) in this scenario.","title":"Topology"},{"location":"microk8s_quick_start_incluster/#setup-microk8s-in-a-single-node","text":"# MicroK8s installation steps sudo apt-get update sudo apt install -y snapd sudo snap install microk8s --classic --channel=1.28/stable","title":"Setup MicroK8s in a single-node"},{"location":"microk8s_quick_start_incluster/#check-microk8s-status","text":"$ sudo microk8s status --wait-ready microk8s is running high-availability: no datastore master nodes: 127.0.0.1:19001 datastore standby nodes: none addons: enabled: dns # (core) CoreDNS ha-cluster # (core) Configure high availability on the current node helm # (core) Helm - the package manager for Kubernetes helm3 # (core) Helm 3 - the package manager for Kubernetes disabled: cert-manager # (core) Cloud native certificate management cis-hardening # (core) Apply CIS K8s hardening community # (core) The community addons repository dashboard # (core) The Kubernetes dashboard gpu # (core) Automatic enablement of Nvidia CUDA host-access # (core) Allow Pods connecting to Host services smoothly hostpath-storage # (core) Storage class; allocates storage from host directory ingress # (core) Ingress controller for external access kube-ovn # (core) An advanced network fabric for Kubernetes mayastor # (core) OpenEBS MayaStor metrics-server # (core) K8s Metrics Server for API access to service metrics minio # (core) MinIO object storage observability # (core) A lightweight observability stack for logs, traces and metrics prometheus # (core) Prometheus operator for monitoring and logging rbac # (core) Role-Based Access Control for authorisation registry # (core) Private image registry exposed on localhost:32000 rook-ceph # (core) Distributed Ceph storage using Rook storage # (core) Alias to hostpath-storage add-on, deprecated","title":"Check MicroK8s status"},{"location":"microk8s_quick_start_incluster/#how-to-deploy-loxilb","text":"loxilb can be deloyed by using the following command in the MicroK8s node sudo microk8s kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/microk8s-incluster/loxilb.yml","title":"How to deploy loxilb ?"},{"location":"microk8s_quick_start_incluster/#how-to-deploy-kube-loxilb","text":"kube-loxilb is used as an operator to manage loxilb. wget https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/microk8s-incluster/kube-loxilb.yml kube-loxilb.yaml args: #- --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setRoles=0.0.0.0 #- --monitor #- --setBGP In the above snippet, loxiURL is commented out which denotes to utilize in-cluster mode to discover loxilb pods automatically. External CIDR represents the IP pool from where serviceLB VIP will be allocated. Apply after making changes (if any) : sudo microk8s kubectl apply -f kube-loxilb.yaml","title":"How to deploy kube-loxilb ?"},{"location":"microk8s_quick_start_incluster/#create-the-service","text":"sudo microk8s kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/microk8s-incluster/tcp-svc-lb.yml","title":"Create the service"},{"location":"microk8s_quick_start_incluster/#check-status-of-various-components-in-microk8s-node","text":"In MicroK8s node: ## Check the pods created $ sudo microk8s kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-node-fjfvz 1/1 Running 0 10m kube-system coredns-864597b5fd-xtmt4 1/1 Running 0 10m kube-system calico-kube-controllers-77bd7c5b-4kldr 1/1 Running 0 10m kube-system loxilb-lb-7xctp 1/1 Running 0 9m11s kube-system kube-loxilb-6f44cdcdf5-4864j 1/1 Running 0 7m40s default tcp-onearm-test 1/1 Running 0 6m49s ## Check the services created $ sudo microk8s kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.152.183.1 <none> 443/TCP 18m tcp-lb-onearm LoadBalancer 10.152.183.216 llb-192.168.82.100 56002:32186/TCP 14m In loxilb pod, we can check internal LB rules: $ sudo microk8s kubectl exec -it -n kube-system loxilb-lb-7xctp -- loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-----------------------|------|-----|--------|-----------|-------|--------|--------|----------| | 192.168.82.100 | | 56002 | tcp | default_tcp-lb-onearm | 0 | rr | onearm | 10.0.2.15 | 32186 | 1 | active | 25:1842 |","title":"Check status of various components in MicroK8s node"},{"location":"microk8s_quick_start_incluster/#connect-from-hostclient","text":"$ curl http://192.168.82.100:56002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> For more detailed information on incluster deployment of loxilb with bgp in a full-blown cluster, kindly follow this blog . All of the above steps are also available as part of loxilb CICD workflow. Follow the steps below to replicate the above (please note that you will need vagrant tool installed to run: $ git clone https://github.com/loxilb-io/loxilb.git $ cd cicd/microk8s-incluster/ # To setup the single node microk8s setup and loxilb in-cluster $ ./config.sh # To validate the results $ ./validation.sh # To login to the node and check the installation $ vagrant ssh k8s-node1 # Cleanup $ ./rmconfig.sh","title":"Connect from host/client"},{"location":"multi-cloud-ha/","text":"Deploy LoxiLB with multi-cloud HA support LoxiLB supports stateful HA configuration in various cloud environments such as AWS. Especially for AWS, one can configure HA using the Floating IP pattern , together with LoxiLB . Overall Scenario Overall scenario will look like this: Setup configuration for Multi-Cloud/Multi-region will be similar to Multi-AZ-HA configuration Important considerations The steps mentioned in the above documentation are for a single AWS region. For cross-region, similar configuration needs to be done in other AWS regions. Two LoxiLB instances - loxilb1 and loxilb2 will be deployed in different AZs per region. These two loxilbs form a HA pair and operate in active-backup roles. One instance of kube-loxilb will be deployed per region. Every region\u2019s private CIDR will be different and one region\u2019s privateCIDR should be reachable to others through VPC peering. As elastic IP is bound to a particular region, it is impossible to provide connection synchronization for cross-region HA. Only warm stand-by cross-region HA is supported. Full Support for elasticIP in GCP is not available yet. For testing HA with GCP, run single loxilb and kube-loxilb with standard configuration. There will not be any privateCIDR in kube-loxilb.yaml. Mention loxilb IP as externalCIDR. To summarize, when a failover occurs within the region, the public ElasticIP address is always associated to the active LoxiLB instance, so users who were previously accessing EKS using the same ElasticIP address can continue to do so without being affected by any node failure or other issues. When a region-wise failover occurs, DNS will redirect the requests to the different region. An example configuration Please follow the steps to create cluster and prepare VM instances mentioned here . Configuring LoxiLB EC2 Instances kube-loxilb deployment kube-loxilb is a K8s operator for LoxiLB. Download the manifest file required for your deployment in EKS. Create the ServiceAccount and other necessary settings for the cluster before start deploying kube-loxilb per cluster. --- apiVersion: v1 kind: ServiceAccount metadata: name: kube-loxilb namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb rules: - apiGroups: - \"\" resources: - nodes verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - pods verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - endpoints - services - services/status verbs: - get - watch - list - patch - update - apiGroups: - gateway.networking.k8s.io resources: - gatewayclasses - gatewayclasses/status - gateways - gateways/status - tcproutes - udproutes verbs: [\"get\", \"watch\", \"list\", \"patch\", \"update\"] - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - get - watch - list - apiGroups: - authentication.k8s.io resources: - tokenreviews verbs: - create - apiGroups: - authorization.k8s.io resources: - subjectaccessreviews verbs: - create - apiGroups: - bgppeer.loxilb.io resources: - bgppeerservices verbs: - get - watch - list - create - update - delete --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-loxilb subjects: - kind: ServiceAccount name: kube-loxilb namespace: kube-system Change the args inside the yaml below(as applicable) and install it for every region. kube-loxilb-osaka-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kube-loxilb-osaka namespace: kube-system labels: app: kube-loxilb-app spec: replicas: 1 selector: matchLabels: app: kube-loxilb-app template: metadata: labels: app: kube-loxilb-app spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet tolerations: # Mark the pod as a critical add-on for rescheduling. - key: CriticalAddonsOnly operator: Exists priorityClassName: system-node-critical serviceAccountName: kube-loxilb terminationGracePeriodSeconds: 0 containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:aws-support imagePullPolicy: Always command: - /bin/kube-loxilb args: - --loxiURL=http://192.168.218.60:11111,192.168.218.61:11111 - --externalCIDR=13.208.X.X/32 - --privateCIDR=192.168.248.254/32 - --setLBMode=2 - --zone=osaka resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"] kube-loxilb-seoul-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kube-loxilb-seoul namespace: kube-system labels: app: kube-loxilb-app spec: replicas: 1 selector: matchLabels: app: kube-loxilb-app template: metadata: labels: app: kube-loxilb-app spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet tolerations: # Mark the pod as a critical add-on for rescheduling. - key: CriticalAddonsOnly operator: Exists priorityClassName: system-node-critical serviceAccountName: kube-loxilb terminationGracePeriodSeconds: 0 containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:aws-support imagePullPolicy: Always command: - /bin/kube-loxilb args: - --loxiURL=http://192.168.119.11:11111,192.168.119.12:11111 - --externalCIDR=14.112.X.X/32 - --privateCIDR=192.168.150.254/32 - --setLBMode=2 - --zone=seoul resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"] For every region, Edit kube-loxilb- region .yaml * Modify loxiURL with the IPs of the LoxiLB EC2 instances created in the region above. * For externalCIDR, specify the Elastic IP created above. * PrivateCIDR specifies the VIP that will be associated with the Elastic IP. Run LoxiLB Pods Install docker on LoxiLB instance(s) LoxiLB is deployed as a container on each instance. To use containers, docker must first be installed on the instance. Docker installation guide can be found here Running LoxiLB container The following command is for a LoxiLB instance (loxilb1) using subnet-a. sudo docker run -u root --cap-add SYS_ADMIN \\ --restart unless-stopped \\ --net=host \\ --privileged \\ -dit \\ -v /dev/log:/dev/log -e AWS_REGION=ap-northeast-3 \\ --name loxilb \\ ghcr.io/loxilb-io/loxilb:aws-support \\ --cloud=aws --cloudcidrblock=192.168.248.0/24 --cluster=192.168.218.61 --self=0 In the cloudcidrblock option, specify the IP band that includes the VIP set in kube-loxilb's privateCIDR. master LoxiLB uses the value set here to create a new subnet in the AZ where it is located and uses it for HA operation. The cluster option specifies the IP of the partner instance (LoxiLB instance using subnet-b) for which HA is configured. The self option is set to 0. It is just a identier used internally to identify each instance Similarily we can run loxilb2 instance in the second EC2 instance using subnet-b: sudo docker run -u root --cap-add SYS_ADMIN \\ --restart unless-stopped \\ --net=host \\ --privileged \\ -dit \\ -v /dev/log:/dev/log -e AWS_REGION=ap-northeast-3 \\ --name loxilb \\ ghcr.io/loxilb-io/loxilb:aws-support \\ --cloud=aws --cloudcidrblock=192.168.248.0/24 --cluster=192.168.218.60 --self=1 For each instance, HA status can be checked as follows: When the container runs, you can check the HA status as follows: ubuntu@ip-192-168-218-60:~$ sudo docker exec -ti loxilb bash root@ip-192-168-228-108:/# loxicmd get ha | INSTANCE | HASTATE | |----------|---------| | default | MASTER | root@ip-192-168-218-60:/# Creating a service Let's create a test service to test HA functionality. Below are the manifest files for the nginx pod and service that we will use for testing. apiVersion: v1 kind: Service metadata: name: nginx-lb1 spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: nginx-test ports: - port: 55002 targetPort: 80 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: nginx-test labels: what: nginx-test spec: containers: - name: nginx-test image: nginx:stable ports: - containerPort: 80 After creating an nginx service with the above, weu can see that the ElasticIP has been designated as the externalIP of the service. LEIS6N3:~/workspace/aws-demo$ kubectl apply -f nginx.yaml service/nginx-lb1 created pod/nginx-test created LEIS6N3:~/workspace/aws-demo$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 22h nginx-lb1 LoadBalancer 10.100.178.3 llb-13.208.X.X 55002:32403/TCP 15s","title":"Multi cloud ha"},{"location":"multi-cloud-ha/#deploy-loxilb-with-multi-cloud-ha-support","text":"LoxiLB supports stateful HA configuration in various cloud environments such as AWS. Especially for AWS, one can configure HA using the Floating IP pattern , together with LoxiLB .","title":"Deploy LoxiLB with multi-cloud HA support"},{"location":"multi-cloud-ha/#overall-scenario","text":"Overall scenario will look like this: Setup configuration for Multi-Cloud/Multi-region will be similar to Multi-AZ-HA configuration","title":"Overall Scenario"},{"location":"multi-cloud-ha/#important-considerations","text":"The steps mentioned in the above documentation are for a single AWS region. For cross-region, similar configuration needs to be done in other AWS regions. Two LoxiLB instances - loxilb1 and loxilb2 will be deployed in different AZs per region. These two loxilbs form a HA pair and operate in active-backup roles. One instance of kube-loxilb will be deployed per region. Every region\u2019s private CIDR will be different and one region\u2019s privateCIDR should be reachable to others through VPC peering. As elastic IP is bound to a particular region, it is impossible to provide connection synchronization for cross-region HA. Only warm stand-by cross-region HA is supported. Full Support for elasticIP in GCP is not available yet. For testing HA with GCP, run single loxilb and kube-loxilb with standard configuration. There will not be any privateCIDR in kube-loxilb.yaml. Mention loxilb IP as externalCIDR. To summarize, when a failover occurs within the region, the public ElasticIP address is always associated to the active LoxiLB instance, so users who were previously accessing EKS using the same ElasticIP address can continue to do so without being affected by any node failure or other issues. When a region-wise failover occurs, DNS will redirect the requests to the different region.","title":"Important considerations"},{"location":"multi-cloud-ha/#an-example-configuration","text":"Please follow the steps to create cluster and prepare VM instances mentioned here .","title":"An example configuration"},{"location":"multi-cloud-ha/#configuring-loxilb-ec2-instances","text":"","title":"Configuring LoxiLB EC2 Instances"},{"location":"multi-cloud-ha/#kube-loxilb-deployment","text":"kube-loxilb is a K8s operator for LoxiLB. Download the manifest file required for your deployment in EKS. Create the ServiceAccount and other necessary settings for the cluster before start deploying kube-loxilb per cluster. --- apiVersion: v1 kind: ServiceAccount metadata: name: kube-loxilb namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb rules: - apiGroups: - \"\" resources: - nodes verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - pods verbs: - get - watch - list - patch - apiGroups: - \"\" resources: - endpoints - services - services/status verbs: - get - watch - list - patch - update - apiGroups: - gateway.networking.k8s.io resources: - gatewayclasses - gatewayclasses/status - gateways - gateways/status - tcproutes - udproutes verbs: [\"get\", \"watch\", \"list\", \"patch\", \"update\"] - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - get - watch - list - apiGroups: - authentication.k8s.io resources: - tokenreviews verbs: - create - apiGroups: - authorization.k8s.io resources: - subjectaccessreviews verbs: - create - apiGroups: - bgppeer.loxilb.io resources: - bgppeerservices verbs: - get - watch - list - create - update - delete --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-loxilb roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-loxilb subjects: - kind: ServiceAccount name: kube-loxilb namespace: kube-system","title":"kube-loxilb deployment"},{"location":"multi-cloud-ha/#change-the-args-inside-the-yaml-belowas-applicable-and-install-it-for-every-region","text":"kube-loxilb-osaka-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kube-loxilb-osaka namespace: kube-system labels: app: kube-loxilb-app spec: replicas: 1 selector: matchLabels: app: kube-loxilb-app template: metadata: labels: app: kube-loxilb-app spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet tolerations: # Mark the pod as a critical add-on for rescheduling. - key: CriticalAddonsOnly operator: Exists priorityClassName: system-node-critical serviceAccountName: kube-loxilb terminationGracePeriodSeconds: 0 containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:aws-support imagePullPolicy: Always command: - /bin/kube-loxilb args: - --loxiURL=http://192.168.218.60:11111,192.168.218.61:11111 - --externalCIDR=13.208.X.X/32 - --privateCIDR=192.168.248.254/32 - --setLBMode=2 - --zone=osaka resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"] kube-loxilb-seoul-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kube-loxilb-seoul namespace: kube-system labels: app: kube-loxilb-app spec: replicas: 1 selector: matchLabels: app: kube-loxilb-app template: metadata: labels: app: kube-loxilb-app spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet tolerations: # Mark the pod as a critical add-on for rescheduling. - key: CriticalAddonsOnly operator: Exists priorityClassName: system-node-critical serviceAccountName: kube-loxilb terminationGracePeriodSeconds: 0 containers: - name: kube-loxilb image: ghcr.io/loxilb-io/kube-loxilb:aws-support imagePullPolicy: Always command: - /bin/kube-loxilb args: - --loxiURL=http://192.168.119.11:11111,192.168.119.12:11111 - --externalCIDR=14.112.X.X/32 - --privateCIDR=192.168.150.254/32 - --setLBMode=2 - --zone=seoul resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: true capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"] For every region, Edit kube-loxilb- region .yaml * Modify loxiURL with the IPs of the LoxiLB EC2 instances created in the region above. * For externalCIDR, specify the Elastic IP created above. * PrivateCIDR specifies the VIP that will be associated with the Elastic IP.","title":"Change the args inside the yaml below(as applicable) and install it for every region."},{"location":"multi-cloud-ha/#run-loxilb-pods","text":"","title":"Run LoxiLB Pods"},{"location":"multi-cloud-ha/#install-docker-on-loxilb-instances","text":"LoxiLB is deployed as a container on each instance. To use containers, docker must first be installed on the instance. Docker installation guide can be found here","title":"Install docker on LoxiLB instance(s)"},{"location":"multi-cloud-ha/#running-loxilb-container","text":"The following command is for a LoxiLB instance (loxilb1) using subnet-a. sudo docker run -u root --cap-add SYS_ADMIN \\ --restart unless-stopped \\ --net=host \\ --privileged \\ -dit \\ -v /dev/log:/dev/log -e AWS_REGION=ap-northeast-3 \\ --name loxilb \\ ghcr.io/loxilb-io/loxilb:aws-support \\ --cloud=aws --cloudcidrblock=192.168.248.0/24 --cluster=192.168.218.61 --self=0 In the cloudcidrblock option, specify the IP band that includes the VIP set in kube-loxilb's privateCIDR. master LoxiLB uses the value set here to create a new subnet in the AZ where it is located and uses it for HA operation. The cluster option specifies the IP of the partner instance (LoxiLB instance using subnet-b) for which HA is configured. The self option is set to 0. It is just a identier used internally to identify each instance Similarily we can run loxilb2 instance in the second EC2 instance using subnet-b: sudo docker run -u root --cap-add SYS_ADMIN \\ --restart unless-stopped \\ --net=host \\ --privileged \\ -dit \\ -v /dev/log:/dev/log -e AWS_REGION=ap-northeast-3 \\ --name loxilb \\ ghcr.io/loxilb-io/loxilb:aws-support \\ --cloud=aws --cloudcidrblock=192.168.248.0/24 --cluster=192.168.218.60 --self=1 For each instance, HA status can be checked as follows: When the container runs, you can check the HA status as follows: ubuntu@ip-192-168-218-60:~$ sudo docker exec -ti loxilb bash root@ip-192-168-228-108:/# loxicmd get ha | INSTANCE | HASTATE | |----------|---------| | default | MASTER | root@ip-192-168-218-60:/#","title":"Running LoxiLB container"},{"location":"multi-cloud-ha/#creating-a-service","text":"Let's create a test service to test HA functionality. Below are the manifest files for the nginx pod and service that we will use for testing. apiVersion: v1 kind: Service metadata: name: nginx-lb1 spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: nginx-test ports: - port: 55002 targetPort: 80 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: nginx-test labels: what: nginx-test spec: containers: - name: nginx-test image: nginx:stable ports: - containerPort: 80 After creating an nginx service with the above, weu can see that the ElasticIP has been designated as the externalIP of the service. LEIS6N3:~/workspace/aws-demo$ kubectl apply -f nginx.yaml service/nginx-lb1 created pod/nginx-test created LEIS6N3:~/workspace/aws-demo$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 22h nginx-lb1 LoadBalancer 10.100.178.3 llb-13.208.X.X 55002:32403/TCP 15s","title":"Creating a service"},{"location":"nat/","text":"NAT modes in loxilb loxilb implements a variety of NAT modes to achieve load-balancing for different scenarios as far as L4 load-balancing is concerned. These NAT modes have subtle differences and this guide will shed light on these details 1. Normal NAT This is basic NAT mode used by loxilb. In this mode, loxilb employs simple DNAT for incoming requests i.e destination IP (which is also the service IP) is changed to the chosen end-point IP. For the outgoing responses it does the exactly opposite(SNAT). Since loxilb relies on statefulness for this mode, it is necessary that return packets also traverse through loxilb. The following figure illustrates this operation - In this mode, original source IP is preserved till the end-point and provides best visibility for anyone needing it. Finally, this also means the end-points should know how to reach the source. 2. One-ARM Traditionally one-arm NAT mode meant that the LB node used to have a single arm (or connection) to the LAN instead of separate ingress and egress networks. loxilb's one-arm NAT mode is a little extended version of the traditional one-arm mode. In one-arm mode, loxilb chooses its LAN IP as source-IP when sending incoming requests towards end-points nodes. Even if the originating source is not on the same LAN, this is loxilb's default behaviour for one arm mode. 3. Full-NAT In the full-NAT mode, loxilb replaces the source-IP of an incoming request to a special instance IP. This instance IP is associated with each instance in a cluster deployment and maintained internally by loxilb. In this mode, various instances of loxilb cluster will have unique instance IPs and each of them will be advertised by BGP towards the end-point to set the return PATH accordingly. This helps in optimal distribution and spread of traffic in case an active-active clustering mode is desired. 4. L2-DSR mode In L2-DSR (direct server return) mode, loxilb performs load-balancing operation but without changing any IP addresses. It just updates the layer2 header as per selected end-point. Also in DSR mode, loxilb does not need statefulness and end-point can choose a different return path not involving loxilb. This maybe advantageous for certain scenarios where there is a need to reduce load in LB nodes by allowing return traffic to bypass the LB. 5. L3-DSR mode In L3-DSR (direct server return) mode, loxilb performs load-balancing operation but encapsulates the original payload with an IPinIP tunnel towards the end-points. Also like L2-DSR mode, loxilb does not need statefulness and end-point can choose a different/direct return path not involving loxilb.","title":"NAT Modes of loxilb"},{"location":"nat/#nat-modes-in-loxilb","text":"loxilb implements a variety of NAT modes to achieve load-balancing for different scenarios as far as L4 load-balancing is concerned. These NAT modes have subtle differences and this guide will shed light on these details","title":"NAT modes in loxilb"},{"location":"nat/#1-normal-nat","text":"This is basic NAT mode used by loxilb. In this mode, loxilb employs simple DNAT for incoming requests i.e destination IP (which is also the service IP) is changed to the chosen end-point IP. For the outgoing responses it does the exactly opposite(SNAT). Since loxilb relies on statefulness for this mode, it is necessary that return packets also traverse through loxilb. The following figure illustrates this operation - In this mode, original source IP is preserved till the end-point and provides best visibility for anyone needing it. Finally, this also means the end-points should know how to reach the source.","title":"1. Normal NAT"},{"location":"nat/#2-one-arm","text":"Traditionally one-arm NAT mode meant that the LB node used to have a single arm (or connection) to the LAN instead of separate ingress and egress networks. loxilb's one-arm NAT mode is a little extended version of the traditional one-arm mode. In one-arm mode, loxilb chooses its LAN IP as source-IP when sending incoming requests towards end-points nodes. Even if the originating source is not on the same LAN, this is loxilb's default behaviour for one arm mode.","title":"2. One-ARM"},{"location":"nat/#3-full-nat","text":"In the full-NAT mode, loxilb replaces the source-IP of an incoming request to a special instance IP. This instance IP is associated with each instance in a cluster deployment and maintained internally by loxilb. In this mode, various instances of loxilb cluster will have unique instance IPs and each of them will be advertised by BGP towards the end-point to set the return PATH accordingly. This helps in optimal distribution and spread of traffic in case an active-active clustering mode is desired.","title":"3. Full-NAT"},{"location":"nat/#4-l2-dsr-mode","text":"In L2-DSR (direct server return) mode, loxilb performs load-balancing operation but without changing any IP addresses. It just updates the layer2 header as per selected end-point. Also in DSR mode, loxilb does not need statefulness and end-point can choose a different return path not involving loxilb. This maybe advantageous for certain scenarios where there is a need to reduce load in LB nodes by allowing return traffic to bypass the LB.","title":"4. L2-DSR mode"},{"location":"nat/#5-l3-dsr-mode","text":"In L3-DSR (direct server return) mode, loxilb performs load-balancing operation but encapsulates the original payload with an IPinIP tunnel towards the end-points. Also like L2-DSR mode, loxilb does not need statefulness and end-point can choose a different/direct return path not involving loxilb.","title":"5. L3-DSR mode"},{"location":"perf-multi/","text":"Bare-Metal Performance The topology for this test is as follows : In this test, all the hosts, end-points and load-balancer run in separate dedicated servers/nodes. Server specs used - Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz - 40 core RAM 125GB, Kernel 5.15.0-52-generic . The following command can be used to configure loxilb for the given topology: # loxicmd create lb 20.20.20.1 --tcp=2020:5001 --endpoints=31.31.31.1:1,32.32.32.1:1,33.33.33.1:1 The default mode of LoxiLB is RR(round-robin) while other popular distribution modes such as consistent hash( Maglev ), WRR etc are also supported. We run popular tool netperf for the above topology. A quick explanation of terminologies used : RPS - requests per seconds. Given a fixed number of connections, this denotes how many requests/message per second can be supported CPS - connections per second. This denotes how many new TCP connection setup/teardowns can be supported per second and hence one of the most important indicators of load-balancer performance CRR - connect/request/response. This is same as CPS but netperf tool uses this term to refer to CPS as part of its test scenario RR - request/response. This is another netperf test option. We used it to measure min and avg latency We are comparing loxilb with ipvs and haproxy . The results are as follows : Connections per second (TCP_CRR) Requests per second (TCP_RR) Minimum Latency Average Latency Conclusion/Notes - loxilb provides enhanced performance across the spectrum of tests. There is a noticeable gain in CPS. loxilb's CPS scales linearly with number of cores haproxy version used - 2.0.29 netperf test scripts can be found here","title":"Perf multi"},{"location":"perf-multi/#bare-metal-performance","text":"The topology for this test is as follows : In this test, all the hosts, end-points and load-balancer run in separate dedicated servers/nodes. Server specs used - Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz - 40 core RAM 125GB, Kernel 5.15.0-52-generic . The following command can be used to configure loxilb for the given topology: # loxicmd create lb 20.20.20.1 --tcp=2020:5001 --endpoints=31.31.31.1:1,32.32.32.1:1,33.33.33.1:1 The default mode of LoxiLB is RR(round-robin) while other popular distribution modes such as consistent hash( Maglev ), WRR etc are also supported. We run popular tool netperf for the above topology. A quick explanation of terminologies used : RPS - requests per seconds. Given a fixed number of connections, this denotes how many requests/message per second can be supported CPS - connections per second. This denotes how many new TCP connection setup/teardowns can be supported per second and hence one of the most important indicators of load-balancer performance CRR - connect/request/response. This is same as CPS but netperf tool uses this term to refer to CPS as part of its test scenario RR - request/response. This is another netperf test option. We used it to measure min and avg latency We are comparing loxilb with ipvs and haproxy . The results are as follows :","title":"Bare-Metal Performance"},{"location":"perf-multi/#connections-per-second-tcp_crr","text":"","title":"Connections per second (TCP_CRR)"},{"location":"perf-multi/#requests-per-second-tcp_rr","text":"","title":"Requests per second (TCP_RR)"},{"location":"perf-multi/#minimum-latency","text":"","title":"Minimum Latency"},{"location":"perf-multi/#average-latency","text":"","title":"Average Latency"},{"location":"perf-multi/#conclusionnotes-","text":"loxilb provides enhanced performance across the spectrum of tests. There is a noticeable gain in CPS. loxilb's CPS scales linearly with number of cores haproxy version used - 2.0.29 netperf test scripts can be found here","title":"Conclusion/Notes -"},{"location":"perf-single/","text":"Single-node performance The hosts/LB/end-points are run as docker pods inside a single server/node. The topology is as follows : The following command can be used to configure lb for the given topology: # loxicmd create lb 20.20.20.1 --tcp=2020:5001 --endpoints=31.31.31.1:1,32.32.32.1:1,33.33.33.1:1 The testing is done with full stateful connection tracking enabled (non dsr mode). To create the above topology for testing loxilb, users can follow this guide . A go webserver with an empty response is used for benchmark purposes. The code is as following : package main import ( \"log\" \"net/http\" ) func main() { http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { }) if err := http.ListenAndServe(\":5001\", nil); err != nil { log.Fatal(\"ListenAndServe: \", err) } } The above code runs in each of the load-balancer end-points as following : go run ./webserver.go wrk based HTTP benchmarking is one of the tools used in this test. This tool is run with the following parameters: root@loxilb:/home/loxilb # wrk -t8 -c400 -d30s http://20.20.20.1:2020/ - where t: No. of threads, c: No. of connections. d: Duration of test We also run other popular performance testing tools like netperf , iperf along with wrk for the above topology. A quick explanation of terminologies used : RPS - requests per seconds. Given a fixed number of connections, this denotes how many requests/message per second can be supported CPS - connections per second. This denotes how many new TCP connection setup/teardowns can be supported per second and hence one of the most important indicators of load-balancer performance CRR - connect/request/response. This is same as CPS but netperf tool uses this term to refer to CPS as part of its test scenario RR - request/response. This is another netperf test option. We used it to measure min and avg latency The results are as follows : Case 1. System Configuration - Intel(R) Core(TM) i7-4770HQ CPU @ 2.20GHz , 3-Core, 6GB RAM, Kernel 5.15.0-52-generic Tool loopback loxilb ipvs wrk(RPS) 38040 44833 40012 wrk(CPS) n/a 7020 6048 netperf(CRR) n/a 11674 9901 netperf(RR min) 12.31 us 15.2us 19.75us netperf(RR avg) 61.27 us 78.1us 131us iperf 43.5Gbps 41.2Gbps 34.4Gbps Case 2. System Configuration - Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz, 40-core, 124GB RAM, Kernel 5.15.0-52-generic Tool loopback loxilb ipvs haproxy wrk(RPS) 406953 421746 388021 217004 wrk(CPS) n/a 45064 24400 22000 netperf(CRR) n/a 375k 174k 21k netperf(RR min) n/a 12 us 15us 27us netperf(RR avg) n/a 15.78 us 18.25us 35.76us iperf 456Gbps 402Gbps 374Gbps 91Gbps Conclusion/Notes - loxilb provides enhanced performance across the spectrum of tests. There is a noticeable gain in CPS loxilb's CPS is limited only by the fact that this is a single node scenario with shared resources \"loopback\" here refers to client and server running in the same host/pod. This is supposed to be the best case scenario but since there is only a single end-point for lo compared to 3 for LB testing , hence the RPS measurements are on the lower side. iperf is run with 100 threads ( iperf X.X.X.X -P 100 ) haproxy version used - 2.0.29 netperf test scripts can be found here Watch the video https://github.com/loxilb-io/loxilbdocs/assets/106566094/6cf85c4e-7cb4-4d23-b5f6-a7854e07cd7b","title":"Perf single"},{"location":"perf-single/#single-node-performance","text":"The hosts/LB/end-points are run as docker pods inside a single server/node. The topology is as follows : The following command can be used to configure lb for the given topology: # loxicmd create lb 20.20.20.1 --tcp=2020:5001 --endpoints=31.31.31.1:1,32.32.32.1:1,33.33.33.1:1 The testing is done with full stateful connection tracking enabled (non dsr mode). To create the above topology for testing loxilb, users can follow this guide . A go webserver with an empty response is used for benchmark purposes. The code is as following : package main import ( \"log\" \"net/http\" ) func main() { http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { }) if err := http.ListenAndServe(\":5001\", nil); err != nil { log.Fatal(\"ListenAndServe: \", err) } } The above code runs in each of the load-balancer end-points as following : go run ./webserver.go wrk based HTTP benchmarking is one of the tools used in this test. This tool is run with the following parameters: root@loxilb:/home/loxilb # wrk -t8 -c400 -d30s http://20.20.20.1:2020/ - where t: No. of threads, c: No. of connections. d: Duration of test We also run other popular performance testing tools like netperf , iperf along with wrk for the above topology. A quick explanation of terminologies used : RPS - requests per seconds. Given a fixed number of connections, this denotes how many requests/message per second can be supported CPS - connections per second. This denotes how many new TCP connection setup/teardowns can be supported per second and hence one of the most important indicators of load-balancer performance CRR - connect/request/response. This is same as CPS but netperf tool uses this term to refer to CPS as part of its test scenario RR - request/response. This is another netperf test option. We used it to measure min and avg latency The results are as follows :","title":"Single-node performance"},{"location":"perf-single/#case-1-system-configuration-intelr-coretm-i7-4770hq-cpu-220ghz-3-core-6gb-ram-kernel-5150-52-generic","text":"Tool loopback loxilb ipvs wrk(RPS) 38040 44833 40012 wrk(CPS) n/a 7020 6048 netperf(CRR) n/a 11674 9901 netperf(RR min) 12.31 us 15.2us 19.75us netperf(RR avg) 61.27 us 78.1us 131us iperf 43.5Gbps 41.2Gbps 34.4Gbps","title":"Case 1. System Configuration - Intel(R) Core(TM) i7-4770HQ CPU @ 2.20GHz , 3-Core, 6GB RAM, Kernel 5.15.0-52-generic"},{"location":"perf-single/#case-2-system-configuration-intelr-xeonr-silver-4210r-cpu-240ghz-40-core-124gb-ram-kernel-5150-52-generic","text":"Tool loopback loxilb ipvs haproxy wrk(RPS) 406953 421746 388021 217004 wrk(CPS) n/a 45064 24400 22000 netperf(CRR) n/a 375k 174k 21k netperf(RR min) n/a 12 us 15us 27us netperf(RR avg) n/a 15.78 us 18.25us 35.76us iperf 456Gbps 402Gbps 374Gbps 91Gbps","title":"Case 2. System Configuration - Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz, 40-core, 124GB RAM, Kernel 5.15.0-52-generic"},{"location":"perf-single/#conclusionnotes-","text":"loxilb provides enhanced performance across the spectrum of tests. There is a noticeable gain in CPS loxilb's CPS is limited only by the fact that this is a single node scenario with shared resources \"loopback\" here refers to client and server running in the same host/pod. This is supposed to be the best case scenario but since there is only a single end-point for lo compared to 3 for LB testing , hence the RPS measurements are on the lower side. iperf is run with 100 threads ( iperf X.X.X.X -P 100 ) haproxy version used - 2.0.29 netperf test scripts can be found here","title":"Conclusion/Notes -"},{"location":"perf-single/#watch-the-video","text":"https://github.com/loxilb-io/loxilbdocs/assets/106566094/6cf85c4e-7cb4-4d23-b5f6-a7854e07cd7b","title":"Watch the video"},{"location":"perf/","text":"loxilb Performance Single-node (cnf) performance report Bare-metal performance report","title":"Performance Report"},{"location":"perf/#loxilb-performance","text":"Single-node (cnf) performance report Bare-metal performance report","title":"loxilb Performance"},{"location":"quick_start_with_cilium/","text":"LoxiLB Quick Start Guide with Cilium This guide will explain how to: Deploy a single-node K3s cluster with cilium networking Expose services with loxilb as an external load balancer Pre-requisite Single node with Linux Install docker runtime to manage loxilb Topology For quickly bringing up loxilb with cilium CNI, we will be deploying all components in a single node : loxilb and cilium both uses ebpf technology for load balancing and implementing policies. So, to avoid the conflict we have to run them in separate network space. This is reason we are going to run loxilb in a docker and use macvlan for the incoming traffic. Also, this is to mimic a topology close to cloud-hosted k8s where LB nodes run outside a cluster. Install loxilb docker ## Set promisc mode for mac-vlan to work sudo ifconfig eth1 promisc sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged --entrypoint /root/loxilb-io/loxilb/loxilb -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest # Create mac-vlan on top of underlying eth1 interface sudo docker network create -d macvlan -o parent=eth1 --subnet 192.168.82.0/24 --gateway 192.168.82.1 --aux-address 'host=192.168.82.252' llbnet # Assign mac-vlan to loxilb docker with specified IP (which will be used as LB VIP) sudo docker network connect llbnet loxilb --ip=192.168.82.100 # Add iptables rule to allow traffic from source IP(192.168.82.1) to loxilb sudo iptables -A DOCKER -s 192.168.82.1 -j ACCEPT Setup K3s with cilium #K3s installation curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik --disable servicelb --disable-cloud-controller \\ --flannel-backend=none \\ --disable-network-policy\" sh - #Install Cilium CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/master/stable.txt) CLI_ARCH=amd64 if [ \"$(uname -m)\" = \"aarch64\" ]; then CLI_ARCH=arm64; fi curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum} sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum} mkdir -p ~/.kube/ sudo cat /etc/rancher/k3s/k3s.yaml > ~/.kube/config cilium install echo $MASTER_IP > /vagrant/master-ip sudo cp /var/lib/rancher/k3s/server/node-token /vagrant/node-token sudo cp /etc/rancher/k3s/k3s.yaml /vagrant/k3s.yaml sudo sed -i -e \"s/127.0.0.1/${MASTER_IP}/g\" /vagrant/k3s.yaml How to deploy kube-loxilb ? kube-loxilb is used to deploy loxilb with Kubernetes. wget https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/ext-cluster/kube-loxilb.yaml kube-loxilb.yaml args: - --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setMode=1 In the above snippet, loxiURL uses docker interface IP of loxilb, which can be different for each setup. Apply in k8s: kubectl apply -f kube-loxilb.yaml Create the service kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/docker-k3s-cilium/tcp-svc-lb.yml Check the status In k3s: kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 80m tcp-lb-onearm LoadBalancer 10.43.183.123 llb-192.168.82.100 56002:30001/TCP 6m50s In loxilb docker: $ sudo docker exec -it loxilb loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-----------------------|------|-----|--------|-----------|-------|--------|--------|----------| | 192.168.82.100 | | 56002 | tcp | default_tcp-lb-onearm | 0 | rr | onearm | 10.0.2.15 | 30001 | 1 | active | 12:880 | Connect from client $ curl http://192.168.82.100:56002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> All of the above steps are also available as part of loxilb CICD workflow. Follow the steps below to replicate the above: $ cd cicd/docker-k3s-cilium/ # To setup the single node k3s setup with cilium as CNI and loxilb as external load balancer $ ./config.sh # To validate the results $ ./validation.sh # Cleanup $ ./rmconfig.sh","title":"ext-cluster pod - K3s/loxilb with cilium"},{"location":"quick_start_with_cilium/#loxilb-quick-start-guide-with-cilium","text":"This guide will explain how to: Deploy a single-node K3s cluster with cilium networking Expose services with loxilb as an external load balancer","title":"LoxiLB Quick Start Guide with Cilium"},{"location":"quick_start_with_cilium/#pre-requisite","text":"Single node with Linux Install docker runtime to manage loxilb","title":"Pre-requisite"},{"location":"quick_start_with_cilium/#topology","text":"For quickly bringing up loxilb with cilium CNI, we will be deploying all components in a single node : loxilb and cilium both uses ebpf technology for load balancing and implementing policies. So, to avoid the conflict we have to run them in separate network space. This is reason we are going to run loxilb in a docker and use macvlan for the incoming traffic. Also, this is to mimic a topology close to cloud-hosted k8s where LB nodes run outside a cluster.","title":"Topology"},{"location":"quick_start_with_cilium/#install-loxilb-docker","text":"## Set promisc mode for mac-vlan to work sudo ifconfig eth1 promisc sudo docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged --entrypoint /root/loxilb-io/loxilb/loxilb -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest # Create mac-vlan on top of underlying eth1 interface sudo docker network create -d macvlan -o parent=eth1 --subnet 192.168.82.0/24 --gateway 192.168.82.1 --aux-address 'host=192.168.82.252' llbnet # Assign mac-vlan to loxilb docker with specified IP (which will be used as LB VIP) sudo docker network connect llbnet loxilb --ip=192.168.82.100 # Add iptables rule to allow traffic from source IP(192.168.82.1) to loxilb sudo iptables -A DOCKER -s 192.168.82.1 -j ACCEPT","title":"Install loxilb docker"},{"location":"quick_start_with_cilium/#setup-k3s-with-cilium","text":"#K3s installation curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik --disable servicelb --disable-cloud-controller \\ --flannel-backend=none \\ --disable-network-policy\" sh - #Install Cilium CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/master/stable.txt) CLI_ARCH=amd64 if [ \"$(uname -m)\" = \"aarch64\" ]; then CLI_ARCH=arm64; fi curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum} sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum} mkdir -p ~/.kube/ sudo cat /etc/rancher/k3s/k3s.yaml > ~/.kube/config cilium install echo $MASTER_IP > /vagrant/master-ip sudo cp /var/lib/rancher/k3s/server/node-token /vagrant/node-token sudo cp /etc/rancher/k3s/k3s.yaml /vagrant/k3s.yaml sudo sed -i -e \"s/127.0.0.1/${MASTER_IP}/g\" /vagrant/k3s.yaml","title":"Setup K3s with cilium"},{"location":"quick_start_with_cilium/#how-to-deploy-kube-loxilb","text":"kube-loxilb is used to deploy loxilb with Kubernetes. wget https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/ext-cluster/kube-loxilb.yaml kube-loxilb.yaml args: - --loxiURL=http://172.17.0.2:11111 - --externalCIDR=192.168.82.100/32 - --setMode=1 In the above snippet, loxiURL uses docker interface IP of loxilb, which can be different for each setup. Apply in k8s: kubectl apply -f kube-loxilb.yaml","title":"How to deploy kube-loxilb ?"},{"location":"quick_start_with_cilium/#create-the-service","text":"kubectl apply -f https://raw.githubusercontent.com/loxilb-io/loxilb/main/cicd/docker-k3s-cilium/tcp-svc-lb.yml","title":"Create the service"},{"location":"quick_start_with_cilium/#check-the-status","text":"In k3s: kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 80m tcp-lb-onearm LoadBalancer 10.43.183.123 llb-192.168.82.100 56002:30001/TCP 6m50s In loxilb docker: $ sudo docker exec -it loxilb loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-----------------------|------|-----|--------|-----------|-------|--------|--------|----------| | 192.168.82.100 | | 56002 | tcp | default_tcp-lb-onearm | 0 | rr | onearm | 10.0.2.15 | 30001 | 1 | active | 12:880 |","title":"Check the status"},{"location":"quick_start_with_cilium/#connect-from-client","text":"$ curl http://192.168.82.100:56002 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> All of the above steps are also available as part of loxilb CICD workflow. Follow the steps below to replicate the above: $ cd cicd/docker-k3s-cilium/ # To setup the single node k3s setup with cilium as CNI and loxilb as external load balancer $ ./config.sh # To validate the results $ ./validation.sh # Cleanup $ ./rmconfig.sh","title":"Connect from client"},{"location":"requirements/","text":"LoxiLB system requirements To run loxilb, we need to have the following - Host OS requirements To install LoxiLB software packages, you need the 64-bit version of one of these OS versions: Ubuntu 20.04(LTS) Ubuntu 22.04(LTS) Fedora 36 RockyOS Enterprise Redhat (Planned) Windows Server(Planned) Kernel Requirements Linux Kernel Version >= 5.15.x && < 6.5.x Windows (Planned) Compatible Kubernetes Versions Kubernetes 1.19 ~ 1.29 (k0s, k3s, k8s, eks, openshift, kind etc) Hardware Requirements None as long as above criteria are met (2vcpu/2GB should be enough for starters)","title":"System Requirements"},{"location":"requirements/#loxilb-system-requirements","text":"To run loxilb, we need to have the following -","title":"LoxiLB system requirements"},{"location":"requirements/#host-os-requirements","text":"To install LoxiLB software packages, you need the 64-bit version of one of these OS versions: Ubuntu 20.04(LTS) Ubuntu 22.04(LTS) Fedora 36 RockyOS Enterprise Redhat (Planned) Windows Server(Planned)","title":"Host OS requirements"},{"location":"requirements/#kernel-requirements","text":"Linux Kernel Version >= 5.15.x && < 6.5.x Windows (Planned)","title":"Kernel Requirements"},{"location":"requirements/#compatible-kubernetes-versions","text":"Kubernetes 1.19 ~ 1.29 (k0s, k3s, k8s, eks, openshift, kind etc)","title":"Compatible Kubernetes Versions"},{"location":"requirements/#hardware-requirements","text":"None as long as above criteria are met (2vcpu/2GB should be enough for starters)","title":"Hardware Requirements"},{"location":"roadmap/","text":"Release Notes (For major release milestones) 0.7.0 beta (Aug, 2022) Initial release of loxilb Functional Features : Two-Arm Load-Balancer (NAT+Routed mode) Upto 16 end-points support Load-balancer selection policy Round-robin, traffic-hash (fallback to RR if hash fails) Conntrack support in eBPF - TCP/UDP/ICMP/SCTP profiles GTP with QFI extension support ULCL classifier support Native QoS-Policer support (SRTCM/TRTCM) GoBGP Integration Extended visibility and statistics LB Spec Support : IP allocation policy Kubernetes 1.20 base support Support for Calico Networking Utilities : loxicmd support : Configuration utlity with the look and feel of kubectl 0.8.0 (Dec, 2022) Functional Features : Enhanced load-balancer support including SCTP statefulness, WRR distribution Integrated Firewall support Integrated end-point health-checks One-ARM, FullNAT, DSR LB mode support NAT66/NAT64 support Clustering support Integration with Linux egress TC hooks LB Spec : Stand-alone mode to support LB Spec kube-loxilb Load-balancer class support Advanced IPAM for ipv4/ipv6 with shared/exclusive mode Kubernetes 1.25 Integration Utilities : loxicmd support : Data-Store support, more commands 0.9.0 (Nov, 2023) Functional Features : Hardened NAT Support - CGNAT'ish L3 DSR mode Support Https end-point liveness probes Maglev clustering SCTP multihoming support Integration with Linux native QoS Support for Cilium, Weave Networking Grafana based dashboard IPSEC Support (with VTI) Initial support for in-cluster mode kube-loxilb/LB Spec Support : OpenShift Integration Support for per-service liveness-checks, IPAM type, multi-homing annotations Kubernetes 1.26 (k0s, k3s, k8s ) Operator support AWS EKS support 0.9.3 (May, 2024) Functional Features : Kube-proxy replacement Support IPVS compatibility mode Master-plane HA support BFD and GARP support for Hitless HA Enhancements for Multus support SCTP multi-homing end-to-end Support Cloud Availability zone(s) Support Redhat9 and Ubuntu24 Support Support for upto Linux Kernel 6.8 Full Support for Oracle OCI SockAddr eBPF for LocalVIP access Container size enhancements HA enhancements for multiple cloud-providers and various scenarios (active-active, active-standby, clustered etc) CICD infra enhancements Robust secret management for HTTPS apis Performance enhancements with CT scaling Enhanced exception handling GoLang Profiling Support Full support for in-cluster mode Better support for virtio environments Enhanced RSS distribution mode via XDP (especially for SCTP workloads) Loadbalancer algorithms - LeastConnections and SessionAffinity added kube-loxilb Support : Kubernetes 1.29 BGP (auto) Mesh Support CRD for BGP peers Kubernetes GW-API Support Utilities : N4 pfcp test-tool added Seagull test tool integrated Massive updates to documentation 0.9.5 (Jul, 2024) - Planned Functional Features : SRv6 support Rolling upgrades L7 (Transparent) proxy HTTPS termination URL Filtering DNS caching Wireguard Support SIP protocol Support Policy based SNAT Kubernetes vCluster Support Rootless Container Support kube-loxilb Support : Kubernetes 1.30 Multi-cluster support CRD for BGP policies Kubernetes network policy support","title":"Development Roadmap"},{"location":"roadmap/#release-notes-for-major-release-milestones","text":"","title":"Release Notes (For major release milestones)"},{"location":"roadmap/#070-beta-aug-2022","text":"Initial release of loxilb Functional Features : Two-Arm Load-Balancer (NAT+Routed mode) Upto 16 end-points support Load-balancer selection policy Round-robin, traffic-hash (fallback to RR if hash fails) Conntrack support in eBPF - TCP/UDP/ICMP/SCTP profiles GTP with QFI extension support ULCL classifier support Native QoS-Policer support (SRTCM/TRTCM) GoBGP Integration Extended visibility and statistics LB Spec Support : IP allocation policy Kubernetes 1.20 base support Support for Calico Networking Utilities : loxicmd support : Configuration utlity with the look and feel of kubectl","title":"0.7.0 beta (Aug, 2022)"},{"location":"roadmap/#080-dec-2022","text":"Functional Features : Enhanced load-balancer support including SCTP statefulness, WRR distribution Integrated Firewall support Integrated end-point health-checks One-ARM, FullNAT, DSR LB mode support NAT66/NAT64 support Clustering support Integration with Linux egress TC hooks LB Spec : Stand-alone mode to support LB Spec kube-loxilb Load-balancer class support Advanced IPAM for ipv4/ipv6 with shared/exclusive mode Kubernetes 1.25 Integration Utilities : loxicmd support : Data-Store support, more commands","title":"0.8.0 (Dec, 2022)"},{"location":"roadmap/#090-nov-2023","text":"Functional Features : Hardened NAT Support - CGNAT'ish L3 DSR mode Support Https end-point liveness probes Maglev clustering SCTP multihoming support Integration with Linux native QoS Support for Cilium, Weave Networking Grafana based dashboard IPSEC Support (with VTI) Initial support for in-cluster mode kube-loxilb/LB Spec Support : OpenShift Integration Support for per-service liveness-checks, IPAM type, multi-homing annotations Kubernetes 1.26 (k0s, k3s, k8s ) Operator support AWS EKS support","title":"0.9.0 (Nov, 2023)"},{"location":"roadmap/#093-may-2024","text":"Functional Features : Kube-proxy replacement Support IPVS compatibility mode Master-plane HA support BFD and GARP support for Hitless HA Enhancements for Multus support SCTP multi-homing end-to-end Support Cloud Availability zone(s) Support Redhat9 and Ubuntu24 Support Support for upto Linux Kernel 6.8 Full Support for Oracle OCI SockAddr eBPF for LocalVIP access Container size enhancements HA enhancements for multiple cloud-providers and various scenarios (active-active, active-standby, clustered etc) CICD infra enhancements Robust secret management for HTTPS apis Performance enhancements with CT scaling Enhanced exception handling GoLang Profiling Support Full support for in-cluster mode Better support for virtio environments Enhanced RSS distribution mode via XDP (especially for SCTP workloads) Loadbalancer algorithms - LeastConnections and SessionAffinity added kube-loxilb Support : Kubernetes 1.29 BGP (auto) Mesh Support CRD for BGP peers Kubernetes GW-API Support Utilities : N4 pfcp test-tool added Seagull test tool integrated Massive updates to documentation","title":"0.9.3 (May, 2024)"},{"location":"roadmap/#095-jul-2024-planned","text":"Functional Features : SRv6 support Rolling upgrades L7 (Transparent) proxy HTTPS termination URL Filtering DNS caching Wireguard Support SIP protocol Support Policy based SNAT Kubernetes vCluster Support Rootless Container Support kube-loxilb Support : Kubernetes 1.30 Multi-cluster support CRD for BGP policies Kubernetes network policy support","title":"0.9.5 (Jul, 2024) - Planned"},{"location":"run/","text":"loxilb - How to build/run 1. Build from code and run (difficult) Install GoLang > v1.17 wget https://go.dev/dl/go1.22.0.linux-amd64.tar.gz && sudo tar -xzf go1.22.0.linux-amd64.tar.gz --directory /usr/local/ export PATH=\"${PATH}:/usr/local/go/bin\" Install standard packages sudo apt install -y clang llvm libelf-dev gcc-multilib libpcap-dev vim net-tools linux-tools-$(uname -r) elfutils dwarves git libbsd-dev bridge-utils wget unzip build-essential bison flex iproute2 curl Install loxilb eBPF loader tools curl -sfL https://github.com/loxilb-io/tools/raw/main/loader/install.sh | sh - Build and run loxilb git clone --recurse-submodules https://github.com/loxilb-io/loxilb.git cd loxilb ./loxilb-ebpf/utils/mkllb_bpffs.sh make sudo ./loxilb Build and use loxicmd git clone https://github.com/loxilb-io/loxicmd.git cd loxicmd go get . make sudo cp -f loxicmd /usr/local/sbin/ loxicmd usage guide can be found here 2. Build and run using docker (easy) Build the docker image git clone --recurse-submodules https://github.com/loxilb-io/loxilb.git cd loxilb make docker This would create the docker image ghcr.io/loxilb-io/loxilb:latest locally. One can then run loxilb in standalone mode by following guide here 3. Running in Kubernetes For running in K8s environment, kindly follow kube-loxilb guide","title":"Manual steps to build/run"},{"location":"run/#loxilb-how-to-buildrun","text":"","title":"loxilb - How to build/run"},{"location":"run/#1-build-from-code-and-run-difficult","text":"Install GoLang > v1.17 wget https://go.dev/dl/go1.22.0.linux-amd64.tar.gz && sudo tar -xzf go1.22.0.linux-amd64.tar.gz --directory /usr/local/ export PATH=\"${PATH}:/usr/local/go/bin\" Install standard packages sudo apt install -y clang llvm libelf-dev gcc-multilib libpcap-dev vim net-tools linux-tools-$(uname -r) elfutils dwarves git libbsd-dev bridge-utils wget unzip build-essential bison flex iproute2 curl Install loxilb eBPF loader tools curl -sfL https://github.com/loxilb-io/tools/raw/main/loader/install.sh | sh - Build and run loxilb git clone --recurse-submodules https://github.com/loxilb-io/loxilb.git cd loxilb ./loxilb-ebpf/utils/mkllb_bpffs.sh make sudo ./loxilb Build and use loxicmd git clone https://github.com/loxilb-io/loxicmd.git cd loxicmd go get . make sudo cp -f loxicmd /usr/local/sbin/ loxicmd usage guide can be found here","title":"1. Build from code and run (difficult)"},{"location":"run/#2-build-and-run-using-docker-easy","text":"Build the docker image git clone --recurse-submodules https://github.com/loxilb-io/loxilb.git cd loxilb make docker This would create the docker image ghcr.io/loxilb-io/loxilb:latest locally. One can then run loxilb in standalone mode by following guide here","title":"2. Build and run using docker (easy)"},{"location":"run/#3-running-in-kubernetes","text":"For running in K8s environment, kindly follow kube-loxilb guide","title":"3. Running in Kubernetes"},{"location":"service-proxy-calico/","text":"Quick Start Guide - K3s, LoxiLB \"service-proxy\" and Calico This document will explain how to install a K3s cluster with loxilb in \"service-proxy\" mode alongside calico networking. What is service-proxy mode? service-proxy mode is where kubernetes kube-proxy services are entirely replaced by loxilb for better performance. Users can continue to use their existing networking providers while enjoying streamlined performance and superior feature-set provided by loxilb. Looking at the left side of the image, you will notice the traffic flow of the packet as it enters the Kubernetes cluster. Kube-proxy, the de-facto networking agent in the Kubernetes which runs on each node of the cluster which monitors the services and translates them to either iptables or IPVS tangible rules. If we talk about the functionality or a cluster with low volume traffic then kube-proxy is fine but when it comes to scalability or a high volume traffic then it acts as a bottle-neck. loxilb \"service-proxy\" mode works with Flannel/Calico and kube-proxy in IPVS mode only as of now. It inherits the IPVS rules and imports these in it's in-kernel eBPF implementation. Traffic will reach at the interface, will be processed by eBPF and sent directly to the pod or to the other node, bypassing all the layers of Linux networking. This way, all the services, be it External, NodePort or ClusterIP, can be managed through LoxiLB and provide optimal performance for the users. The added benefit for the user's is the fact that there is no need to rip and replace their current networking provider (e.g flannel or calico). Kindly note that Kubernetes network policies can't be supported in this miode currently. Topology For quickly bringing up loxilb \"service-proxy\" in K3s with Calico, we will be deploying a single node k3s cluster (v1.29.3+k3s1) : loxilb and kube-loxilb components run as pods managed by kubernetes in this scenario. Setup K3s Configure K3s node $ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik \\ --disable servicelb --disable-cloud-controller --kube-proxy-arg proxy-mode=ipvs \\ cloud-provider=external --flannel-backend=none --disable-network-policy --cluster-cidr=10.42.0.0/16 \\ --node-ip=${MASTER_IP} --node-external-ip=${MASTER_IP} \\ --bind-address=${MASTER_IP}\" sh - Deploy calico K3s uses by default flannel for networking but here we are using calico to provide the same: sudo kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml sudo kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yaml Deploy kube-loxilb and loxilb ? kube-loxilb is used as an operator to manage loxilb. We need to deploy both kube-loxilb and loxilb components in your kubernetes cluster sudo kubectl apply -f https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/service-proxy/kube-loxilb.yml sudo kubectl apply -f https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/service-proxy/loxilb-service-proxy.yml Check the status In k3s node: ## Check the pods created $ sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE tigera-operator tigera-operator-689d868448-wwvts 1/1 Running 0 2d23h calico-system calico-typha-67d4484996-2cmzs 1/1 Running 0 2d23h calico-system calico-node-l8r8b 1/1 Running 0 2d23h kube-system local-path-provisioner-6c86858495-mrtzv 1/1 Running 0 2d23h calico-system csi-node-driver-ssbnf 2/2 Running 0 2d23h calico-apiserver calico-apiserver-7dccc79b59-txnl5 1/1 Running 0 2d10h calico-apiserver calico-apiserver-7dccc79b59-vk68t 1/1 Running 0 2d10h calico-system calico-node-glm64 1/1 Running 0 2d23h calico-system calico-node-hs7pw 1/1 Running 0 2d23h calico-system csi-node-driver-xqjcd 2/2 Running 0 2d23h calico-system calico-typha-67d4484996-wctwv 1/1 Running 0 2d23h kube-system kube-loxilb-5fb5566999-4vvls 1/1 Running 0 38h calico-system csi-node-driver-hz87c 2/2 Running 0 2d23h kube-system coredns-6799fbcd5-mhgwg 1/1 Running 0 2d8h calico-system calico-kube-controllers-f5c6cdbdc-vztls 1/1 Running 0 32h calico-system calico-node-mjjs5 1/1 Running 0 2d23h calico-system csi-node-driver-l5r75 2/2 Running 0 2d23h default iperf1 1/1 Running 0 32h kube-system metrics-server-54fd9b65b-78mwr 1/1 Running 0 2d23h kube-system loxilb-lb-px6th 1/1 Running 0 20h In loxilb pod, we can check internal LB rules: $ sudo kubectl exec -it -n kube-system loxilb-lb-px6th -- loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-------------------------------|------|-----|---------|-----------------|-------|--------|--------|----------| | 10.0.2.15 | | 32598 | tcp | ipvs_10.0.2.15:32598-tcp | 0 | rr | fullnat | 192.168.235.161 | 5001 | 1 | active | 0:0 | | 10.43.0.10 | | 53 | tcp | ipvs_10.43.0.10:53-tcp | 0 | rr | default | 192.168.182.39 | 53 | 1 | - | 0:0 | | 10.43.0.10 | | 53 | udp | ipvs_10.43.0.10:53-udp | 0 | rr | default | 192.168.182.39 | 53 | 1 | - | 6:1149 | | 10.43.0.10 | | 9153 | tcp | ipvs_10.43.0.10:9153-tcp | 0 | rr | default | 192.168.182.39 | 9153 | 1 | - | 0:0 | | 10.43.0.1 | | 443 | tcp | ipvs_10.43.0.1:443-tcp | 0 | rr | default | 192.168.80.10 | 6443 | 1 | - | 0:0 | | 10.43.182.250 | | 443 | tcp | ipvs_10.43.182.250:443-tcp | 0 | rr | default | 192.168.182.14 | 5443 | 1 | - | 0:0 | | | | | | | | | | 192.168.189.75 | 5443 | 1 | - | 0:0 | | 10.43.184.155 | | 55001 | tcp | ipvs_10.43.184.155:55001-tcp | 0 | rr | default | 192.168.235.161 | 5001 | 1 | - | 0:0 | | 10.43.78.171 | | 5473 | tcp | ipvs_10.43.78.171:5473-tcp | 0 | rr | default | 192.168.80.10 | 5473 | 1 | - | 0:0 | | | | | | | | | | 192.168.80.102 | 5473 | 1 | - | 0:0 | | 10.43.89.40 | | 443 | tcp | ipvs_10.43.89.40:443-tcp | 0 | rr | default | 192.168.219.68 | 10250 | 1 | - | 0:0 | | 192.168.219.64 | | 32598 | tcp | ipvs_192.168.219.64:32598-tcp | 0 | rr | fullnat | 192.168.235.161 | 5001 | 1 | active | 0:0 | | 192.168.80.10 | | 32598 | tcp | ipvs_192.168.80.10:32598-tcp | 0 | rr | fullnat | 192.168.235.161 | 5001 | 1 | active | 0:0 | | 192.168.80.20 | | 32598 | tcp | ipvs_192.168.80.20:32598-tcp | 0 | rr | fullnat | 192.168.235.161 | 5001 | 1 | active | 0:0 | | 192.168.80.20 | | 55001 | tcp | default_iperf-service | 0 | rr | onearm | 192.168.80.101 | 32598 | 1 | - | 0:0 | Deploy a sample service To deploy a sample service, we can create service as usual in Kubernetes with few extra annotations as follows : sudo kubectl apply -f - <<EOF apiVersion: v1 kind: Service metadata: name: iperf-service annotations: loxilb.io/lbmode: \"onearm\" loxilb.io/staticIP: \"192.168.80.20\" spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: perf-test ports: - port: 55001 targetPort: 5001 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: iperf1 labels: what: perf-test spec: containers: - name: iperf image: ghcr.io/nicolaka/netshoot:latest command: - iperf - \"-s\" ports: - containerPort: 5001 EOF Check the service created : $ sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 3d1h iperf-service LoadBalancer 10.43.131.107 llb-192.168.80.20 55001:31181/TCP 9m14s Test the service created (from a host outside the cluster) : ## Using service VIP $ iperf -c 192.168.80.20 -p 55001 -i1 -t3 ------------------------------------------------------------ Client connecting to 192.168.80.20, TCP port 55001 TCP window size: 85.0 KByte (default) ------------------------------------------------------------ [ 1] local 192.168.80.80 port 58936 connected with 192.168.80.20 port 55001 [ ID] Interval Transfer Bandwidth [ 1] 0.0000-1.0000 sec 282 MBytes 2.36 Gbits/sec [ 1] 1.0000-2.0000 sec 276 MBytes 2.31 Gbits/sec [ 1] 2.0000-3.0000 sec 279 MBytes 2.34 Gbits/sec ## Using node-port $ iperf -c 192.168.80.100 -p 31181 -i1 -t10 ------------------------------------------------------------ Client connecting to 192.168.80.100, TCP port 31181 TCP window size: 85.0 KByte (default) ------------------------------------------------------------ [ 1] local 192.168.80.80 port 43208 connected with 192.168.80.100 port 31181 [ ID] Interval Transfer Bandwidth [ 1] 0.0000-1.0000 sec 612 MBytes 5.14 Gbits/sec [ 1] 1.0000-2.0000 sec 598 MBytes 5.02 Gbits/sec [ 1] 2.0000-3.0000 sec 617 MBytes 5.17 Gbits/sec [ 1] 3.0000-4.0000 sec 600 MBytes 5.04 Gbits/sec [ 1] 4.0000-5.0000 sec 630 MBytes 5.28 Gbits/sec [ 1] 5.0000-6.0000 sec 699 MBytes 5.86 Gbits/sec [ 1] 6.0000-7.0000 sec 682 MBytes 5.72 Gbits/sec For more detailed performance comparison with other solutions, kindly follow this blog and for more detailed information on incluster deployment of loxilb with bgp in a full-blown cluster, kindly follow this blog .","title":"service-proxy - K3s/loxilb service-proxy with calico"},{"location":"service-proxy-calico/#quick-start-guide-k3s-loxilb-service-proxy-and-calico","text":"This document will explain how to install a K3s cluster with loxilb in \"service-proxy\" mode alongside calico networking.","title":"Quick Start Guide - K3s, LoxiLB \"service-proxy\" and Calico"},{"location":"service-proxy-calico/#what-is-service-proxy-mode","text":"service-proxy mode is where kubernetes kube-proxy services are entirely replaced by loxilb for better performance. Users can continue to use their existing networking providers while enjoying streamlined performance and superior feature-set provided by loxilb. Looking at the left side of the image, you will notice the traffic flow of the packet as it enters the Kubernetes cluster. Kube-proxy, the de-facto networking agent in the Kubernetes which runs on each node of the cluster which monitors the services and translates them to either iptables or IPVS tangible rules. If we talk about the functionality or a cluster with low volume traffic then kube-proxy is fine but when it comes to scalability or a high volume traffic then it acts as a bottle-neck. loxilb \"service-proxy\" mode works with Flannel/Calico and kube-proxy in IPVS mode only as of now. It inherits the IPVS rules and imports these in it's in-kernel eBPF implementation. Traffic will reach at the interface, will be processed by eBPF and sent directly to the pod or to the other node, bypassing all the layers of Linux networking. This way, all the services, be it External, NodePort or ClusterIP, can be managed through LoxiLB and provide optimal performance for the users. The added benefit for the user's is the fact that there is no need to rip and replace their current networking provider (e.g flannel or calico). Kindly note that Kubernetes network policies can't be supported in this miode currently.","title":"What is service-proxy mode?"},{"location":"service-proxy-calico/#topology","text":"For quickly bringing up loxilb \"service-proxy\" in K3s with Calico, we will be deploying a single node k3s cluster (v1.29.3+k3s1) : loxilb and kube-loxilb components run as pods managed by kubernetes in this scenario.","title":"Topology"},{"location":"service-proxy-calico/#setup-k3s","text":"","title":"Setup K3s"},{"location":"service-proxy-calico/#configure-k3s-node","text":"$ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik \\ --disable servicelb --disable-cloud-controller --kube-proxy-arg proxy-mode=ipvs \\ cloud-provider=external --flannel-backend=none --disable-network-policy --cluster-cidr=10.42.0.0/16 \\ --node-ip=${MASTER_IP} --node-external-ip=${MASTER_IP} \\ --bind-address=${MASTER_IP}\" sh -","title":"Configure K3s node"},{"location":"service-proxy-calico/#deploy-calico","text":"K3s uses by default flannel for networking but here we are using calico to provide the same: sudo kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml sudo kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yaml","title":"Deploy calico"},{"location":"service-proxy-calico/#deploy-kube-loxilb-and-loxilb","text":"kube-loxilb is used as an operator to manage loxilb. We need to deploy both kube-loxilb and loxilb components in your kubernetes cluster sudo kubectl apply -f https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/service-proxy/kube-loxilb.yml sudo kubectl apply -f https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/service-proxy/loxilb-service-proxy.yml","title":"Deploy kube-loxilb and loxilb ?"},{"location":"service-proxy-calico/#check-the-status","text":"In k3s node: ## Check the pods created $ sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE tigera-operator tigera-operator-689d868448-wwvts 1/1 Running 0 2d23h calico-system calico-typha-67d4484996-2cmzs 1/1 Running 0 2d23h calico-system calico-node-l8r8b 1/1 Running 0 2d23h kube-system local-path-provisioner-6c86858495-mrtzv 1/1 Running 0 2d23h calico-system csi-node-driver-ssbnf 2/2 Running 0 2d23h calico-apiserver calico-apiserver-7dccc79b59-txnl5 1/1 Running 0 2d10h calico-apiserver calico-apiserver-7dccc79b59-vk68t 1/1 Running 0 2d10h calico-system calico-node-glm64 1/1 Running 0 2d23h calico-system calico-node-hs7pw 1/1 Running 0 2d23h calico-system csi-node-driver-xqjcd 2/2 Running 0 2d23h calico-system calico-typha-67d4484996-wctwv 1/1 Running 0 2d23h kube-system kube-loxilb-5fb5566999-4vvls 1/1 Running 0 38h calico-system csi-node-driver-hz87c 2/2 Running 0 2d23h kube-system coredns-6799fbcd5-mhgwg 1/1 Running 0 2d8h calico-system calico-kube-controllers-f5c6cdbdc-vztls 1/1 Running 0 32h calico-system calico-node-mjjs5 1/1 Running 0 2d23h calico-system csi-node-driver-l5r75 2/2 Running 0 2d23h default iperf1 1/1 Running 0 32h kube-system metrics-server-54fd9b65b-78mwr 1/1 Running 0 2d23h kube-system loxilb-lb-px6th 1/1 Running 0 20h In loxilb pod, we can check internal LB rules: $ sudo kubectl exec -it -n kube-system loxilb-lb-px6th -- loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-------------------------------|------|-----|---------|-----------------|-------|--------|--------|----------| | 10.0.2.15 | | 32598 | tcp | ipvs_10.0.2.15:32598-tcp | 0 | rr | fullnat | 192.168.235.161 | 5001 | 1 | active | 0:0 | | 10.43.0.10 | | 53 | tcp | ipvs_10.43.0.10:53-tcp | 0 | rr | default | 192.168.182.39 | 53 | 1 | - | 0:0 | | 10.43.0.10 | | 53 | udp | ipvs_10.43.0.10:53-udp | 0 | rr | default | 192.168.182.39 | 53 | 1 | - | 6:1149 | | 10.43.0.10 | | 9153 | tcp | ipvs_10.43.0.10:9153-tcp | 0 | rr | default | 192.168.182.39 | 9153 | 1 | - | 0:0 | | 10.43.0.1 | | 443 | tcp | ipvs_10.43.0.1:443-tcp | 0 | rr | default | 192.168.80.10 | 6443 | 1 | - | 0:0 | | 10.43.182.250 | | 443 | tcp | ipvs_10.43.182.250:443-tcp | 0 | rr | default | 192.168.182.14 | 5443 | 1 | - | 0:0 | | | | | | | | | | 192.168.189.75 | 5443 | 1 | - | 0:0 | | 10.43.184.155 | | 55001 | tcp | ipvs_10.43.184.155:55001-tcp | 0 | rr | default | 192.168.235.161 | 5001 | 1 | - | 0:0 | | 10.43.78.171 | | 5473 | tcp | ipvs_10.43.78.171:5473-tcp | 0 | rr | default | 192.168.80.10 | 5473 | 1 | - | 0:0 | | | | | | | | | | 192.168.80.102 | 5473 | 1 | - | 0:0 | | 10.43.89.40 | | 443 | tcp | ipvs_10.43.89.40:443-tcp | 0 | rr | default | 192.168.219.68 | 10250 | 1 | - | 0:0 | | 192.168.219.64 | | 32598 | tcp | ipvs_192.168.219.64:32598-tcp | 0 | rr | fullnat | 192.168.235.161 | 5001 | 1 | active | 0:0 | | 192.168.80.10 | | 32598 | tcp | ipvs_192.168.80.10:32598-tcp | 0 | rr | fullnat | 192.168.235.161 | 5001 | 1 | active | 0:0 | | 192.168.80.20 | | 32598 | tcp | ipvs_192.168.80.20:32598-tcp | 0 | rr | fullnat | 192.168.235.161 | 5001 | 1 | active | 0:0 | | 192.168.80.20 | | 55001 | tcp | default_iperf-service | 0 | rr | onearm | 192.168.80.101 | 32598 | 1 | - | 0:0 |","title":"Check the status"},{"location":"service-proxy-calico/#deploy-a-sample-service","text":"To deploy a sample service, we can create service as usual in Kubernetes with few extra annotations as follows : sudo kubectl apply -f - <<EOF apiVersion: v1 kind: Service metadata: name: iperf-service annotations: loxilb.io/lbmode: \"onearm\" loxilb.io/staticIP: \"192.168.80.20\" spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: perf-test ports: - port: 55001 targetPort: 5001 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: iperf1 labels: what: perf-test spec: containers: - name: iperf image: ghcr.io/nicolaka/netshoot:latest command: - iperf - \"-s\" ports: - containerPort: 5001 EOF Check the service created : $ sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 3d1h iperf-service LoadBalancer 10.43.131.107 llb-192.168.80.20 55001:31181/TCP 9m14s Test the service created (from a host outside the cluster) : ## Using service VIP $ iperf -c 192.168.80.20 -p 55001 -i1 -t3 ------------------------------------------------------------ Client connecting to 192.168.80.20, TCP port 55001 TCP window size: 85.0 KByte (default) ------------------------------------------------------------ [ 1] local 192.168.80.80 port 58936 connected with 192.168.80.20 port 55001 [ ID] Interval Transfer Bandwidth [ 1] 0.0000-1.0000 sec 282 MBytes 2.36 Gbits/sec [ 1] 1.0000-2.0000 sec 276 MBytes 2.31 Gbits/sec [ 1] 2.0000-3.0000 sec 279 MBytes 2.34 Gbits/sec ## Using node-port $ iperf -c 192.168.80.100 -p 31181 -i1 -t10 ------------------------------------------------------------ Client connecting to 192.168.80.100, TCP port 31181 TCP window size: 85.0 KByte (default) ------------------------------------------------------------ [ 1] local 192.168.80.80 port 43208 connected with 192.168.80.100 port 31181 [ ID] Interval Transfer Bandwidth [ 1] 0.0000-1.0000 sec 612 MBytes 5.14 Gbits/sec [ 1] 1.0000-2.0000 sec 598 MBytes 5.02 Gbits/sec [ 1] 2.0000-3.0000 sec 617 MBytes 5.17 Gbits/sec [ 1] 3.0000-4.0000 sec 600 MBytes 5.04 Gbits/sec [ 1] 4.0000-5.0000 sec 630 MBytes 5.28 Gbits/sec [ 1] 5.0000-6.0000 sec 699 MBytes 5.86 Gbits/sec [ 1] 6.0000-7.0000 sec 682 MBytes 5.72 Gbits/sec For more detailed performance comparison with other solutions, kindly follow this blog and for more detailed information on incluster deployment of loxilb with bgp in a full-blown cluster, kindly follow this blog .","title":"Deploy a sample service"},{"location":"service-proxy-flannel/","text":"Quick Start Guide - K3s with LoxiLB \"service-proxy\" This document will explain how to install a K3s cluster with loxilb in \"service-proxy\" mode alongside flannel networking (default for k3s). What is service-proxy mode? service-proxy mode is where kubernetes kube-proxy services are entirely replaced by loxilb for better performance. Users can continue to use their existing networking providers while enjoying streamlined performance and superior feature-set provided by loxilb. Looking at the left side of the image, you will notice the traffic flow of the packet as it enters the Kubernetes cluster. Kube-proxy, the de-facto networking agent in the Kubernetes which runs on each node of the cluster which monitors the services and translates them to either iptables or IPVS tangible rules. If we talk about the functionality or a cluster with low volume traffic then kube-proxy is fine but when it comes to scalability or a high volume traffic then it acts as a bottle-neck. loxilb \"service-proxy\" mode works with Flannel/Calico and kube-proxy in IPVS mode only as of now. It inherits the IPVS rules and imports these in it's in-kernel eBPF implementation. Traffic will reach at the interface, will be processed by eBPF and sent directly to the pod or to the other node, bypassing all the layers of Linux networking. This way, all the services, be it External, NodePort or ClusterIP, can be managed through LoxiLB and provide optimal performance for the users. The added benefit for the user's is the fact that there is no need to rip and replace their current networking provider (e.g flannel or calico). Topology For quickly bringing up loxilb \"service-proxy\" in K3s, we will be deploying a single node k3s cluster (v1.29.3+k3s1) : loxilb and kube-loxilb components run as pods managed by kubernetes in this scenario. Setup K3s Configure K3s node $ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik \\ --disable servicelb --disable-cloud-controller --kube-proxy-arg proxy-mode=ipvs \\ cloud-provider=external --node-ip=${MASTER_IP} --node-external-ip=${MASTER_IP} \\ --bind-address=${MASTER_IP}\" sh - Deploy kube-loxilb and loxilb ? kube-loxilb is used as an operator to manage loxilb. We need to deploy both kube-loxilb and loxilb components in your kubernetes cluster sudo kubectl apply -f https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/service-proxy/kube-loxilb.yml sudo kubectl apply -f https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/service-proxy/loxilb-service-proxy.yml Check the status In k3s node: ## Check the pods created $ sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-6799fbcd5-c68ws 1/1 Running 0 15m kube-system local-path-provisioner-6c86858495-rxk2w 1/1 Running 0 15m kube-system metrics-server-54fd9b65b-xtgk2 1/1 Running 0 15m kube-system loxilb-lb-5p6pg 1/1 Running 0 6m58s kube-system kube-loxilb-5fb5566999-7xdkk 1/1 Running 0 6m59s In loxilb pod, we can check internal LB rules: $ udo kubectl exec -it -n kube-system loxilb-lb-5p6pg -- loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-------------------------------|------|-----|---------|----------------|-------|--------|--------|----------| | 10.0.2.15 | | 31377 | tcp | ipvs_10.0.2.15:31377-tcp | 0 | rr | fullnat | 10.42.1.2 | 5001 | 1 | active | 0:0 | | 10.42.1.0 | | 31377 | tcp | ipvs_10.42.1.0:31377-tcp | 0 | rr | fullnat | 10.42.1.2 | 5001 | 1 | active | 0:0 | | 10.42.1.1 | | 31377 | tcp | ipvs_10.42.1.1:31377-tcp | 0 | rr | fullnat | 10.42.1.2 | 5001 | 1 | active | 0:0 | | 10.43.0.10 | | 53 | tcp | ipvs_10.43.0.10:53-tcp | 0 | rr | default | 10.42.0.3 | 53 | 1 | - | 0:0 | | 10.43.0.10 | | 53 | udp | ipvs_10.43.0.10:53-udp | 0 | rr | default | 10.42.0.3 | 53 | 1 | - | 0:0 | | 10.43.0.10 | | 9153 | tcp | ipvs_10.43.0.10:9153-tcp | 0 | rr | default | 10.42.0.3 | 9153 | 1 | - | 0:0 | | 10.43.0.1 | | 443 | tcp | ipvs_10.43.0.1:443-tcp | 0 | rr | default | 192.168.80.10 | 6443 | 1 | - | 0:0 | | 10.43.202.90 | | 55001 | tcp | ipvs_10.43.202.90:55001-tcp | 0 | rr | default | 10.42.1.2 | 5001 | 1 | - | 0:0 | | 10.43.30.93 | | 443 | tcp | ipvs_10.43.30.93:443-tcp | 0 | rr | default | 10.42.0.4 | 10250 | 1 | - | 0:0 | | 192.168.80.101 | | 31377 | tcp | ipvs_192.168.80.101:31377-tcp | 0 | rr | fullnat | 10.42.1.2 | 5001 | 1 | active | 15:1014 | | 192.168.80.20 | | 55001 | tcp | default_iperf-service | 0 | rr | onearm | 192.168.80.101 | 31377 | 1 | - | 0:0 | Deploy a sample service To deploy a sample service, we can create service as usual in Kubernetes with few extra annotations as follows : sudo kubectl apply -f - <<EOF apiVersion: v1 kind: Service metadata: name: iperf-service annotations: loxilb.io/lbmode: \"onearm\" loxilb.io/staticIP: \"192.168.80.20\" spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: perf-test ports: - port: 55001 targetPort: 5001 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: iperf1 labels: what: perf-test spec: containers: - name: iperf image: ghcr.io/nicolaka/netshoot:latest command: - iperf - \"-s\" ports: - containerPort: 5001 EOF Check the service created : $ sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 17m iperf-service LoadBalancer 10.43.202.90 llb-192.168.80.20 55001:31377/TCP 2m34s Test the service created (from a host outside the cluster) : ## Using service VIP $ iperf -c 192.168.80.20 -p 55001 -i1 -t3 ------------------------------------------------------------ Client connecting to 192.168.80.20, TCP port 55001 TCP window size: 85.0 KByte (default) ------------------------------------------------------------ [ 1] local 192.168.80.80 port 55686 connected with 192.168.80.20 port 55001 [ ID] Interval Transfer Bandwidth [ 1] 0.0000-1.0000 sec 311 MBytes 2.61 Gbits/sec [ 1] 1.0000-2.0000 sec 309 MBytes 2.59 Gbits/sec [ 1] 2.0000-3.0000 sec 305 MBytes 2.56 Gbits/sec [ 1] 0.0000-3.0109 sec 926 MBytes 2.58 Gbits/sec ## Using node-port $ iperf -c 192.168.80.101 -p 31377 -i1 -t10 ------------------------------------------------------------ Client connecting to 192.168.80.101, TCP port 31377 TCP window size: 85.0 KByte (default) ------------------------------------------------------------ [ 1] local 192.168.80.80 port 34066 connected with 192.168.80.101 port 31377 [ ID] Interval Transfer Bandwidth [ 1] 0.0000-1.0000 sec 792 MBytes 6.64 Gbits/sec [ 1] 1.0000-2.0000 sec 727 MBytes 6.10 Gbits/sec [ 1] 2.0000-3.0000 sec 784 MBytes 6.57 Gbits/sec [ 1] 3.0000-4.0000 sec 814 MBytes 6.83 Gbits/sec [ 1] 4.0000-5.0000 sec 1.01 GBytes 8.64 Gbits/sec [ 1] 5.0000-6.0000 sec 1.02 GBytes 8.79 Gbits/sec [ 1] 6.0000-7.0000 sec 1.03 GBytes 8.84 Gbits/sec [ 1] 7.0000-8.0000 sec 814 MBytes 6.83 Gbits/sec [ 1] 8.0000-9.0000 sec 965 MBytes 8.09 Gbits/sec [ 1] 9.0000-10.0000 sec 946 MBytes 7.93 Gbits/sec [ 1] 0.0000-10.0170 sec 8.76 GBytes 7.51 Gbits/sec If you are wondering why there is a performance difference between serviceLB and node-port, there is an interesting blog about it here by one our users. For more detailed performance comparison with other solutions, kindly follow this blog and for more detailed information on incluster deployment of loxilb with bgp in a full-blown cluster, kindly follow this blog .","title":"service-proxy - K3s/loxilb service-proxy with flannel"},{"location":"service-proxy-flannel/#quick-start-guide-k3s-with-loxilb-service-proxy","text":"This document will explain how to install a K3s cluster with loxilb in \"service-proxy\" mode alongside flannel networking (default for k3s).","title":"Quick Start Guide - K3s with LoxiLB \"service-proxy\""},{"location":"service-proxy-flannel/#what-is-service-proxy-mode","text":"service-proxy mode is where kubernetes kube-proxy services are entirely replaced by loxilb for better performance. Users can continue to use their existing networking providers while enjoying streamlined performance and superior feature-set provided by loxilb. Looking at the left side of the image, you will notice the traffic flow of the packet as it enters the Kubernetes cluster. Kube-proxy, the de-facto networking agent in the Kubernetes which runs on each node of the cluster which monitors the services and translates them to either iptables or IPVS tangible rules. If we talk about the functionality or a cluster with low volume traffic then kube-proxy is fine but when it comes to scalability or a high volume traffic then it acts as a bottle-neck. loxilb \"service-proxy\" mode works with Flannel/Calico and kube-proxy in IPVS mode only as of now. It inherits the IPVS rules and imports these in it's in-kernel eBPF implementation. Traffic will reach at the interface, will be processed by eBPF and sent directly to the pod or to the other node, bypassing all the layers of Linux networking. This way, all the services, be it External, NodePort or ClusterIP, can be managed through LoxiLB and provide optimal performance for the users. The added benefit for the user's is the fact that there is no need to rip and replace their current networking provider (e.g flannel or calico).","title":"What is service-proxy mode?"},{"location":"service-proxy-flannel/#topology","text":"For quickly bringing up loxilb \"service-proxy\" in K3s, we will be deploying a single node k3s cluster (v1.29.3+k3s1) : loxilb and kube-loxilb components run as pods managed by kubernetes in this scenario.","title":"Topology"},{"location":"service-proxy-flannel/#setup-k3s","text":"","title":"Setup K3s"},{"location":"service-proxy-flannel/#configure-k3s-node","text":"$ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik \\ --disable servicelb --disable-cloud-controller --kube-proxy-arg proxy-mode=ipvs \\ cloud-provider=external --node-ip=${MASTER_IP} --node-external-ip=${MASTER_IP} \\ --bind-address=${MASTER_IP}\" sh -","title":"Configure K3s node"},{"location":"service-proxy-flannel/#deploy-kube-loxilb-and-loxilb","text":"kube-loxilb is used as an operator to manage loxilb. We need to deploy both kube-loxilb and loxilb components in your kubernetes cluster sudo kubectl apply -f https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/service-proxy/kube-loxilb.yml sudo kubectl apply -f https://raw.githubusercontent.com/loxilb-io/kube-loxilb/main/manifest/service-proxy/loxilb-service-proxy.yml","title":"Deploy kube-loxilb and loxilb ?"},{"location":"service-proxy-flannel/#check-the-status","text":"In k3s node: ## Check the pods created $ sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-6799fbcd5-c68ws 1/1 Running 0 15m kube-system local-path-provisioner-6c86858495-rxk2w 1/1 Running 0 15m kube-system metrics-server-54fd9b65b-xtgk2 1/1 Running 0 15m kube-system loxilb-lb-5p6pg 1/1 Running 0 6m58s kube-system kube-loxilb-5fb5566999-7xdkk 1/1 Running 0 6m59s In loxilb pod, we can check internal LB rules: $ udo kubectl exec -it -n kube-system loxilb-lb-5p6pg -- loxicmd get lb -o wide | EXT IP | SEC IPS | PORT | PROTO | NAME | MARK | SEL | MODE | ENDPOINT | EPORT | WEIGHT | STATE | COUNTERS | |----------------|---------|-------|-------|-------------------------------|------|-----|---------|----------------|-------|--------|--------|----------| | 10.0.2.15 | | 31377 | tcp | ipvs_10.0.2.15:31377-tcp | 0 | rr | fullnat | 10.42.1.2 | 5001 | 1 | active | 0:0 | | 10.42.1.0 | | 31377 | tcp | ipvs_10.42.1.0:31377-tcp | 0 | rr | fullnat | 10.42.1.2 | 5001 | 1 | active | 0:0 | | 10.42.1.1 | | 31377 | tcp | ipvs_10.42.1.1:31377-tcp | 0 | rr | fullnat | 10.42.1.2 | 5001 | 1 | active | 0:0 | | 10.43.0.10 | | 53 | tcp | ipvs_10.43.0.10:53-tcp | 0 | rr | default | 10.42.0.3 | 53 | 1 | - | 0:0 | | 10.43.0.10 | | 53 | udp | ipvs_10.43.0.10:53-udp | 0 | rr | default | 10.42.0.3 | 53 | 1 | - | 0:0 | | 10.43.0.10 | | 9153 | tcp | ipvs_10.43.0.10:9153-tcp | 0 | rr | default | 10.42.0.3 | 9153 | 1 | - | 0:0 | | 10.43.0.1 | | 443 | tcp | ipvs_10.43.0.1:443-tcp | 0 | rr | default | 192.168.80.10 | 6443 | 1 | - | 0:0 | | 10.43.202.90 | | 55001 | tcp | ipvs_10.43.202.90:55001-tcp | 0 | rr | default | 10.42.1.2 | 5001 | 1 | - | 0:0 | | 10.43.30.93 | | 443 | tcp | ipvs_10.43.30.93:443-tcp | 0 | rr | default | 10.42.0.4 | 10250 | 1 | - | 0:0 | | 192.168.80.101 | | 31377 | tcp | ipvs_192.168.80.101:31377-tcp | 0 | rr | fullnat | 10.42.1.2 | 5001 | 1 | active | 15:1014 | | 192.168.80.20 | | 55001 | tcp | default_iperf-service | 0 | rr | onearm | 192.168.80.101 | 31377 | 1 | - | 0:0 |","title":"Check the status"},{"location":"service-proxy-flannel/#deploy-a-sample-service","text":"To deploy a sample service, we can create service as usual in Kubernetes with few extra annotations as follows : sudo kubectl apply -f - <<EOF apiVersion: v1 kind: Service metadata: name: iperf-service annotations: loxilb.io/lbmode: \"onearm\" loxilb.io/staticIP: \"192.168.80.20\" spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: perf-test ports: - port: 55001 targetPort: 5001 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: iperf1 labels: what: perf-test spec: containers: - name: iperf image: ghcr.io/nicolaka/netshoot:latest command: - iperf - \"-s\" ports: - containerPort: 5001 EOF Check the service created : $ sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 <none> 443/TCP 17m iperf-service LoadBalancer 10.43.202.90 llb-192.168.80.20 55001:31377/TCP 2m34s Test the service created (from a host outside the cluster) : ## Using service VIP $ iperf -c 192.168.80.20 -p 55001 -i1 -t3 ------------------------------------------------------------ Client connecting to 192.168.80.20, TCP port 55001 TCP window size: 85.0 KByte (default) ------------------------------------------------------------ [ 1] local 192.168.80.80 port 55686 connected with 192.168.80.20 port 55001 [ ID] Interval Transfer Bandwidth [ 1] 0.0000-1.0000 sec 311 MBytes 2.61 Gbits/sec [ 1] 1.0000-2.0000 sec 309 MBytes 2.59 Gbits/sec [ 1] 2.0000-3.0000 sec 305 MBytes 2.56 Gbits/sec [ 1] 0.0000-3.0109 sec 926 MBytes 2.58 Gbits/sec ## Using node-port $ iperf -c 192.168.80.101 -p 31377 -i1 -t10 ------------------------------------------------------------ Client connecting to 192.168.80.101, TCP port 31377 TCP window size: 85.0 KByte (default) ------------------------------------------------------------ [ 1] local 192.168.80.80 port 34066 connected with 192.168.80.101 port 31377 [ ID] Interval Transfer Bandwidth [ 1] 0.0000-1.0000 sec 792 MBytes 6.64 Gbits/sec [ 1] 1.0000-2.0000 sec 727 MBytes 6.10 Gbits/sec [ 1] 2.0000-3.0000 sec 784 MBytes 6.57 Gbits/sec [ 1] 3.0000-4.0000 sec 814 MBytes 6.83 Gbits/sec [ 1] 4.0000-5.0000 sec 1.01 GBytes 8.64 Gbits/sec [ 1] 5.0000-6.0000 sec 1.02 GBytes 8.79 Gbits/sec [ 1] 6.0000-7.0000 sec 1.03 GBytes 8.84 Gbits/sec [ 1] 7.0000-8.0000 sec 814 MBytes 6.83 Gbits/sec [ 1] 8.0000-9.0000 sec 965 MBytes 8.09 Gbits/sec [ 1] 9.0000-10.0000 sec 946 MBytes 7.93 Gbits/sec [ 1] 0.0000-10.0170 sec 8.76 GBytes 7.51 Gbits/sec If you are wondering why there is a performance difference between serviceLB and node-port, there is an interesting blog about it here by one our users. For more detailed performance comparison with other solutions, kindly follow this blog and for more detailed information on incluster deployment of loxilb with bgp in a full-blown cluster, kindly follow this blog .","title":"Deploy a sample service"},{"location":"service-zones/","text":"Service-Group zoning in loxilb kube-loxilb is used to deploy loxilb with Kubernetes. By default a kube-loxilb instance does not differentiate the services in any way and uses a set-of loxilb instances to setup rules related to these services. But there are potential scenarios where grouping of services is necessary. It might be beneficial for increasing capacity, uptime and security of the cluster services. Overall topology For implementing service-groups with zones, the overall topology including all components should be similar to the following : The overall concept is to run multiple sets of kube-loxilb each for a separate zone. Each set of kube-loxilb communicates with a particular set of designated loxilb instances dedicated for that zone. Finally when the services are created, we need to mention which zone we want to place them in using special loxilb annotation. How to deploy kube-loxilb for zones ? The manifest files for deploying kube-loxilb for zones need to mention the zone they cater to. For example: kube-loxilb-south.yml args: - --loxiURL=http://12.12.12.1:11111 - --externalCIDR=123.123.123.1/24 - --zone=south kube-loxilb-north.yml args: - --loxiURL=http://12.12.12.2:11111 - --externalCIDR=124.124.124.1/24 - --zone=north Complete kube-loxilb manifests for zones can be found here which can be further modified as per user need After deployment, you can find multiple sets of kube-loxilb running as follows : # sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-6799fbcd5-6w52r 1/1 Running 0 11h kube-system local-path-provisioner-6c86858495-gkqgc 1/1 Running 0 11h kube-system metrics-server-67c658944b-vgjqd 1/1 Running 0 11h default udp-test 1/1 Running 0 11h kube-system kube-loxilb-south-596fb8957b-7xg2k 1/1 Running 0 11h kube-system kube-loxilb-north-5887f5d848-f86jv 1/1 Running 0 10h How to deploy services for zones ? The manifest files for services need to have annotation related to zone they will be served by. For example, we need to specify \"loxilb.io/zoneselect\" annotation : apiVersion: v1 kind: Service metadata: name: nginx-lb1 annotations: loxilb.io/lbmode: \"fullnat\" loxilb.io/probetimeout: \"10\" loxilb.io/proberetries: \"2\" loxilb.io/zoneselect: \"north\" spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: nginx-test ports: - port: 55002 targetPort: 80 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: nginx-test labels: what: nginx-test spec: containers: - name: nginx-test image: nginx:stable ports: - containerPort: 80 Example services manifests for zones can be found here which can be further modified as per user need","title":"How-To - service-group zones"},{"location":"service-zones/#service-group-zoning-in-loxilb","text":"kube-loxilb is used to deploy loxilb with Kubernetes. By default a kube-loxilb instance does not differentiate the services in any way and uses a set-of loxilb instances to setup rules related to these services. But there are potential scenarios where grouping of services is necessary. It might be beneficial for increasing capacity, uptime and security of the cluster services.","title":"Service-Group zoning in loxilb"},{"location":"service-zones/#overall-topology","text":"For implementing service-groups with zones, the overall topology including all components should be similar to the following : The overall concept is to run multiple sets of kube-loxilb each for a separate zone. Each set of kube-loxilb communicates with a particular set of designated loxilb instances dedicated for that zone. Finally when the services are created, we need to mention which zone we want to place them in using special loxilb annotation.","title":"Overall topology"},{"location":"service-zones/#how-to-deploy-kube-loxilb-for-zones","text":"The manifest files for deploying kube-loxilb for zones need to mention the zone they cater to. For example: kube-loxilb-south.yml args: - --loxiURL=http://12.12.12.1:11111 - --externalCIDR=123.123.123.1/24 - --zone=south kube-loxilb-north.yml args: - --loxiURL=http://12.12.12.2:11111 - --externalCIDR=124.124.124.1/24 - --zone=north Complete kube-loxilb manifests for zones can be found here which can be further modified as per user need After deployment, you can find multiple sets of kube-loxilb running as follows : # sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-6799fbcd5-6w52r 1/1 Running 0 11h kube-system local-path-provisioner-6c86858495-gkqgc 1/1 Running 0 11h kube-system metrics-server-67c658944b-vgjqd 1/1 Running 0 11h default udp-test 1/1 Running 0 11h kube-system kube-loxilb-south-596fb8957b-7xg2k 1/1 Running 0 11h kube-system kube-loxilb-north-5887f5d848-f86jv 1/1 Running 0 10h","title":"How to deploy kube-loxilb for zones ?"},{"location":"service-zones/#how-to-deploy-services-for-zones","text":"The manifest files for services need to have annotation related to zone they will be served by. For example, we need to specify \"loxilb.io/zoneselect\" annotation : apiVersion: v1 kind: Service metadata: name: nginx-lb1 annotations: loxilb.io/lbmode: \"fullnat\" loxilb.io/probetimeout: \"10\" loxilb.io/proberetries: \"2\" loxilb.io/zoneselect: \"north\" spec: externalTrafficPolicy: Local loadBalancerClass: loxilb.io/loxilb selector: what: nginx-test ports: - port: 55002 targetPort: 80 type: LoadBalancer --- apiVersion: v1 kind: Pod metadata: name: nginx-test labels: what: nginx-test spec: containers: - name: nginx-test image: nginx:stable ports: - containerPort: 80 Example services manifests for zones can be found here which can be further modified as per user need","title":"How to deploy services for zones ?"},{"location":"simple_topo/","text":"Creating a simple test topology for loxilb To test loxilb in a single node cloud-native environment, it is possible to quickly create a test topology. We will explain the steps required to create a very simple topology (more complex topologies can be built using this example) : Prerequisites : Docker should be preinstalled Pull and run loxilb docker # docker pull ghcr.io/loxilb-io/loxilb:latest # docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest Next step is to run the following script to create and configure the above topology : #!/bin/bash docker=$1 HADD=\"sudo ip netns add \" LBHCMD=\"sudo ip netns exec loxilb \" HCMD=\"sudo ip netns exec \" id=`docker ps -f name=loxilb | cut -d \" \" -f 1 | grep -iv \"CONTAINER\"` echo $id pid=`docker inspect -f '{{.State.Pid}}' $id` if [ ! -f /var/run/netns/loxilb ]; then sudo touch /var/run/netns/loxilb sudo mount -o bind /proc/$pid/ns/net /var/run/netns/loxilb fi $HADD ep1 $HADD ep2 $HADD ep3 $HADD h1 ## Configure load-balancer end-point ep1 sudo ip -n loxilb link add ellb1ep1 type veth peer name eep1llb1 netns ep1 sudo ip -n loxilb link set ellb1ep1 mtu 9000 up sudo ip -n ep1 link set eep1llb1 mtu 7000 up $LBHCMD ip addr add 31.31.31.254/24 dev ellb1ep1 $HCMD ep1 ifconfig eep1llb1 31.31.31.1/24 up $HCMD ep1 ip route add default via 31.31.31.254 $HCMD ep1 ifconfig lo up ## Configure load-balancer end-point ep2 sudo ip -n loxilb link add ellb1ep2 type veth peer name eep2llb1 netns ep2 sudo ip -n loxilb link set ellb1ep2 mtu 9000 up sudo ip -n ep2 link set eep2llb1 mtu 7000 up $LBHCMD ip addr add 32.32.32.254/24 dev ellb1ep2 $HCMD ep2 ifconfig eep2llb1 32.32.32.1/24 up $HCMD ep2 ip route add default via 32.32.32.254 $HCMD ep2 ifconfig lo up ## Configure load-balancer end-point ep3 sudo ip -n loxilb link add ellb1ep3 type veth peer name eep3llb1 netns ep3 sudo ip -n loxilb link set ellb1ep3 mtu 9000 up sudo ip -n ep3 link set eep3llb1 mtu 7000 up $LBHCMD ip addr add 33.33.33.254/24 dev ellb1ep3 $HCMD ep3 ifconfig eep3llb1 33.33.33.1/24 up $HCMD ep3 ip route add default via 33.33.33.254 $HCMD ep3 ifconfig lo up ## Configure load-balancer end-point h1 sudo ip -n loxilb link add ellb1h1 type veth peer name eh1llb1 netns h1 sudo ip -n loxilb link set ellb1h1 mtu 9000 up sudo ip -n h1 link set eh1llb1 mtu 7000 up $LBHCMD ip addr add 10.10.10.254/24 dev ellb1h1 $HCMD h1 ifconfig eh1llb1 10.10.10.1/24 up $HCMD h1 ip route add default via 10.10.10.254 $HCMD h1 ifconfig lo up Finally, we need to configure load-balancer rule inside loxilb docker as follows : docker exec -it loxilb bash root@8b74b5ddc4d2:/# loxicmd create lb 20.20.20.1 --tcp=2020:5001 --endpoints=31.31.31.1:1,32.32.32.1:1,33.33.33.1:1 So, we now have loxilb running as a docker pod with 4 hosts connected to it. 3 of the hosts act as load-balancer end-points and 1 of them act as a client. We can run any workloads as we wish inside the host pods and start testing loxilb","title":"Creating a simple test topology for loxilb"},{"location":"simple_topo/#creating-a-simple-test-topology-for-loxilb","text":"To test loxilb in a single node cloud-native environment, it is possible to quickly create a test topology. We will explain the steps required to create a very simple topology (more complex topologies can be built using this example) : Prerequisites : Docker should be preinstalled Pull and run loxilb docker # docker pull ghcr.io/loxilb-io/loxilb:latest # docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest Next step is to run the following script to create and configure the above topology : #!/bin/bash docker=$1 HADD=\"sudo ip netns add \" LBHCMD=\"sudo ip netns exec loxilb \" HCMD=\"sudo ip netns exec \" id=`docker ps -f name=loxilb | cut -d \" \" -f 1 | grep -iv \"CONTAINER\"` echo $id pid=`docker inspect -f '{{.State.Pid}}' $id` if [ ! -f /var/run/netns/loxilb ]; then sudo touch /var/run/netns/loxilb sudo mount -o bind /proc/$pid/ns/net /var/run/netns/loxilb fi $HADD ep1 $HADD ep2 $HADD ep3 $HADD h1 ## Configure load-balancer end-point ep1 sudo ip -n loxilb link add ellb1ep1 type veth peer name eep1llb1 netns ep1 sudo ip -n loxilb link set ellb1ep1 mtu 9000 up sudo ip -n ep1 link set eep1llb1 mtu 7000 up $LBHCMD ip addr add 31.31.31.254/24 dev ellb1ep1 $HCMD ep1 ifconfig eep1llb1 31.31.31.1/24 up $HCMD ep1 ip route add default via 31.31.31.254 $HCMD ep1 ifconfig lo up ## Configure load-balancer end-point ep2 sudo ip -n loxilb link add ellb1ep2 type veth peer name eep2llb1 netns ep2 sudo ip -n loxilb link set ellb1ep2 mtu 9000 up sudo ip -n ep2 link set eep2llb1 mtu 7000 up $LBHCMD ip addr add 32.32.32.254/24 dev ellb1ep2 $HCMD ep2 ifconfig eep2llb1 32.32.32.1/24 up $HCMD ep2 ip route add default via 32.32.32.254 $HCMD ep2 ifconfig lo up ## Configure load-balancer end-point ep3 sudo ip -n loxilb link add ellb1ep3 type veth peer name eep3llb1 netns ep3 sudo ip -n loxilb link set ellb1ep3 mtu 9000 up sudo ip -n ep3 link set eep3llb1 mtu 7000 up $LBHCMD ip addr add 33.33.33.254/24 dev ellb1ep3 $HCMD ep3 ifconfig eep3llb1 33.33.33.1/24 up $HCMD ep3 ip route add default via 33.33.33.254 $HCMD ep3 ifconfig lo up ## Configure load-balancer end-point h1 sudo ip -n loxilb link add ellb1h1 type veth peer name eh1llb1 netns h1 sudo ip -n loxilb link set ellb1h1 mtu 9000 up sudo ip -n h1 link set eh1llb1 mtu 7000 up $LBHCMD ip addr add 10.10.10.254/24 dev ellb1h1 $HCMD h1 ifconfig eh1llb1 10.10.10.1/24 up $HCMD h1 ip route add default via 10.10.10.254 $HCMD h1 ifconfig lo up Finally, we need to configure load-balancer rule inside loxilb docker as follows : docker exec -it loxilb bash root@8b74b5ddc4d2:/# loxicmd create lb 20.20.20.1 --tcp=2020:5001 --endpoints=31.31.31.1:1,32.32.32.1:1,33.33.33.1:1 So, we now have loxilb running as a docker pod with 4 hosts connected to it. 3 of the hosts act as load-balancer end-points and 1 of them act as a client. We can run any workloads as we wish inside the host pods and start testing loxilb","title":"Creating a simple test topology for loxilb"},{"location":"standalone/","text":"How to run loxilb in standalone mode This guide will help users to run loxilb in a standalone mode decoupled from kubernetes Pre-requisites This guide uses Ubuntu 20.04.5 LTS as the base operating system Install docker One can follow the guide here to install latest docker engine or use snap to install docker. sudo apt update sudo apt install snapd sudo snap install docker Enable IPv6 (if running NAT64/NAT66) sysctl net.ipv6.conf.all.disable_ipv6=0 sysctl net.ipv6.conf.default.disable_ipv6=0 Run loxilb Get the loxilb official docker image Latest build image (multi-arch amd64/arm64) docker pull ghcr.io/loxilb-io/loxilb:latest Release build image docker pull ghcr.io/loxilb-io/loxilb:v0.9.5 To run loxilb docker, we can use the following commands : docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest To drop in to a shell of loxilb doker : docker exec -it loxilb bash For load-balancing to effectively work in a bare-metal environment, we need multiple interfaces assigned to the docker (external and internal connectivitiy). loxilb docker relies on docker's macvlan driver for achieving this. The following is an example of creating macvlan network and using with loxilb: # Create a mac-vlan (on an underlying interface e.g. enp0s3). # Subnet used for mac-vlan is usually the same as underlying interface docker network create -d macvlan -o parent=enp0s3 --subnet 172.30.1.0/24 --gateway 172.30.1.254 --aux-address 'host=172.30.1.193\u2019 llbnet # Run loxilb docker with the created macvlan docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --net=llbnet --ip=172.30.1.195 --name loxilb ghcr.io/loxilb-io/loxilb:latest # If we still want to connect loxilb docker additionally to docker's default \"bridge\" network or more macvlan networks docker network connect bridge loxilb docker network connect llbnet2 loxilb --ip=172.30.2.195 Note: While working with macvlan interfaces, the parent/underlying interface should be put in promiscous mode One can further use docker-compose to automate attaching multiple networks to loxilb docker or use --net=host as per requirement To use local socket policy or eBPF sockmap related features, we need to use --pid=host --cgroupns=host as additional arguments to docker run. To create a simple and self-contained topology for testing loxilb, users can follow this guide If loxilb docker is in the same node as the app/workload docker, it is advised that \"tx checksum offload\" inside app/workload docker is turned off for sctp load-balancing to work properly docker exec -dt <app-docker-name> ethtool -K <app-docker-interface> tx off Configuration loxicmd command line tool can be used to configure loxilb in standalone mode. A simple example of configuration using loxilb is as follows: Drop into loxilb shell sudo docker exec -it loxilb bash Create a LB rule inside loxilb docker. Various other options for LB manipulation can be found here loxicmd create lb 2001::1 --tcp=2020:8080 --endpoints=33.33.33.1:1 Validate entry is created using the command: loxicmd get lb -o wide The detailed usage guide of loxicmd can be found here . Working with gobgp loxilb works in tandem with gobgp when bgp services are required. As a first step, create a file gobgp.conf in host where loxilb docker will run and add the basic necessary fields : [global.config] as = 64512 router-id = \"10.10.10.1\" [[neighbors]] [neighbors.config] neighbor-address = \"10.10.10.254\" peer-as = 64512 Run loxilb docker with following arguments: docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v gobgp.conf:/etc/gobgp/gobgp.conf -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest -b The gobgp daemon should pick the configuration. The neighbors can be verified by : sudo docker exec -it loxilb gobgp neighbor At run time, there are two ways to change gobgp configuration. Ephemeral configuration can simply be done using \u201cgobgp\u201d command as detailed here . If persistence is required, then one can change the gobgp config file /etc/gobgp/gobgp.conf and apply SIGHUP to gobgpd process for loading the edited configuration. sudo docker exec -it loxilb pkill -1 gobgpd Persistent LB entries To save the created rules across reboots, one can use the following command: sudo mkdir -p /etc/loxilb/ sudo loxicmd save --lb","title":"loxilb in standalone mode"},{"location":"standalone/#how-to-run-loxilb-in-standalone-mode","text":"This guide will help users to run loxilb in a standalone mode decoupled from kubernetes","title":"How to run loxilb in standalone mode"},{"location":"standalone/#pre-requisites","text":"This guide uses Ubuntu 20.04.5 LTS as the base operating system","title":"Pre-requisites"},{"location":"standalone/#install-docker","text":"One can follow the guide here to install latest docker engine or use snap to install docker. sudo apt update sudo apt install snapd sudo snap install docker","title":"Install docker"},{"location":"standalone/#enable-ipv6-if-running-nat64nat66","text":"sysctl net.ipv6.conf.all.disable_ipv6=0 sysctl net.ipv6.conf.default.disable_ipv6=0","title":"Enable IPv6 (if running NAT64/NAT66)"},{"location":"standalone/#run-loxilb","text":"Get the loxilb official docker image Latest build image (multi-arch amd64/arm64) docker pull ghcr.io/loxilb-io/loxilb:latest Release build image docker pull ghcr.io/loxilb-io/loxilb:v0.9.5 To run loxilb docker, we can use the following commands : docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest To drop in to a shell of loxilb doker : docker exec -it loxilb bash For load-balancing to effectively work in a bare-metal environment, we need multiple interfaces assigned to the docker (external and internal connectivitiy). loxilb docker relies on docker's macvlan driver for achieving this. The following is an example of creating macvlan network and using with loxilb: # Create a mac-vlan (on an underlying interface e.g. enp0s3). # Subnet used for mac-vlan is usually the same as underlying interface docker network create -d macvlan -o parent=enp0s3 --subnet 172.30.1.0/24 --gateway 172.30.1.254 --aux-address 'host=172.30.1.193\u2019 llbnet # Run loxilb docker with the created macvlan docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v /dev/log:/dev/log --net=llbnet --ip=172.30.1.195 --name loxilb ghcr.io/loxilb-io/loxilb:latest # If we still want to connect loxilb docker additionally to docker's default \"bridge\" network or more macvlan networks docker network connect bridge loxilb docker network connect llbnet2 loxilb --ip=172.30.2.195 Note: While working with macvlan interfaces, the parent/underlying interface should be put in promiscous mode One can further use docker-compose to automate attaching multiple networks to loxilb docker or use --net=host as per requirement To use local socket policy or eBPF sockmap related features, we need to use --pid=host --cgroupns=host as additional arguments to docker run. To create a simple and self-contained topology for testing loxilb, users can follow this guide If loxilb docker is in the same node as the app/workload docker, it is advised that \"tx checksum offload\" inside app/workload docker is turned off for sctp load-balancing to work properly docker exec -dt <app-docker-name> ethtool -K <app-docker-interface> tx off","title":"Run loxilb"},{"location":"standalone/#configuration","text":"loxicmd command line tool can be used to configure loxilb in standalone mode. A simple example of configuration using loxilb is as follows: Drop into loxilb shell sudo docker exec -it loxilb bash Create a LB rule inside loxilb docker. Various other options for LB manipulation can be found here loxicmd create lb 2001::1 --tcp=2020:8080 --endpoints=33.33.33.1:1 Validate entry is created using the command: loxicmd get lb -o wide The detailed usage guide of loxicmd can be found here .","title":"Configuration"},{"location":"standalone/#working-with-gobgp","text":"loxilb works in tandem with gobgp when bgp services are required. As a first step, create a file gobgp.conf in host where loxilb docker will run and add the basic necessary fields : [global.config] as = 64512 router-id = \"10.10.10.1\" [[neighbors]] [neighbors.config] neighbor-address = \"10.10.10.254\" peer-as = 64512 Run loxilb docker with following arguments: docker run -u root --cap-add SYS_ADMIN --restart unless-stopped --privileged -dit -v gobgp.conf:/etc/gobgp/gobgp.conf -v /dev/log:/dev/log --name loxilb ghcr.io/loxilb-io/loxilb:latest -b The gobgp daemon should pick the configuration. The neighbors can be verified by : sudo docker exec -it loxilb gobgp neighbor At run time, there are two ways to change gobgp configuration. Ephemeral configuration can simply be done using \u201cgobgp\u201d command as detailed here . If persistence is required, then one can change the gobgp config file /etc/gobgp/gobgp.conf and apply SIGHUP to gobgpd process for loading the edited configuration. sudo docker exec -it loxilb pkill -1 gobgpd","title":"Working with gobgp"},{"location":"standalone/#persistent-lb-entries","text":"To save the created rules across reboots, one can use the following command: sudo mkdir -p /etc/loxilb/ sudo loxicmd save --lb","title":"Persistent LB entries"}]}